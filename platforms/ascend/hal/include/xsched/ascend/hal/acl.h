// Merged header file - auto-generated by python3 ./merge_headers.py ./inc/aclnnop

// Begin content from: acl/error_codes/ge_error_codes.h
/* Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 * ===================================================================================================================*/

#ifndef INC_EXTERNAL_GE_GE_ERROR_CODES_H_
#define INC_EXTERNAL_GE_GE_ERROR_CODES_H_

#if defined(_MSC_VER)
#ifdef FUNC_VISIBILITY
#define GE_FUNC_VISIBILITY _declspec(dllexport)
#else
#define GE_FUNC_VISIBILITY
#endif
#else
#ifdef FUNC_VISIBILITY
#define GE_FUNC_VISIBILITY __attribute__((visibility("default")))
#else
#define GE_FUNC_VISIBILITY
#endif
#endif

#include <stddef.h>
#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif
static const uint32_t ACL_ERROR_GE_PARAM_INVALID = 145000U;
static const uint32_t ACL_ERROR_GE_EXEC_NOT_INIT = 145001U;
static const uint32_t ACL_ERROR_GE_EXEC_MODEL_PATH_INVALID = 145002U;
static const uint32_t ACL_ERROR_GE_EXEC_MODEL_ID_INVALID = 145003U;
static const uint32_t ACL_ERROR_GE_EXEC_MODEL_DATA_SIZE_INVALID = 145006U;
static const uint32_t ACL_ERROR_GE_EXEC_MODEL_ADDR_INVALID = 145007U;
static const uint32_t ACL_ERROR_GE_EXEC_MODEL_QUEUE_ID_INVALID = 145008U;
static const uint32_t ACL_ERROR_GE_EXEC_LOAD_MODEL_REPEATED = 145009U;
static const uint32_t ACL_ERROR_GE_DYNAMIC_INPUT_ADDR_INVALID = 145011U;
static const uint32_t ACL_ERROR_GE_DYNAMIC_INPUT_LENGTH_INVALID = 145012U;
static const uint32_t ACL_ERROR_GE_DYNAMIC_BATCH_SIZE_INVALID = 145013U;
static const uint32_t ACL_ERROR_GE_AIPP_BATCH_EMPTY = 145014U;
static const uint32_t ACL_ERROR_GE_AIPP_NOT_EXIST = 145015U;
static const uint32_t ACL_ERROR_GE_AIPP_MODE_INVALID = 145016U;
static const uint32_t ACL_ERROR_GE_OP_TASK_TYPE_INVALID = 145017U;
static const uint32_t ACL_ERROR_GE_OP_KERNEL_TYPE_INVALID = 145018U;
static const uint32_t ACL_ERROR_GE_PLGMGR_PATH_INVALID = 145019U;
static const uint32_t ACL_ERROR_GE_FORMAT_INVALID = 145020U;
static const uint32_t ACL_ERROR_GE_SHAPE_INVALID = 145021U;
static const uint32_t ACL_ERROR_GE_DATATYPE_INVALID = 145022U;
static const uint32_t ACL_ERROR_GE_MEMORY_ALLOCATION = 245000U;
static const uint32_t ACL_ERROR_GE_MEMORY_OPERATE_FAILED = 245001U;
static const uint32_t ACL_ERROR_GE_DEVICE_MEMORY_OPERATE_FAILED = 245002U;
static const uint32_t ACL_ERROR_GE_SUBHEALTHY = 345102U;
static const uint32_t ACL_ERROR_GE_USER_RAISE_EXCEPTION = 345103U;
static const uint32_t ACL_ERROR_GE_DATA_NOT_ALIGNED = 345104U;
static const uint32_t ACL_ERROR_GE_INTERNAL_ERROR = 545000U;
static const uint32_t ACL_ERROR_GE_LOAD_MODEL = 545001U;
static const uint32_t ACL_ERROR_GE_EXEC_LOAD_MODEL_PARTITION_FAILED = 545002U;
static const uint32_t ACL_ERROR_GE_EXEC_LOAD_WEIGHT_PARTITION_FAILED = 545003U;
static const uint32_t ACL_ERROR_GE_EXEC_LOAD_TASK_PARTITION_FAILED = 545004U;
static const uint32_t ACL_ERROR_GE_EXEC_LOAD_KERNEL_PARTITION_FAILED = 545005U;
static const uint32_t ACL_ERROR_GE_EXEC_RELEASE_MODEL_DATA = 545006U;
static const uint32_t ACL_ERROR_GE_COMMAND_HANDLE = 545007U;
static const uint32_t ACL_ERROR_GE_GET_TENSOR_INFO = 545008U;
static const uint32_t ACL_ERROR_GE_UNLOAD_MODEL = 545009U;
static const uint32_t ACL_ERROR_GE_MODEL_EXECUTE_TIMEOUT = 545601U;
static const uint32_t ACL_ERROR_GE_REDEPLOYING = 545602U;

#ifdef __cplusplus
}  // namespace ge
#endif
#endif  // INC_EXTERNAL_GE_GE_ERROR_CODES_H_
// End content from: acl/error_codes/ge_error_codes.h

// Begin content from: acl/error_codes/rt_error_codes.h
/**
* @file rt_error_codes.h
*
* Copyright (C) Huawei Technologies Co., Ltd. 2019-2020. All Rights Reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/

#ifndef __INC_EXTERNEL_RT_ERROR_CODES_H__
#define __INC_EXTERNEL_RT_ERROR_CODES_H__

#include <stddef.h>

#ifdef __cplusplus
extern "C" {
#endif

#define  ACL_RT_SUCCESS    0                               // success
#define  ACL_ERROR_RT_PARAM_INVALID              107000 // param invalid
#define  ACL_ERROR_RT_INVALID_DEVICEID           107001 // invalid device id
#define  ACL_ERROR_RT_CONTEXT_NULL               107002 // current context null
#define  ACL_ERROR_RT_STREAM_CONTEXT             107003 // stream not in current context
#define  ACL_ERROR_RT_MODEL_CONTEXT              107004 // model not in current context
#define  ACL_ERROR_RT_STREAM_MODEL               107005 // stream not in model
#define  ACL_ERROR_RT_EVENT_TIMESTAMP_INVALID    107006 // event timestamp invalid
#define  ACL_ERROR_RT_EVENT_TIMESTAMP_REVERSAL   107007 // event timestamp reversal
#define  ACL_ERROR_RT_ADDR_UNALIGNED             107008 // memory address unaligned
#define  ACL_ERROR_RT_FILE_OPEN                  107009 // open file failed
#define  ACL_ERROR_RT_FILE_WRITE                 107010 // write file failed
#define  ACL_ERROR_RT_STREAM_SUBSCRIBE           107011 // error subscribe stream
#define  ACL_ERROR_RT_THREAD_SUBSCRIBE           107012 // error subscribe thread
#define  ACL_ERROR_RT_GROUP_NOT_SET              107013 // group not set
#define  ACL_ERROR_RT_GROUP_NOT_CREATE           107014 // group not create
#define  ACL_ERROR_RT_STREAM_NO_CB_REG           107015 // callback not register to stream
#define  ACL_ERROR_RT_INVALID_MEMORY_TYPE        107016 // invalid memory type
#define  ACL_ERROR_RT_INVALID_HANDLE             107017 // invalid handle
#define  ACL_ERROR_RT_INVALID_MALLOC_TYPE        107018 // invalid malloc type
#define  ACL_ERROR_RT_WAIT_TIMEOUT               107019 // wait timeout
#define  ACL_ERROR_RT_TASK_TIMEOUT               107020 // task timeout
#define  ACL_ERROR_RT_SYSPARAMOPT_NOT_SET        107021 // not set sysparamopt
#define  ACL_ERROR_RT_DEVICE_TASK_ABORT          107022 // device task aborting
#define  ACL_ERROR_RT_STREAM_ABORT               107023 // stream aborting

#define  ACL_ERROR_RT_FEATURE_NOT_SUPPORT        207000 // feature not support
#define  ACL_ERROR_RT_MEMORY_ALLOCATION          207001 // memory allocation error
#define  ACL_ERROR_RT_MEMORY_FREE                207002 // memory free error
#define  ACL_ERROR_RT_AICORE_OVER_FLOW           207003 // aicore over flow
#define  ACL_ERROR_RT_NO_DEVICE                  207004 // no device
#define  ACL_ERROR_RT_RESOURCE_ALLOC_FAIL        207005 // resource alloc fail
#define  ACL_ERROR_RT_NO_PERMISSION              207006 // no permission
#define  ACL_ERROR_RT_NO_EVENT_RESOURCE          207007 // no event resource
#define  ACL_ERROR_RT_NO_STREAM_RESOURCE         207008 // no stream resource
#define  ACL_ERROR_RT_NO_NOTIFY_RESOURCE         207009 // no notify resource
#define  ACL_ERROR_RT_NO_MODEL_RESOURCE          207010 // no model resource
#define  ACL_ERROR_RT_NO_CDQ_RESOURCE            207011 // no cdq resource
#define  ACL_ERROR_RT_OVER_LIMIT                 207012 // over limit
#define  ACL_ERROR_RT_QUEUE_EMPTY                207013 // queue is empty
#define  ACL_ERROR_RT_QUEUE_FULL                 207014 // queue is full
#define  ACL_ERROR_RT_REPEATED_INIT              207015 // repeated init
#define  ACL_ERROR_RT_AIVEC_OVER_FLOW            207016 // aivec over flow
#define  ACL_ERROR_RT_OVER_FLOW                  207017 // common over flow
#define  ACL_ERROR_RT_DEVICE_OOM                 207018 // device oom

#define  ACL_ERROR_RT_INTERNAL_ERROR             507000 // runtime internal error
#define  ACL_ERROR_RT_TS_ERROR                   507001 // ts internel error
#define  ACL_ERROR_RT_STREAM_TASK_FULL           507002 // task full in stream
#define  ACL_ERROR_RT_STREAM_TASK_EMPTY          507003 // task empty in stream
#define  ACL_ERROR_RT_STREAM_NOT_COMPLETE        507004 // stream not complete
#define  ACL_ERROR_RT_END_OF_SEQUENCE            507005 // end of sequence
#define  ACL_ERROR_RT_EVENT_NOT_COMPLETE         507006 // event not complete
#define  ACL_ERROR_RT_CONTEXT_RELEASE_ERROR      507007 // context release error
#define  ACL_ERROR_RT_SOC_VERSION                507008 // soc version error
#define  ACL_ERROR_RT_TASK_TYPE_NOT_SUPPORT      507009 // task type not support
#define  ACL_ERROR_RT_LOST_HEARTBEAT             507010 // ts lost heartbeat
#define  ACL_ERROR_RT_MODEL_EXECUTE              507011 // model execute failed
#define  ACL_ERROR_RT_REPORT_TIMEOUT             507012 // report timeout
#define  ACL_ERROR_RT_SYS_DMA                    507013 // sys dma error
#define  ACL_ERROR_RT_AICORE_TIMEOUT             507014 // aicore timeout
#define  ACL_ERROR_RT_AICORE_EXCEPTION           507015 // aicore exception
#define  ACL_ERROR_RT_AICORE_TRAP_EXCEPTION      507016 // aicore trap exception
#define  ACL_ERROR_RT_AICPU_TIMEOUT              507017 // aicpu timeout
#define  ACL_ERROR_RT_AICPU_EXCEPTION            507018 // aicpu exception
#define  ACL_ERROR_RT_AICPU_DATADUMP_RSP_ERR     507019 // aicpu datadump response error
#define  ACL_ERROR_RT_AICPU_MODEL_RSP_ERR        507020 // aicpu model operate response error
#define  ACL_ERROR_RT_PROFILING_ERROR            507021 // profiling error
#define  ACL_ERROR_RT_IPC_ERROR                  507022 // ipc error
#define  ACL_ERROR_RT_MODEL_ABORT_NORMAL         507023 // model abort normal
#define  ACL_ERROR_RT_KERNEL_UNREGISTERING       507024 // kernel unregistering
#define  ACL_ERROR_RT_RINGBUFFER_NOT_INIT        507025 // ringbuffer not init
#define  ACL_ERROR_RT_RINGBUFFER_NO_DATA         507026 // ringbuffer no data
#define  ACL_ERROR_RT_KERNEL_LOOKUP              507027 // kernel lookup error
#define  ACL_ERROR_RT_KERNEL_DUPLICATE           507028 // kernel register duplicate
#define  ACL_ERROR_RT_DEBUG_REGISTER_FAIL        507029 // debug register failed
#define  ACL_ERROR_RT_DEBUG_UNREGISTER_FAIL      507030 // debug unregister failed
#define  ACL_ERROR_RT_LABEL_CONTEXT              507031 // label not in current context
#define  ACL_ERROR_RT_PROGRAM_USE_OUT            507032 // program register num use out
#define  ACL_ERROR_RT_DEV_SETUP_ERROR            507033 // device setup error
#define  ACL_ERROR_RT_VECTOR_CORE_TIMEOUT        507034 // vector core timeout
#define  ACL_ERROR_RT_VECTOR_CORE_EXCEPTION      507035 // vector core exception
#define  ACL_ERROR_RT_VECTOR_CORE_TRAP_EXCEPTION 507036 // vector core trap exception
#define  ACL_ERROR_RT_CDQ_BATCH_ABNORMAL         507037 // cdq alloc batch abnormal
#define  ACL_ERROR_RT_DIE_MODE_CHANGE_ERROR      507038 // can not change die mode
#define  ACL_ERROR_RT_DIE_SET_ERROR              507039 // single die mode can not set die
#define  ACL_ERROR_RT_INVALID_DIEID              507040 // invalid die id
#define  ACL_ERROR_RT_DIE_MODE_NOT_SET           507041 // die mode not set
#define  ACL_ERROR_RT_AICORE_TRAP_READ_OVERFLOW       507042 // aic trap read overflow
#define  ACL_ERROR_RT_AICORE_TRAP_WRITE_OVERFLOW      507043 // aic trap write overflow
#define  ACL_ERROR_RT_VECTOR_CORE_TRAP_READ_OVERFLOW  507044 // aiv trap read overflow
#define  ACL_ERROR_RT_VECTOR_CORE_TRAP_WRITE_OVERFLOW 507045 // aiv trap write overflow
#define  ACL_ERROR_RT_STREAM_SYNC_TIMEOUT        507046 // stream sync time out
#define  ACL_ERROR_RT_EVENT_SYNC_TIMEOUT         507047 // event sync time out
#define  ACL_ERROR_RT_FFTS_PLUS_TIMEOUT          507048 // ffts+ timeout
#define  ACL_ERROR_RT_FFTS_PLUS_EXCEPTION        507049 // ffts+ exception
#define  ACL_ERROR_RT_FFTS_PLUS_TRAP_EXCEPTION   507050 // ffts+ trap exception
#define  ACL_ERROR_RT_SEND_MSG                   507051 // hdc send msg fail
#define  ACL_ERROR_RT_COPY_DATA                  507052 // copy data fail
#define  ACL_ERROR_RT_DEVICE_MEM_ERROR           507053 // device MEM ERROR
#define  ACL_ERROR_RT_DRV_INTERNAL_ERROR         507899 // drv internal error
#define  ACL_ERROR_RT_AICPU_INTERNAL_ERROR       507900 // aicpu internal error
#define  ACL_ERROR_RT_SOCKET_CLOSE               507901 // hdc disconnect
#define  ACL_ERROR_RT_AICPU_INFO_LOAD_RSP_ERR    507902 // aicpu info load response error

#ifdef __cplusplus
}
#endif
#endif // __INC_EXTERNEL_RT_ERROR_CODES_H__
// End content from: acl/error_codes/rt_error_codes.h

// Begin content from: acl/acl_base.h
/**
* @file acl_base.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/

#ifndef INC_EXTERNAL_ACL_ACL_BASE_H_
#define INC_EXTERNAL_ACL_ACL_BASE_H_

#include <stdint.h>
#include <stddef.h>
// #include "error_codes/rt_error_codes.h"
// #include "error_codes/ge_error_codes.h"

#ifdef __cplusplus
extern "C" {
#endif

#if defined(_MSC_VER)
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY _declspec(dllexport)
#else
#define ACL_FUNC_VISIBILITY
#endif
#else
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY __attribute__((visibility("default")))
#else
#define ACL_FUNC_VISIBILITY
#endif
#endif

#ifdef __GNUC__
#define ACL_DEPRECATED __attribute__((deprecated))
#define ACL_DEPRECATED_MESSAGE(message) __attribute__((deprecated(message)))
#elif defined(_MSC_VER)
#define ACL_DEPRECATED __declspec(deprecated)
#define ACL_DEPRECATED_MESSAGE(message) __declspec(deprecated(message))
#else
#define ACL_DEPRECATED
#define ACL_DEPRECATED_MESSAGE(message)
#endif

typedef void *aclrtStream;
typedef void *aclrtEvent;
typedef void *aclrtContext;
typedef int aclError;
typedef uint16_t aclFloat16;
typedef struct aclDataBuffer aclDataBuffer;
typedef struct aclTensorDesc aclTensorDesc;
typedef void *aclrtAllocatorDesc;
typedef void *aclrtAllocator;
typedef void *aclrtAllocatorBlock;
typedef void *aclrtAllocatorAddr;

static const int ACL_ERROR_NONE = 0;
static const int ACL_SUCCESS = 0;

static const int ACL_ERROR_INVALID_PARAM = 100000;
static const int ACL_ERROR_UNINITIALIZE = 100001;
static const int ACL_ERROR_REPEAT_INITIALIZE = 100002;
static const int ACL_ERROR_INVALID_FILE = 100003;
static const int ACL_ERROR_WRITE_FILE = 100004;
static const int ACL_ERROR_INVALID_FILE_SIZE = 100005;
static const int ACL_ERROR_PARSE_FILE = 100006;
static const int ACL_ERROR_FILE_MISSING_ATTR = 100007;
static const int ACL_ERROR_FILE_ATTR_INVALID = 100008;
static const int ACL_ERROR_INVALID_DUMP_CONFIG = 100009;
static const int ACL_ERROR_INVALID_PROFILING_CONFIG = 100010;
static const int ACL_ERROR_INVALID_MODEL_ID = 100011;
static const int ACL_ERROR_DESERIALIZE_MODEL = 100012;
static const int ACL_ERROR_PARSE_MODEL = 100013;
static const int ACL_ERROR_READ_MODEL_FAILURE = 100014;
static const int ACL_ERROR_MODEL_SIZE_INVALID = 100015;
static const int ACL_ERROR_MODEL_MISSING_ATTR = 100016;
static const int ACL_ERROR_MODEL_INPUT_NOT_MATCH = 100017;
static const int ACL_ERROR_MODEL_OUTPUT_NOT_MATCH = 100018;
static const int ACL_ERROR_MODEL_NOT_DYNAMIC = 100019;
static const int ACL_ERROR_OP_TYPE_NOT_MATCH = 100020;
static const int ACL_ERROR_OP_INPUT_NOT_MATCH = 100021;
static const int ACL_ERROR_OP_OUTPUT_NOT_MATCH = 100022;
static const int ACL_ERROR_OP_ATTR_NOT_MATCH = 100023;
static const int ACL_ERROR_OP_NOT_FOUND = 100024;
static const int ACL_ERROR_OP_LOAD_FAILED = 100025;
static const int ACL_ERROR_UNSUPPORTED_DATA_TYPE = 100026;
static const int ACL_ERROR_FORMAT_NOT_MATCH = 100027;
static const int ACL_ERROR_BIN_SELECTOR_NOT_REGISTERED = 100028;
static const int ACL_ERROR_KERNEL_NOT_FOUND = 100029;
static const int ACL_ERROR_BIN_SELECTOR_ALREADY_REGISTERED = 100030;
static const int ACL_ERROR_KERNEL_ALREADY_REGISTERED = 100031;
static const int ACL_ERROR_INVALID_QUEUE_ID = 100032;
static const int ACL_ERROR_REPEAT_SUBSCRIBE = 100033;
static const int ACL_ERROR_STREAM_NOT_SUBSCRIBE = 100034;
static const int ACL_ERROR_THREAD_NOT_SUBSCRIBE = 100035;
static const int ACL_ERROR_WAIT_CALLBACK_TIMEOUT = 100036;
static const int ACL_ERROR_REPEAT_FINALIZE = 100037;
static const int ACL_ERROR_NOT_STATIC_AIPP = 100038;
static const int ACL_ERROR_COMPILING_STUB_MODE = 100039;
static const int ACL_ERROR_GROUP_NOT_SET = 100040;
static const int ACL_ERROR_GROUP_NOT_CREATE = 100041;
static const int ACL_ERROR_PROF_ALREADY_RUN = 100042;
static const int ACL_ERROR_PROF_NOT_RUN = 100043;
static const int ACL_ERROR_DUMP_ALREADY_RUN = 100044;
static const int ACL_ERROR_DUMP_NOT_RUN = 100045;
static const int ACL_ERROR_PROF_REPEAT_SUBSCRIBE = 148046;
static const int ACL_ERROR_PROF_API_CONFLICT = 148047;
static const int ACL_ERROR_INVALID_MAX_OPQUEUE_NUM_CONFIG = 148048;
static const int ACL_ERROR_INVALID_OPP_PATH = 148049;
static const int ACL_ERROR_OP_UNSUPPORTED_DYNAMIC = 148050;
static const int ACL_ERROR_RELATIVE_RESOURCE_NOT_CLEARED = 148051;
static const int ACL_ERROR_UNSUPPORTED_JPEG = 148052;
static const int ACL_ERROR_INVALID_BUNDLE_MODEL_ID = 148053;

static const int ACL_ERROR_BAD_ALLOC = 200000;
static const int ACL_ERROR_API_NOT_SUPPORT = 200001;
static const int ACL_ERROR_INVALID_DEVICE = 200002;
static const int ACL_ERROR_MEMORY_ADDRESS_UNALIGNED = 200003;
static const int ACL_ERROR_RESOURCE_NOT_MATCH = 200004;
static const int ACL_ERROR_INVALID_RESOURCE_HANDLE = 200005;
static const int ACL_ERROR_FEATURE_UNSUPPORTED = 200006;
static const int ACL_ERROR_PROF_MODULES_UNSUPPORTED = 200007;

static const int ACL_ERROR_STORAGE_OVER_LIMIT = 300000;

static const int ACL_ERROR_INTERNAL_ERROR = 500000;
static const int ACL_ERROR_FAILURE = 500001;
static const int ACL_ERROR_GE_FAILURE = 500002;
static const int ACL_ERROR_RT_FAILURE = 500003;
static const int ACL_ERROR_DRV_FAILURE = 500004;
static const int ACL_ERROR_PROFILING_FAILURE = 500005;

#define ACL_TENSOR_SHAPE_RANGE_NUM 2
#define ACL_TENSOR_VALUE_RANGE_NUM 2
#define ACL_UNKNOWN_RANK 0xFFFFFFFFFFFFFFFE

typedef enum {
    ACL_DT_UNDEFINED = -1,
    ACL_FLOAT = 0,
    ACL_FLOAT16 = 1,
    ACL_INT8 = 2,
    ACL_INT32 = 3,
    ACL_UINT8 = 4,
    ACL_INT16 = 6,
    ACL_UINT16 = 7,
    ACL_UINT32 = 8,
    ACL_INT64 = 9,
    ACL_UINT64 = 10,
    ACL_DOUBLE = 11,
    ACL_BOOL = 12,
    ACL_STRING = 13,
    ACL_COMPLEX64 = 16,
    ACL_COMPLEX128 = 17,
    ACL_BF16 = 27,
    ACL_INT4 = 29,
    ACL_UINT1 = 30,
    ACL_COMPLEX32 = 33,
} aclDataType;

typedef enum {
    ACL_FORMAT_UNDEFINED = -1,
    ACL_FORMAT_NCHW = 0,
    ACL_FORMAT_NHWC = 1,
    ACL_FORMAT_ND = 2,
    ACL_FORMAT_NC1HWC0 = 3,
    ACL_FORMAT_FRACTAL_Z = 4,
    ACL_FORMAT_NC1HWC0_C04 = 12,
    ACL_FORMAT_HWCN = 16,
    ACL_FORMAT_NDHWC = 27,
    ACL_FORMAT_FRACTAL_NZ = 29,
    ACL_FORMAT_NCDHW = 30,
    ACL_FORMAT_NDC1HWC0 = 32,
    ACL_FRACTAL_Z_3D = 33,
    ACL_FORMAT_NC = 35,
    ACL_FORMAT_NCL = 47,
} aclFormat;

typedef enum {
    ACL_DEBUG = 0,
    ACL_INFO = 1,
    ACL_WARNING = 2,
    ACL_ERROR = 3,
} aclLogLevel;

typedef enum {
    ACL_MEMTYPE_DEVICE = 0,
    ACL_MEMTYPE_HOST = 1,
    ACL_MEMTYPE_HOST_COMPILE_INDEPENDENT = 2
} aclMemType;

typedef enum {
    ACL_OPT_DETERMINISTIC = 0,
    ACL_OPT_ENABLE_DEBUG_KERNEL = 1
} aclSysParamOpt;

typedef enum {
    ACL_CANN_ATTR_UNDEFINED = -1,
    ACL_CANN_ATTR_INF_NAN = 0,
    ACL_CANN_ATTR_BF16 = 1,
    ACL_CANN_ATTR_JIT_COMPILE = 2
} aclCannAttr;

typedef enum {
    ACL_DEVICE_INFO_UNDEFINED = -1,
    ACL_DEVICE_INFO_AI_CORE_NUM = 0,
    ACL_DEVICE_INFO_VECTOR_CORE_NUM = 1,
    ACL_DEVICE_INFO_L2_SIZE = 2
} aclDeviceInfo;

/**
 * @ingroup AscendCL
 * @brief Converts data of type aclFloat16 to data of type float
 *
 * @param value [IN]   Data to be converted
 *
 * @retval Transformed data
 */
ACL_FUNC_VISIBILITY float aclFloat16ToFloat(aclFloat16 value);

/**
 * @ingroup AscendCL
 * @brief Converts data of type float to data of type aclFloat16
 *
 * @param value [IN]   Data to be converted
 *
 * @retval Transformed data
 */
ACL_FUNC_VISIBILITY aclFloat16 aclFloatToFloat16(float value);

/**
 * @ingroup AscendCL
 * @brief create data of aclDataBuffer
 *
 * @param data [IN]    pointer to data
 * @li Need to be managed by the user,
 *  call aclrtMalloc interface to apply for memory,
 *  call aclrtFree interface to release memory
 *
 * @param size [IN]    size of data in bytes
 *
 * @retval pointer to created instance. nullptr if run out of memory
 *
 * @see aclrtMalloc | aclrtFree
 */
ACL_FUNC_VISIBILITY aclDataBuffer *aclCreateDataBuffer(void *data, size_t size);

/**
 * @ingroup AscendCL
 * @brief destroy data of aclDataBuffer
 *
 * @par Function
 *  Only the aclDataBuffer type data is destroyed here.
 *  The memory of the data passed in when the aclDataDataBuffer interface
 *  is called to create aclDataBuffer type data must be released by the user
 *
 * @param  dataBuffer [IN]   pointer to the aclDataBuffer
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclCreateDataBuffer
 */
ACL_FUNC_VISIBILITY aclError aclDestroyDataBuffer(const aclDataBuffer *dataBuffer);

/**
 * @ingroup AscendCL
 * @brief update new data of aclDataBuffer
 *
 * @param dataBuffer [OUT]    pointer to aclDataBuffer
 * @li The old data need to be released by the user, otherwise it may occur memory leak leakage
 *  call aclGetDataBufferAddr interface to get old data address
 *  call aclrtFree interface to release memory
 *
 * @param data [IN]    pointer to new data
 * @li Need to be managed by the user,
 *  call aclrtMalloc interface to apply for memory,
 *  call aclrtFree interface to release memory
 *
 * @param size [IN]    size of data in bytes
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtMalloc | aclrtFree | aclGetDataBufferAddr
 */
ACL_FUNC_VISIBILITY aclError aclUpdateDataBuffer(aclDataBuffer *dataBuffer, void *data, size_t size);

/**
 * @ingroup AscendCL
 * @brief get data address from aclDataBuffer
 *
 * @param dataBuffer [IN]    pointer to the data of aclDataBuffer
 *
 * @retval data address
 */
ACL_FUNC_VISIBILITY void *aclGetDataBufferAddr(const aclDataBuffer *dataBuffer);

/**
 * @ingroup AscendCL
 * @brief get data size of aclDataBuffer
 *
 * @param  dataBuffer [IN]    pointer to the data of aclDataBuffer
 *
 * @retval data size
 */
ACL_DEPRECATED_MESSAGE("aclGetDataBufferSize is deprecated, use aclGetDataBufferSizeV2 instead")
ACL_FUNC_VISIBILITY uint32_t aclGetDataBufferSize(const aclDataBuffer *dataBuffer);

/**
 * @ingroup AscendCL
 * @brief get data size of aclDataBuffer to replace aclGetDataBufferSize
 *
 * @param  dataBuffer [IN]    pointer to the data of aclDataBuffer
 *
 * @retval data size
 */
ACL_FUNC_VISIBILITY size_t aclGetDataBufferSizeV2(const aclDataBuffer *dataBuffer);

/**
 * @ingroup AscendCL
 * @brief get size of aclDataType
 *
 * @param  dataType [IN]    aclDataType data the size to get
 *
 * @retval size of the aclDataType
 */
ACL_FUNC_VISIBILITY size_t aclDataTypeSize(aclDataType dataType);

// interfaces of tensor desc
/**
 * @ingroup AscendCL
 * @brief create data aclTensorDesc
 *
 * @param  dataType [IN]    Data types described by tensor
 * @param  numDims [IN]     the number of dimensions of the shape
 * @param  dims [IN]        the size of the specified dimension
 * @param  format [IN]      tensor format
 *
 * @retval aclTensorDesc pointer.
 * @retval nullptr if param is invalid or run out of memory
 */
ACL_FUNC_VISIBILITY aclTensorDesc *aclCreateTensorDesc(aclDataType dataType,
                                                       int numDims,
                                                       const int64_t *dims,
                                                       aclFormat format);

/**
 * @ingroup AscendCL
 * @brief destroy data aclTensorDesc
 *
 * @param desc [IN]     pointer to the data of aclTensorDesc to destroy
 */
ACL_FUNC_VISIBILITY void aclDestroyTensorDesc(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief set tensor shape range for aclTensorDesc
 *
 * @param  desc [OUT]     pointer to the data of aclTensorDesc
 * @param  dimsCount [IN]     the number of dimensions of the shape
 * @param  dimsRange [IN]     the range of dimensions of the shape
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorShapeRange(aclTensorDesc* desc,
                                                    size_t dimsCount,
                                                    int64_t dimsRange[][ACL_TENSOR_SHAPE_RANGE_NUM]);

/**
 * @ingroup AscendCL
 * @brief set value range for aclTensorDesc
 *
 * @param  desc [OUT]     pointer to the data of aclTensorDesc
 * @param  valueCount [IN]     the number of value
 * @param  valueRange [IN]     the range of value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorValueRange(aclTensorDesc* desc,
                                                    size_t valueCount,
                                                    int64_t valueRange[][ACL_TENSOR_VALUE_RANGE_NUM]);
/**
 * @ingroup AscendCL
 * @brief get data type specified by the tensor description
 *
 * @param desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval data type specified by the tensor description.
 * @retval ACL_DT_UNDEFINED if description is null
 */
ACL_FUNC_VISIBILITY aclDataType aclGetTensorDescType(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief get data format specified by the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval data format specified by the tensor description.
 * @retval ACL_FORMAT_UNDEFINED if description is null
 */
ACL_FUNC_VISIBILITY aclFormat aclGetTensorDescFormat(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief get tensor size specified by the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval data size specified by the tensor description.
 * @retval 0 if description is null
 */
ACL_FUNC_VISIBILITY size_t aclGetTensorDescSize(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief get element count specified by the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval element count specified by the tensor description.
 * @retval 0 if description is null
 */
ACL_FUNC_VISIBILITY size_t aclGetTensorDescElementCount(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief get number of dims specified by the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval number of dims specified by the tensor description.
 * @retval 0 if description is null
 * @retval ACL_UNKNOWN_RANK if the tensor dim is -2
 */
ACL_FUNC_VISIBILITY size_t aclGetTensorDescNumDims(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief Get the size of the specified dim in the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 * @param  index [IN]       index of dims, start from 0.
 *
 * @retval dim specified by the tensor description and index.
 * @retval -1 if description or index is invalid
 */
ACL_DEPRECATED_MESSAGE("aclGetTensorDescDim is deprecated, use aclGetTensorDescDimV2 instead")
ACL_FUNC_VISIBILITY int64_t aclGetTensorDescDim(const aclTensorDesc *desc, size_t index);

/**
 * @ingroup AscendCL
 * @brief Get the size of the specified dim in the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 * @param  index [IN]       index of dims, start from 0.
 * @param  dimSize [OUT]    size of the specified dim.
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetTensorDescDimV2(const aclTensorDesc *desc, size_t index, int64_t *dimSize);

/**
 * @ingroup AscendCL
 * @brief Get the range of the specified dim in the tensor description
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 * @param  index [IN]       index of dims, start from 0.
 * @param  dimRangeNum [IN]     number of dimRange.
 * @param  dimRange [OUT]       range of the specified dim.
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetTensorDescDimRange(const aclTensorDesc *desc,
                                                      size_t index,
                                                      size_t dimRangeNum,
                                                      int64_t *dimRange);

/**
 * @ingroup AscendCL
 * @brief set tensor description name
 *
 * @param desc [OUT]       pointer to the instance of aclTensorDesc
 * @param name [IN]        tensor description name
 */
ACL_FUNC_VISIBILITY void aclSetTensorDescName(aclTensorDesc *desc, const char *name);

/**
 * @ingroup AscendCL
 * @brief get tensor description name
 *
 * @param  desc [IN]        pointer to the instance of aclTensorDesc
 *
 * @retval tensor description name.
 * @retval empty string if description is null
 */
ACL_FUNC_VISIBILITY const char *aclGetTensorDescName(aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief Convert the format in the source aclTensorDesc according to
 * the specified dstFormat to generate a new target aclTensorDesc.
 * The format in the source aclTensorDesc remains unchanged.
 *
 * @param  srcDesc [IN]     pointer to the source tensor desc
 * @param  dstFormat [IN]   destination format
 * @param  dstDesc [OUT]    pointer to the pointer to the destination tensor desc
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclTransTensorDescFormat(const aclTensorDesc *srcDesc, aclFormat dstFormat,
    aclTensorDesc **dstDesc);

/**
 * @ingroup AscendCL
 * @brief Set the storage format specified by the tensor description
 *
 * @param  desc [OUT]     pointer to the instance of aclTensorDesc
 * @param  format [IN]    the storage format
 *
 * @retval ACL_SUCCESS    The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_DEPRECATED_MESSAGE("aclSetTensorStorageFormat is deprecated, use aclSetTensorFormat instead")
ACL_FUNC_VISIBILITY aclError aclSetTensorStorageFormat(aclTensorDesc *desc, aclFormat format);

/**
 * @ingroup AscendCL
 * @brief Set the storage shape specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  numDims [IN]    the number of dimensions of the shape
 * @param  dims [IN]       the size of the specified dimension
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_DEPRECATED_MESSAGE("aclSetTensorStorageShape is deprecated, use aclSetTensorShape instead")
ACL_FUNC_VISIBILITY aclError aclSetTensorStorageShape(aclTensorDesc *desc, int numDims, const int64_t *dims);

/**
 * @ingroup AscendCL
 * @brief Set the format specified by the tensor description
 *
 * @param  desc [OUT]     pointer to the instance of aclTensorDesc
 * @param  format [IN]    the storage format
 *
 * @retval ACL_SUCCESS    The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorFormat(aclTensorDesc *desc, aclFormat format);

/**
 * @ingroup AscendCL
 * @brief Set the shape specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  numDims [IN]    the number of dimensions of the shape
 * @param  dims [IN]       the size of the specified dimension
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorShape(aclTensorDesc *desc, int numDims, const int64_t *dims);

/**
 * @ingroup AscendCL
 * @brief Set the original format specified by the tensor description
 *
 * @param  desc [OUT]     pointer to the instance of aclTensorDesc
 * @param  format [IN]    the storage format
 *
 * @retval ACL_SUCCESS    The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorOriginFormat(aclTensorDesc *desc, aclFormat format);

/**
 * @ingroup AscendCL
 * @brief Set the original shape specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  numDims [IN]    the number of dimensions of the shape
 * @param  dims [IN]       the size of the specified dimension
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorOriginShape(aclTensorDesc *desc, int numDims, const int64_t *dims);

/**
 * @ingroup AscendCL
 * @brief get op description info
 *
 * @param desc [IN]     pointer to tensor description
 * @param index [IN]    index of tensor
 *
 * @retval null for failed.
 * @retval OtherValues success.
*/
ACL_FUNC_VISIBILITY aclTensorDesc *aclGetTensorDescByIndex(aclTensorDesc *desc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get address of tensor
 *
 * @param desc [IN]    pointer to tensor description
 *
 * @retval null for failed
 * @retval OtherValues success
*/
ACL_FUNC_VISIBILITY void *aclGetTensorDescAddress(const aclTensorDesc *desc);

/**
 * @ingroup AscendCL
 * @brief Set the dynamic input name specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  dynamicInputName [IN]       pointer to the dynamic input name
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorDynamicInput(aclTensorDesc *desc, const char *dynamicInputName);

/**
 * @ingroup AscendCL
 * @brief Set const data specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  dataBuffer [IN]       pointer to the const databuffer
 * @param  length [IN]       the length of const databuffer
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorConst(aclTensorDesc *desc, void *dataBuffer, size_t length);

/**
 * @ingroup AscendCL
 * @brief Set tensor memory type specified by the tensor description
 *
 * @param  desc [OUT]      pointer to the instance of aclTensorDesc
 * @param  memType [IN]       ACL_MEMTYPE_DEVICE means device, ACL_MEMTYPE_HOST or
 * ACL_MEMTYPE_HOST_COMPILE_INDEPENDENT means host
 *
 * @retval ACL_SUCCESS     The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetTensorPlaceMent(aclTensorDesc *desc, aclMemType memType);

/**
 * @ingroup AscendCL
 * @brief an interface for users to output  APP logs
 *
 * @param logLevel [IN]    the level of current log
 * @param func [IN]        the function where the log is located
 * @param file [IN]        the file where the log is located
 * @param line [IN]        Number of source lines where the log is located
 * @param fmt [IN]         the format of current log
 * @param ... [IN]         the value of current log
 */
ACL_FUNC_VISIBILITY void aclAppLog(aclLogLevel logLevel, const char *func, const char *file, uint32_t line,
    const char *fmt, ...);

/**
 * @ingroup AscendCL
 * @brief get soc name
 *
 * @retval null for failed
 * @retval OtherValues success
*/
ACL_FUNC_VISIBILITY const char *aclrtGetSocName();

#define ACL_APP_LOG(level, fmt, ...) \
    aclAppLog(level, __FUNCTION__, __FILE__, __LINE__, fmt, ##__VA_ARGS__)

/**
 * @ingroup AscendCL
 * @brief Get a list of the available CANN attributes in current environment
 *
 * @param  cannAttrList [OUT]  list of the available CANN attributes
 * @param  num [OUT]  the number of the available CANN attributes
 *
 * @retval ACL_SUCCESS  The function is successfully executed.
 * @retval OtherValues  Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetCannAttributeList(const aclCannAttr **cannAttrList, size_t *num);

/**
 * @ingroup AscendCL
 * @brief Check whether the specified CANN attribute is available in current
 * environment
 *
 * @param  cannAttr [IN]  CANN attributes to query
 * @param  num [OUT]  0/1: 0 represents unavailable , 1 available
 *
 * @retval ACL_SUCCESS  The function is successfully executed.
 * @retval OtherValues  Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetCannAttribute(aclCannAttr cannAttr, int32_t *value);

/**
 * @ingroup AscendCL
 * @brief Get capability value of the specified device
 *
 * @param  deviceId [IN]  device id
 * @param  deviceInfo [IN]  device capability to query
 * @param  value [OUT]    returned device capability value
 *
 * @retval ACL_SUCCESS  The function is successfully executed.
 * @retval OtherValues  Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetDeviceCapability(uint32_t deviceId, aclDeviceInfo deviceInfo, int64_t *value);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_BASE_H_
// End content from: acl/acl_base.h

// Begin content from: acl/acl_rt.h
/**
* @file acl_rt.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/

#ifndef INC_EXTERNAL_ACL_ACL_RT_H_
#define INC_EXTERNAL_ACL_ACL_RT_H_

#include <stdint.h>
#include <stddef.h>
// #include "acl_base.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACL_EVENT_SYNC                    0x00000001u
#define ACL_EVENT_CAPTURE_STREAM_PROGRESS 0x00000002u
#define ACL_EVENT_TIME_LINE               0x00000008u

#define ACL_STREAM_FAST_LAUNCH 0x00000001u
#define ACL_STREAM_FAST_SYNC   0x00000002u

#define ACL_CONTINUE_ON_FAILURE 0x00000000u
#define ACL_STOP_ON_FAILURE     0x00000001u

typedef enum aclrtRunMode {
    ACL_DEVICE,
    ACL_HOST,
} aclrtRunMode;

typedef enum aclrtTsId {
    ACL_TS_ID_AICORE   = 0,
    ACL_TS_ID_AIVECTOR = 1,
    ACL_TS_ID_RESERVED = 2,
} aclrtTsId;

typedef enum aclrtEventStatus {
    ACL_EVENT_STATUS_COMPLETE  = 0,
    ACL_EVENT_STATUS_NOT_READY = 1,
    ACL_EVENT_STATUS_RESERVED  = 2,
} aclrtEventStatus;

typedef enum aclrtEventRecordedStatus {
    ACL_EVENT_RECORDED_STATUS_NOT_READY = 0,
    ACL_EVENT_RECORDED_STATUS_COMPLETE = 1,
} aclrtEventRecordedStatus;

typedef enum aclrtEventWaitStatus {
    ACL_EVENT_WAIT_STATUS_COMPLETE  = 0,
    ACL_EVENT_WAIT_STATUS_NOT_READY = 1,
    ACL_EVENT_WAIT_STATUS_RESERVED  = 0xFFFF,
} aclrtEventWaitStatus;

typedef enum aclrtStreamStatus {
    ACL_STREAM_STATUS_COMPLETE  = 0,
    ACL_STREAM_STATUS_NOT_READY = 1,
    ACL_STREAM_STATUS_RESERVED  = 0xFFFF,
} aclrtStreamStatus;

typedef enum aclrtCallbackBlockType {
    ACL_CALLBACK_NO_BLOCK,
    ACL_CALLBACK_BLOCK,
} aclrtCallbackBlockType;

typedef enum aclrtMemcpyKind {
    ACL_MEMCPY_HOST_TO_HOST,
    ACL_MEMCPY_HOST_TO_DEVICE,
    ACL_MEMCPY_DEVICE_TO_HOST,
    ACL_MEMCPY_DEVICE_TO_DEVICE,
    ACL_MEMCPY_DEFAULT,
} aclrtMemcpyKind;

typedef enum aclrtMemMallocPolicy {
    ACL_MEM_MALLOC_HUGE_FIRST,
    ACL_MEM_MALLOC_HUGE_ONLY,
    ACL_MEM_MALLOC_NORMAL_ONLY,
    ACL_MEM_MALLOC_HUGE_FIRST_P2P,
    ACL_MEM_MALLOC_HUGE_ONLY_P2P,
    ACL_MEM_MALLOC_NORMAL_ONLY_P2P,
    ACL_MEM_TYPE_LOW_BAND_WIDTH   = 0x0100,
    ACL_MEM_TYPE_HIGH_BAND_WIDTH  = 0x1000,
} aclrtMemMallocPolicy;

typedef enum aclrtMemAttr {
    ACL_DDR_MEM,
    ACL_HBM_MEM,
    ACL_DDR_MEM_HUGE,
    ACL_DDR_MEM_NORMAL,
    ACL_HBM_MEM_HUGE,
    ACL_HBM_MEM_NORMAL,
    ACL_DDR_MEM_P2P_HUGE,
    ACL_DDR_MEM_P2P_NORMAL,
    ACL_HBM_MEM_P2P_HUGE,
    ACL_HBM_MEM_P2P_NORMAL,
} aclrtMemAttr;

typedef enum aclrtGroupAttr {
    ACL_GROUP_AICORE_INT,
    ACL_GROUP_AIV_INT,
    ACL_GROUP_AIC_INT,
    ACL_GROUP_SDMANUM_INT,
    ACL_GROUP_ASQNUM_INT,
    ACL_GROUP_GROUPID_INT
} aclrtGroupAttr;

typedef enum aclrtFloatOverflowMode {
    ACL_RT_OVERFLOW_MODE_SATURATION = 0,
    ACL_RT_OVERFLOW_MODE_INFNAN,
    ACL_RT_OVERFLOW_MODE_UNDEF,
} aclrtFloatOverflowMode;

typedef enum {
    ACL_RT_STREAM_WORK_ADDR_PTR = 0, /**< pointer to model work addr */
    ACL_RT_STREAM_WORK_SIZE, /**< pointer to model work size */
    ACL_RT_STREAM_FLAG,
    ACL_RT_STREAM_PRIORITY,
} aclrtStreamConfigAttr;

typedef struct aclrtStreamConfigHandle {
    void* workptr;
    size_t workSize;
    size_t flag;
    uint32_t priority;
} aclrtStreamConfigHandle;

typedef struct aclrtUtilizationExtendInfo aclrtUtilizationExtendInfo;

typedef struct aclrtUtilizationInfo {
    int32_t cubeUtilization;
    int32_t vectorUtilization;
    int32_t aicpuUtilization;
    int32_t memoryUtilization;
    aclrtUtilizationExtendInfo *utilizationExtend; /**< reserved parameters, current version needs to be null */
} aclrtUtilizationInfo;

typedef struct tagRtGroupInfo aclrtGroupInfo;

typedef struct rtExceptionInfo aclrtExceptionInfo;

typedef enum aclrtMemLocationType {
    ACL_MEM_LOCATION_TYPE_HOST = 0, /**< reserved enum, current version not support */
    ACL_MEM_LOCATION_TYPE_DEVICE,
} aclrtMemLocationType;

typedef struct aclrtMemLocation {
    uint32_t id;
    aclrtMemLocationType type;
} aclrtMemLocation;

typedef enum aclrtMemAllocationType {
    ACL_MEM_ALLOCATION_TYPE_PINNED = 0,
} aclrtMemAllocationType;

typedef enum aclrtMemHandleType {
    ACL_MEM_HANDLE_TYPE_NONE = 0,
} aclrtMemHandleType;

typedef struct aclrtPhysicalMemProp {
    aclrtMemHandleType handleType;
    aclrtMemAllocationType allocationType;
    aclrtMemAttr memAttr;
    aclrtMemLocation location;
    uint64_t reserve;
} aclrtPhysicalMemProp;

typedef enum aclrtMemGranularityOptions {
    ACL_RT_MEM_ALLOC_GRANULARITY_MINIMUM,
    ACL_RT_MEM_ALLOC_GRANULARITY_RECOMMENDED,
    ACL_RT_MEM_ALLOC_GRANULARITY_UNDEF = 0xFFFF,
} aclrtMemGranularityOptions;

typedef void* aclrtDrvMemHandle;

typedef void (*aclrtCallback)(void *userData);

typedef void (*aclrtExceptionInfoCallback)(aclrtExceptionInfo *exceptionInfo);

typedef enum aclrtDeviceStatus {
    ACL_RT_DEVICE_STATUS_NORMAL = 0,
    ACL_RT_DEVICE_STATUS_ABNORMAL,
    ACL_RT_DEVICE_STATUS_END = 0xFFFF,
} aclrtDeviceStatus;

typedef void* aclrtBinary;
typedef void* aclrtBinHandle;
typedef void* aclrtFuncHandle;

#define MAX_MEM_UCE_INFO_ARRAY_SIZE 128
#define UCE_INFO_RESERVED_SIZE 14

typedef struct aclrtMemUceInfo {
    void* addr;
    size_t len;
    size_t reserved[UCE_INFO_RESERVED_SIZE];
} aclrtMemUceInfo;

typedef enum aclrtCmoType {
    ACL_RT_CMO_TYPE_PREFETCH = 0,
} aclrtCmoType;

typedef enum aclrtLastErrLevel {
    ACL_RT_THREAD_LEVEL = 0,
} aclrtLastErrLevel;

/**
 * @ingroup AscendCL
 * @brief peek at last error by level
 *
 * @param level [IN] error level
 *
 * @retval Runtime error code
 */
ACL_FUNC_VISIBILITY aclError aclrtPeekAtLastError(aclrtLastErrLevel level);

/**
 * @ingroup AscendCL
 * @brief get last error by level
 *
 * @param level [IN] error level
 *
 * @retval Runtime error code
 */
ACL_FUNC_VISIBILITY aclError aclrtGetLastError(aclrtLastErrLevel level);


/**
 * @ingroup AscendCL
 * @brief Set a callback function to handle exception information
 *
 * @param callback [IN] callback function to handle exception information
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetExceptionInfoCallback(aclrtExceptionInfoCallback callback);

/**
 * @ingroup AscendCL
 * @brief Get task id from exception information
 *
 * @param info [IN]   pointer of exception information
 *
 * @retval The task id from exception information
 * @retval 0xFFFFFFFF if info is null
 */
ACL_FUNC_VISIBILITY uint32_t aclrtGetTaskIdFromExceptionInfo(const aclrtExceptionInfo *info);

/**
 * @ingroup AscendCL
 * @brief Get stream id from exception information
 *
 * @param info [IN]   pointer of exception information
 *
 * @retval The stream id from exception information
 * @retval 0xFFFFFFFF if info is null
 */
ACL_FUNC_VISIBILITY uint32_t aclrtGetStreamIdFromExceptionInfo(const aclrtExceptionInfo *info);

/**
 * @ingroup AscendCL
 * @brief Get thread id from exception information
 *
 * @param info [IN]   pointer of exception information
 *
 * @retval The thread id of fail task
 * @retval 0xFFFFFFFF if info is null
 */
ACL_FUNC_VISIBILITY uint32_t aclrtGetThreadIdFromExceptionInfo(const aclrtExceptionInfo *info);

/**
 * @ingroup AscendCL
 * @brief Get device id from exception information
 *
 * @param info [IN]   pointer of exception information
 *
 * @retval The thread id of fail task
 * @retval 0xFFFFFFFF if info is null
 */
ACL_FUNC_VISIBILITY uint32_t aclrtGetDeviceIdFromExceptionInfo(const aclrtExceptionInfo *info);

/**
 * @ingroup AscendCL
 * @brief Get error code from exception information
 *
 * @param info [IN]   pointer of exception information
 *
 * @retval The error code from exception information
 * @retval 0xFFFFFFFF if info is null
 */
ACL_FUNC_VISIBILITY uint32_t aclrtGetErrorCodeFromExceptionInfo(const aclrtExceptionInfo *info);

/**
 * @ingroup AscendCL
 * @brief The thread that handles the callback function on the Stream
 *
 * @param threadId [IN] thread ID
 * @param stream [IN]   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSubscribeReport(uint64_t threadId, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief Add a callback function to be executed on the host
 *        to the task queue of the Stream
 *
 * @param fn [IN]   Specify the callback function to be added
 *                  The function prototype of the callback function is:
 *                  typedef void (*aclrtCallback)(void *userData);
 * @param userData [IN]   User data to be passed to the callback function
 * @param blockType [IN]  callback block type
 * @param stream [IN]     stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtLaunchCallback(aclrtCallback fn, void *userData, aclrtCallbackBlockType blockType,
                                                 aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief After waiting for a specified time, trigger callback processing
 *
 * @par Function
 *  The thread processing callback specified by
 *  the aclrtSubscribeReport interface
 *
 * @param timeout [IN]   timeout value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSubscribeReport
 */
ACL_FUNC_VISIBILITY aclError aclrtProcessReport(int32_t timeout);

/**
 * @ingroup AscendCL
 * @brief Cancel thread registration,
 *        the callback function on the specified Stream
 *        is no longer processed by the specified thread
 *
 * @param threadId [IN]   thread ID
 * @param stream [IN]     stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtUnSubscribeReport(uint64_t threadId, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create context and associates it with the calling thread
 *
 * @par Function
 * The following use cases are supported:
 * @li If you don't call the aclrtCreateContext interface
 * to explicitly create the context,
 * the system will use the default context, which is implicitly created
 * when the aclrtSetDevice interface is called.
 * @li If multiple contexts are created in a process
 * (there is no limit on the number of contexts),
 * the current thread can only use one of them at the same time.
 * It is recommended to explicitly specify the context of the current thread
 * through the aclrtSetCurrentContext interface to increase.
 * the maintainability of the program.
 *
 * @param  context [OUT]    point to the created context
 * @param  deviceId [IN]    device to create context on
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSetDevice | aclrtSetCurrentContext
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateContext(aclrtContext *context, int32_t deviceId);

/**
 * @ingroup AscendCL
 * @brief destroy context instance
 *
 * @par Function
 * Can only destroy context created through aclrtCreateContext interface
 *
 * @param  context [IN]   the context to destroy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateContext
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyContext(aclrtContext context);

/**
 * @ingroup AscendCL
 * @brief set the context of the thread
 *
 * @par Function
 * The following scenarios are supported:
 * @li If the aclrtCreateContext interface is called in a thread to explicitly
 * create a Context (for example: ctx1), the thread's Context can be specified
 * without calling the aclrtSetCurrentContext interface.
 * The system uses ctx1 as the context of thread1 by default.
 * @li If the aclrtCreateContext interface is not explicitly created,
 * the system uses the default context as the context of the thread.
 * At this time, the aclrtDestroyContext interface cannot be used to release
 * the default context.
 * @li If the aclrtSetCurrentContext interface is called multiple times to
 * set the thread's Context, the last one prevails.
 *
 * @par Restriction
 * @li If the cevice corresponding to the context set for the thread
 * has been reset, you cannot set the context as the context of the thread,
 * otherwise a business exception will result.
 * @li It is recommended to use the context created in a thread.
 * If the aclrtCreateContext interface is called in thread A to create a context,
 * and the context is used in thread B,
 * the user must guarantee the execution order of tasks in the same stream
 * under the same context in two threads.
 *
 * @param  context [IN]   the current context of the thread
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateContext | aclrtDestroyContext
 */
ACL_FUNC_VISIBILITY aclError aclrtSetCurrentContext(aclrtContext context);

/**
 * @ingroup AscendCL
 * @brief get the context of the thread
 *
 * @par Function
 * If the user calls the aclrtSetCurrentContext interface
 * multiple times to set the context of the current thread,
 * then the last set context is obtained
 *
 * @param  context [OUT]   the current context of the thread
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSetCurrentContext
 */
ACL_FUNC_VISIBILITY aclError aclrtGetCurrentContext(aclrtContext *context);

/**
 * @ingroup AscendCL
 * @brief get system param option value in current context
 *
 * @param opt[IN] system option
 * @param value[OUT] value of system option
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclrtCtxGetSysParamOpt(aclSysParamOpt opt, int64_t *value);

/**
 * @ingroup AscendCL
 * @brief set system param option value in current context
 *
 * @param opt[IN] system option
 * @param value[IN] value of system option
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclrtCtxSetSysParamOpt(aclSysParamOpt opt, int64_t value);

/**
 * @ingroup AscendCL
 * @brief get system param option value in current process
 *
 * @param opt[IN] system option
 * @param value[OUT] value of system option
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclrtGetSysParamOpt(aclSysParamOpt opt, int64_t *value);

/**
 * @ingroup AscendCL
 * @brief set system param option value in current process
 *
 * @param opt[IN] system option
 * @param value[IN] value of system option
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclrtSetSysParamOpt(aclSysParamOpt opt, int64_t value);

/**
 * @ingroup AscendCL
 * @brief Specify the device to use for the operation
 * implicitly create the default context and the default stream
 *
 * @par Function
 * The following use cases are supported:
 * @li Device can be specified in the process or thread.
 * If you call the aclrtSetDevice interface multiple
 * times to specify the same device,
 * you only need to call the aclrtResetDevice interface to reset the device.
 * @li The same device can be specified for operation
 *  in different processes or threads.
 * @li Device is specified in a process,
 * and multiple threads in the process can share this device to explicitly
 * create a Context (aclrtCreateContext interface).
 * @li In multi-device scenarios, you can switch to other devices
 * through the aclrtSetDevice interface in the process.
 *
 * @param  deviceId [IN]  the device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtResetDevice |aclrtCreateContext
 */
ACL_FUNC_VISIBILITY aclError aclrtSetDevice(int32_t deviceId);

/**
 * @ingroup AscendCL
 * @brief Reset the current operating Device and free resources on the device,
 * including the default context, the default stream,
 * and all streams created under the default context,
 * and synchronizes the interface.
 * If the task under the default context or stream has not been completed,
 * the system will wait for the task to complete before releasing it.
 *
 * @par Restriction
 * @li The Context, Stream, and Event that are explicitly created
 * on the device to be reset. Before resetting,
 * it is recommended to follow the following interface calling sequence,
 * otherwise business abnormalities may be caused.
 * @li Interface calling sequence:
 * call aclrtDestroyEvent interface to release Event or
 * call aclrtDestroyStream interface to release explicitly created Stream->
 * call aclrtDestroyContext to release explicitly created Context->
 * call aclrtResetDevice interface
 *
 * @param  deviceId [IN]   the device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtResetDevice(int32_t deviceId);

/**
 * @ingroup AscendCL
 * @brief Reset the current operating Device and free resources on the device by FORCE,
 * including the default context, the default stream,
 * and all streams created under the default context,
 * and synchronizes the interface.
 * If the task under the default context or stream has not been completed,
 * the system will wait for the task to complete before releasing it.
 * No matter how many times you call aclrtSetDevice for the same device id,
 * you only need to call aclrtResetDeviceForce once for resetting.
 *
 * @par Restriction
 * @li The Context, Stream, and Event that are explicitly created
 * on the device to be reset. Before resetting,
 * it is recommended to follow the following interface calling sequence,
 * otherwise business abnormalities may be caused.
 * @li Interface calling sequence:
 * call aclrtDestroyEvent interface to release Event or
 * call aclrtDestroyStream interface to release explicitly created Stream->
 * call aclrtDestroyContext to release explicitly created Context->
 * call aclrtResetDeviceForce interface
 *
 * @param  deviceId [IN]   the device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 * 
 * @see aclrtResetDevice
 */
ACL_FUNC_VISIBILITY aclError aclrtResetDeviceForce(int32_t deviceId);

/**
 * @ingroup AscendCL
 * @brief get target device of current thread
 *
 * @param deviceId [OUT]  the device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetDevice(int32_t *deviceId);

/**
 * @ingroup AscendCL
 * @brief set stream failure mode
 *
 * @param stream [IN]  the stream to set
 * @param mode [IN]  stream failure mode
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetStreamFailureMode(aclrtStream stream, uint64_t mode);

/**
 * @ingroup AscendCL
 * @brief get target side
 *
 * @param runMode [OUT]    the run mode
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetRunMode(aclrtRunMode *runMode);

/**
 * @ingroup AscendCL
 * @brief Wait for compute device to finish
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeDevice(void);

/**
 * @ingroup AscendCL
 * @brief Wait for compute device to finish and set timeout
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeDeviceWithTimeout(int32_t timeout);

/**
 * @ingroup AscendCL
 * @brief Set Scheduling TS
 *
 * @param tsId [IN]   the ts id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetTsDevice(aclrtTsId tsId);

/**
 * @ingroup AscendCL
 * @brief Query the comprehensive usage rate of device
 * @param deviceId [IN] the need query's deviceId
 * @param utilizationInfo [OUT] the usage rate of device
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetDeviceUtilizationRate(int32_t deviceId, aclrtUtilizationInfo *utilizationInfo);

/**
 * @ingroup AscendCL
 * @brief get total device number.
 *
 * @param count [OUT]    the device number
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetDeviceCount(uint32_t *count);

/**
 * @ingroup AscendCL
 * @brief create event instance
 *
 * @param event [OUT]   created event
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateEvent(aclrtEvent *event);

/**
 * @ingroup AscendCL
 * @brief create event instance with flag
 *
 * @param event [OUT]   created event
 * @param flag [IN]     event flag
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateEventWithFlag(aclrtEvent *event, uint32_t flag);

/**
 * @ingroup AscendCL
 * @brief create event instance with flag, event can be reused naturally
 *
 * @param event [OUT]   created event
 * @param flag [IN]     event flag
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateEventExWithFlag(aclrtEvent *event, uint32_t flag);

/**
 * @ingroup AscendCL
 * @brief destroy event instance
 *
 * @par Function
 *  Only events created through the aclrtCreateEvent interface can be
 *  destroyed, synchronous interfaces. When destroying an event,
 *  the user must ensure that the tasks involved in the aclrtSynchronizeEvent
 *  interface or the aclrtStreamWaitEvent interface are completed before
 *  they are destroyed.
 *
 * @param  event [IN]   event to destroy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateEvent | aclrtSynchronizeEvent | aclrtStreamWaitEvent
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyEvent(aclrtEvent event);

/**
 * @ingroup AscendCL
 * @brief Record an Event in the Stream
 *
 * @param event [IN]    event to record
 * @param stream [IN]   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtRecordEvent(aclrtEvent event, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief Reset an event
 *
 * @par Function
 *  Users need to make sure to wait for the tasks in the Stream
 *  to complete before resetting the Event
 *
 * @param event [IN]    event to reset
 * @param stream [IN]   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtResetEvent(aclrtEvent event, aclrtStream stream);

 /**
 * @ingroup AscendCL
 * @brief Queries an event's status
 *
 * @param  event [IN]    event to query
 * @param  status [OUT]  event status
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_DEPRECATED_MESSAGE("aclrtQueryEvent is deprecated, use aclrtQueryEventStatus instead")
ACL_FUNC_VISIBILITY aclError aclrtQueryEvent(aclrtEvent event, aclrtEventStatus *status);

/**
 * @ingroup AscendCL
 * @brief Queries an event's status
 *
 * @param  event [IN]    event to query
 * @param  status [OUT]  event recorded status
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtQueryEventStatus(aclrtEvent event, aclrtEventRecordedStatus *status);

/**
* @ingroup AscendCL
* @brief Queries an event's wait-status
*
* @param  event [IN]    event to query
* @param  status [OUT]  event wait-status
*
* @retval ACL_SUCCESS The function is successfully executed.
* @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclrtQueryEventWaitStatus(aclrtEvent event, aclrtEventWaitStatus *status);

/**
 * @ingroup AscendCL
 * @brief Block Host Running, wait event to be complete
 *
 * @param  event [IN]   event to wait
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeEvent(aclrtEvent event);

/**
 * @ingroup AscendCL
 * @brief Block Host Running, wait event to be complete
 *
 * @param  event [IN]   event to wait
 * @param  timeout [IN]  timeout value,the unit is milliseconds
 * -1 means waiting indefinitely, 0 means check whether synchronization is immediately completed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeEventWithTimeout(aclrtEvent event, int32_t timeout);

/**
 * @ingroup AscendCL
 * @brief computes the elapsed time between events.
 *
 * @param ms [OUT]     time between start and end in ms
 * @param start [IN]   starting event
 * @param end [IN]     ending event
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateEvent | aclrtRecordEvent | aclrtSynchronizeStream
 */
ACL_FUNC_VISIBILITY aclError aclrtEventElapsedTime(float *ms, aclrtEvent startEvent, aclrtEvent endEvent);

/**
 * @ingroup AscendCL
 * @brief alloc memory on device, real alloc size is aligned to 32 bytes and padded with 32 bytes
 *
 * @par Function
 *  alloc for size linear memory on device
 *  and return a pointer to allocated memory by *devPtr
 *
 * @par Restriction
 * @li The memory requested by the aclrtMalloc interface needs to be released
 * through the aclrtFree interface.
 * @li Before calling the media data processing interface,
 * if you need to apply memory on the device to store input or output data,
 * you need to call acldvppMalloc to apply for memory.
 *
 * @param devPtr [OUT]  pointer to pointer to allocated memory on device
 * @param size [IN]     alloc memory size
 * @param policy [IN]   memory alloc policy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtFree | acldvppMalloc | aclrtMallocCached
 */
ACL_FUNC_VISIBILITY aclError aclrtMalloc(void **devPtr,
                                         size_t size,
                                         aclrtMemMallocPolicy policy);

/**
 * @ingroup AscendCL
 * @brief alloc memory on device, real alloc size is aligned to 32 bytes with no padding
 *
 * @par Function
 *  alloc for size linear memory on device
 *  and return a pointer to allocated memory by *devPtr
 *
 * @par Restriction
 * @li The memory requested by the aclrtMallocAlign32 interface needs to be released
 * through the aclrtFree interface.
 *
 * @param devPtr [OUT]  pointer to pointer to allocated memory on device
 * @param size [IN]     alloc memory size
 * @param policy [IN]   memory alloc policy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtFree | aclrtMalloc | aclrtMallocCached
 */
ACL_FUNC_VISIBILITY aclError aclrtMallocAlign32(void **devPtr,
                                                size_t size,
                                                aclrtMemMallocPolicy policy);

/**
 * @ingroup AscendCL
 * @brief allocate memory on device with cache
 *
 * @par Function
 *  alloc for size linear memory on device
 *  and return a pointer to allocated memory by *devPtr
 *
 * @par Restriction
 * @li The memory requested by the aclrtMallocCached interface needs to be released
 * through the aclrtFree interface.
 *
 * @param devPtr [OUT]  pointer to pointer to allocated memory on device
 * @param size [IN]     alloc memory size
 * @param policy [IN]   memory alloc policy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtFree | aclrtMalloc
 */
ACL_FUNC_VISIBILITY aclError aclrtMallocCached(void **devPtr,
                                               size_t size,
                                               aclrtMemMallocPolicy policy);

/**
 * @ingroup AscendCL
 * @brief flush cache data to ddr
 *
 * @param devPtr [IN]  the pointer that flush data to ddr
 * @param size [IN]    flush size
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemFlush(void *devPtr, size_t size);

/**
 * @ingroup AscendCL
 * @brief invalidate cache data
 *
 * @param devPtr [IN]  pointer to invalidate cache data
 * @param size [IN]    invalidate size
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemInvalidate(void *devPtr, size_t size);

/**
 * @ingroup AscendCL
 * @brief free device memory
 *
 * @par Function
 *  can only free memory allocated through the aclrtMalloc interface
 *
 * @param  devPtr [IN]  Pointer to memory to be freed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtMalloc
 */
ACL_FUNC_VISIBILITY aclError aclrtFree(void *devPtr);

/**
 * @ingroup AscendCL
 * @brief alloc memory on host
 *
 * @par Restriction
 * @li The requested memory cannot be used in the Device
 * and needs to be explicitly copied to the Device.
 * @li The memory requested by the aclrtMallocHost interface
 * needs to be released through the aclrtFreeHost interface.
 *
 * @param  hostPtr [OUT] pointer to pointer to allocated memory on the host
 * @param  size [IN]     alloc memory size
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtFreeHost
 */
ACL_FUNC_VISIBILITY aclError aclrtMallocHost(void **hostPtr, size_t size);

/**
 * @ingroup AscendCL
 * @brief free host memory
 *
 * @par Function
 *  can only free memory allocated through the aclrtMallocHost interface
 *
 * @param  hostPtr [IN]   free memory pointer
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtMallocHost
 */
ACL_FUNC_VISIBILITY aclError aclrtFreeHost(void *hostPtr);

/**
 * @ingroup AscendCL
 * @brief synchronous memory replication between host and device
 *
 * @param dst [IN]       destination address pointer
 * @param destMax [IN]   Max length of the destination address memory
 * @param src [IN]       source address pointer
 * @param count [IN]     the length of byte to copy
 * @param kind [IN]      memcpy type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemcpy(void *dst,
                                         size_t destMax,
                                         const void *src,
                                         size_t count,
                                         aclrtMemcpyKind kind);

/**
 * @ingroup AscendCL
 * @brief Initialize memory and set contents of memory to specified value
 *
 * @par Function
 *  The memory to be initialized is on the Host or device side,
 *  and the system determines whether
 *  it is host or device according to the address
 *
 * @param devPtr [IN]    Starting address of memory
 * @param maxCount [IN]  Max length of destination address memory
 * @param value [IN]     Set value
 * @param count [IN]     The length of memory
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemset(void *devPtr, size_t maxCount, int32_t value, size_t count);

/**
 * @ingroup AscendCL
 * @brief  Asynchronous memory replication between Host and Device
 *
 * @par Function
 *  After calling this interface,
 *  be sure to call the aclrtSynchronizeStream interface to ensure that
 *  the task of memory replication has been completed
 *
 * @par Restriction
 * @li For on-chip Device-to-Device memory copy,
 *     both the source and destination addresses must be 64-byte aligned
 *
 * @param dst [IN]     destination address pointer
 * @param destMax [IN] Max length of destination address memory
 * @param src [IN]     source address pointer
 * @param count [IN]   the number of byte to copy
 * @param kind [IN]    memcpy type
 * @param stream [IN]  asynchronized task stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSynchronizeStream
 */
ACL_FUNC_VISIBILITY aclError aclrtMemcpyAsync(void *dst,
                                              size_t destMax,
                                              const void *src,
                                              size_t count,
                                              aclrtMemcpyKind kind,
                                              aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief  Asynchronous memory replication between Host and Device, would
 *         be synchronous if memory is not allocated via calling acl or rts api.
 *
 * @par Function
 *  After calling this interface and memory is allocated via calling acl or rts api,
 *  be sure to call the aclrtSynchronizeStream interface to ensure that
 *  the task of memory replication has been completed
 *
 * @par Restriction
 * @li For on-chip Device-to-Device memory copy,
 *     both the source and destination addresses must be 64-byte aligned
 *
 * @param dst [IN]     destination address pointer
 * @param destMax [IN] Max length of destination address memory
 * @param src [IN]     source address pointer
 * @param count [IN]   the number of byte to copy
 * @param kind [IN]    memcpy type
 * @param stream [IN]  asynchronized task stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSynchronizeStream
 */
ACL_FUNC_VISIBILITY aclError aclrtMemcpyAsyncWithCondition(void *dst,
                                                           size_t destMax,
                                                           const void *src,
                                                           size_t count,
                                                           aclrtMemcpyKind kind,
                                                           aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief synchronous memory replication of two-dimensional matrix between host and device
 *
 * @param dst [IN]       destination address pointer
 * @param dpitch [IN]    pitch of destination memory
 * @param src [IN]       source address pointer
 * @param spitch [IN]    pitch of source memory
 * @param width [IN]     width of matrix transfer
 * @param height [IN]    height of matrix transfer
 * @param kind [IN]      memcpy type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemcpy2d(void *dst,
                                           size_t dpitch,
                                           const void *src,
                                           size_t spitch,
                                           size_t width,
                                           size_t height,
                                           aclrtMemcpyKind kind);

/**
 * @ingroup AscendCL
 * @brief asynchronous memory replication of two-dimensional matrix between host and device
 *
 * @param dst [IN]       destination address pointer
 * @param dpitch [IN]    pitch of destination memory
 * @param src [IN]       source address pointer
 * @param spitch [IN]    pitch of source memory
 * @param width [IN]     width of matrix transfer
 * @param height [IN]    height of matrix transfer
 * @param kind [IN]      memcpy type
 * @param stream [IN]    asynchronized task stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemcpy2dAsync(void *dst,
                                                size_t dpitch,
                                                const void *src,
                                                size_t spitch,
                                                size_t width,
                                                size_t height,
                                                aclrtMemcpyKind kind,
                                                aclrtStream stream);

/**
* @ingroup AscendCL
* @brief Asynchronous initialize memory
* and set contents of memory to specified value async
*
* @par Function
 *  The memory to be initialized is on the Host or device side,
 *  and the system determines whether
 *  it is host or device according to the address
 *
* @param devPtr [IN]      destination address pointer
* @param maxCount [IN]    Max length of destination address memory
* @param value [IN]       set value
* @param count [IN]       the number of byte to set
* @param stream [IN]      asynchronized task stream
*
* @retval ACL_SUCCESS The function is successfully executed.
* @retval OtherValues Failure
*
* @see aclrtSynchronizeStream
*/
ACL_FUNC_VISIBILITY aclError aclrtMemsetAsync(void *devPtr,
                                              size_t maxCount,
                                              int32_t value,
                                              size_t count,
                                              aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief Allocate an address range reservation
 *
 * @param virPtr [OUT]    Resulting pointer to start of virtual address range allocated
 * @param size [IN]       Size of the reserved virtual address range requested
 * @param alignment [IN]  Alignment of the reserved virtual address range requested
 * @param expectPtr [IN]  Fixed starting address range requested, must be nullptr
 * @param flags [IN]      Flag of page type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtReleaseMemAddress | aclrtMallocPhysical | aclrtMapMem
 */
ACL_FUNC_VISIBILITY aclError aclrtReserveMemAddress(void **virPtr,
                                                    size_t size,
                                                    size_t alignment,
                                                    void *expectPtr,
                                                    uint64_t flags);

/**
 * @ingroup AscendCL
 * @brief Free an address range reservation
 *
 * @param virPtr [IN]  Starting address of the virtual address range to free
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtReserveMemAddress
 */
ACL_FUNC_VISIBILITY aclError aclrtReleaseMemAddress(void *virPtr);

/**
 * @ingroup AscendCL
 * @brief Create a memory handle representing a memory allocation of a given
 * size described by the given properties
 *
 * @param handle [OUT]  Value of handle returned. All operations on this
 * allocation are to be performed using this handle.
 * @param size [IN]     Size of the allocation requested
 * @param prop [IN]     Properties of the allocation to create
 * @param flags [IN]    Currently unused, must be zero
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtFreePhysical | aclrtReserveMemAddress | aclrtMapMem
 */
ACL_FUNC_VISIBILITY aclError aclrtMallocPhysical(aclrtDrvMemHandle *handle,
                                                 size_t size,
                                                 const aclrtPhysicalMemProp *prop,
                                                 uint64_t flags);

/**
 * @ingroup AscendCL
 * @brief Release a memory handle representing a memory allocation which was
 * previously allocated through aclrtMallocPhysical
 *
 * @param handle [IN]  Value of handle which was returned previously by aclrtMallocPhysical
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtMallocPhysical
 */
ACL_FUNC_VISIBILITY aclError aclrtFreePhysical(aclrtDrvMemHandle handle);

/**
 * @ingroup AscendCL
 * @brief Maps an allocation handle to a reserved virtual address range
 *
 * @param virPtr [IN]  Address where memory will be mapped
 * @param size [IN]    Size of the memory mapping
 * @param offset [IN]  Offset into the memory represented by handle from which to start mapping
 * @param handle [IN]  Handle to a shareable memory
 * @param flags [IN]   Currently unused, must be zero
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtUnmapMem | aclrtReserveMemAddress | aclrtMallocPhysical
 */
ACL_FUNC_VISIBILITY aclError aclrtMapMem(void *virPtr,
                                         size_t size,
                                         size_t offset,
                                         aclrtDrvMemHandle handle,
                                         uint64_t flags);

/**
 * @ingroup AscendCL
 * @brief Unmap the backing memory of a given address range
 *
 * @param virPtr [IN]  Starting address for the virtual address range to unmap
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtMapMem
 */
ACL_FUNC_VISIBILITY aclError aclrtUnmapMem(void *virPtr);

/**
 * @ingroup AscendCL
 * @brief Create config handle of stream
 *
 * @retval the aclrtStreamConfigHandle pointer
 */
ACL_FUNC_VISIBILITY aclrtStreamConfigHandle *aclrtCreateStreamConfigHandle(void);

/**
 * @ingroup AscendCL
 * @brief Destroy config handle of model execute
 *
 * @param  handle [IN]  Pointer to aclrtStreamConfigHandle to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyStreamConfigHandle(aclrtStreamConfigHandle *handle);

/**
 * @ingroup AscendCL
 * @brief set config for stream
 *
 * @param handle [OUT]    pointer to stream config handle
 * @param attr [IN]       config attr in stream config handle to be set
 * @param attrValue [IN]  pointer to stream config value
 * @param valueSize [IN]  memory size of attrValue
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetStreamConfigOpt(aclrtStreamConfigHandle *handle, aclrtStreamConfigAttr attr,
    const void *attrValue, size_t valueSize);

/**
 * @ingroup AscendCL
 * @brief  create stream instance
 *
 * @param  stream [OUT]   the created stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateStream(aclrtStream *stream);

/**
 * @ingroup AscendCL
 * @brief  create stream instance
 *
 * @param  stream [OUT]   the created stream
 * @param  handle [IN]   the config of stream
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateStreamV2(aclrtStream *stream, const aclrtStreamConfigHandle *handle);

/**
 * @ingroup AscendCL
 * @brief  create stream instance with param
 *
 * @par Function
 * Can create fast streams through the aclrtCreateStreamWithConfig interface
 *
 * @param  stream [OUT]   the created stream
 * @param  priority [IN]   the priority of stream, value range:0~7
 * @param  flag [IN]   indicate the function for stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCreateStreamWithConfig(aclrtStream *stream, uint32_t priority, uint32_t flag);

/**
 * @ingroup AscendCL
 * @brief destroy stream instance
 *
 * @par Function
 * Can only destroy streams created through the aclrtCreateStream interface
 *
 * @par Restriction
 * Before calling the aclrtDestroyStream interface to destroy
 * the specified Stream, you need to call the aclrtSynchronizeStream interface
 * to ensure that the tasks in the Stream have been completed.
 *
 * @param stream [IN]  the stream to destroy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateStream | aclrtSynchronizeStream
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyStream(aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief destroy stream instance by force
 *
 * @par Function
 * Can only destroy streams created through the aclrtCreateStream interface
 *
 * @param stream [IN]  the stream to destroy
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateStream
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyStreamForce(aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief block the host until all tasks
 * in the specified stream have completed
 *
 * @param  stream [IN]   the stream to wait
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeStream(aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief block the host until all tasks
 * in the specified stream have completed
 *
 * @param  stream [IN]   the stream to wait
 * @param  timeout [IN]  timeout value,the unit is milliseconds
 * -1 means waiting indefinitely, 0 means check whether synchronization is complete immediately
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSynchronizeStreamWithTimeout(aclrtStream stream, int32_t timeout);

/**
 * @ingroup AscendCL
 * @brief Query a stream for completion status.
 *
 * @param  stream [IN]   the stream to query
 * @param  status [OUT]  stream status
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtStreamQuery(aclrtStream stream, aclrtStreamStatus *status);

/**
 * @ingroup AscendCL
 * @brief Blocks the operation of the specified Stream until
 * the specified Event is completed.
 * Support for multiple streams waiting for the same event.
 *
 * @param  stream [IN]   the wait stream If using thedefault Stream, set NULL
 * @param  event [IN]    the event to wait
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtStreamWaitEvent(aclrtStream stream, aclrtEvent event);

/**
 * @ingroup AscendCL
 * @brief set group
 *
 * @par Function
 *  set the task to the corresponding group
 *
 * @param groupId [IN]   group id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtGetGroupCount | aclrtGetAllGroupInfo | aclrtGetGroupInfoDetail
 */
ACL_FUNC_VISIBILITY aclError aclrtSetGroup(int32_t groupId);

/**
 * @ingroup AscendCL
 * @brief get the number of group
 *
 * @par Function
 *  get the number of group. if the number of group is zero,
 *  it means that group is not supported or group is not created.
 *
 * @param count [OUT]   the number of group
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 */
ACL_FUNC_VISIBILITY aclError aclrtGetGroupCount(uint32_t *count);

/**
 * @ingroup AscendCL
 * @brief create group information
 *
 * @retval null for failed.
 * @retval OtherValues success.
 *
 * @see aclrtDestroyGroupInfo
 */
ACL_FUNC_VISIBILITY aclrtGroupInfo *aclrtCreateGroupInfo();

/**
 * @ingroup AscendCL
 * @brief destroy group information
 *
 * @param groupInfo [IN]   pointer to group information
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtCreateGroupInfo
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyGroupInfo(aclrtGroupInfo *groupInfo);

/**
 * @ingroup AscendCL
 * @brief get all group information
 *
 * @param groupInfo [OUT]   pointer to group information
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtGetGroupCount
 */
ACL_FUNC_VISIBILITY aclError aclrtGetAllGroupInfo(aclrtGroupInfo *groupInfo);

/**
 * @ingroup AscendCL
 * @brief get detail information of group
 *
 * @param groupInfo [IN]    pointer to group information
 * @param groupIndex [IN]   group index value
 * @param attr [IN]         group attribute
 * @param attrValue [OUT]   pointer to attribute value
 * @param valueLen [IN]     length of attribute value
 * @param paramRetSize [OUT]   pointer to real length of attribute value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtGetGroupCount | aclrtGetAllGroupInfo
 */
ACL_FUNC_VISIBILITY aclError aclrtGetGroupInfoDetail(const aclrtGroupInfo *groupInfo,
                                                     int32_t groupIndex,
                                                     aclrtGroupAttr attr,
                                                     void *attrValue,
                                                     size_t valueLen,
                                                     size_t *paramRetSize);

/**
 * @ingroup AscendCL
 * @brief checking whether current device and peer device support the p2p feature
 *
 * @param canAccessPeer [OUT]   pointer to save the checking result
 * @param deviceId [IN]         current device id
 * @param peerDeviceId [IN]     peer device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtDeviceEnablePeerAccess | aclrtDeviceDisablePeerAccess
 */
ACL_FUNC_VISIBILITY aclError aclrtDeviceCanAccessPeer(int32_t *canAccessPeer, int32_t deviceId, int32_t peerDeviceId);

/**
 * @ingroup AscendCL
 * @brief enable the peer device to support the p2p feature
 *
 * @param peerDeviceId [IN]   the peer device id
 * @param flags [IN]   reserved field, now it must be zero
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtDeviceCanAccessPeer | aclrtDeviceDisablePeerAccess
 */
ACL_FUNC_VISIBILITY aclError aclrtDeviceEnablePeerAccess(int32_t peerDeviceId, uint32_t flags);

/**
 * @ingroup AscendCL
 * @brief disable the peer device to support the p2p function
 *
 * @param peerDeviceId [IN]   the peer device id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtDeviceCanAccessPeer | aclrtDeviceEnablePeerAccess
 */
ACL_FUNC_VISIBILITY aclError aclrtDeviceDisablePeerAccess(int32_t peerDeviceId);

/**
 * @ingroup AscendCL
 * @brief Obtain the free memory and total memory of specified attribute.
 * the specified memory include normal memory and huge memory.
 *
 * @param attr [IN]    the memory attribute of specified device
 * @param free [OUT]   the free memory of specified device
 * @param total [OUT]  the total memory of specified device.
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetMemInfo(aclrtMemAttr attr, size_t *free, size_t *total);

/**
 * @ingroup AscendCL
 * @brief Set the timeout interval for waitting of op
 *
 * @param timeout [IN]   op wait timeout
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetOpWaitTimeout(uint32_t timeout);

/**
 * @ingroup AscendCL
 * @brief Set the timeout interval for op executing
 *
 * @param timeout [IN]   op execute timeout
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetOpExecuteTimeOut(uint32_t timeout);

/**
 * @ingroup AscendCL
 * @brief enable or disable overflow switch on some stream
 * @param stream [IN]   set overflow switch on this stream
 * @param flag [IN]  0 : disable 1 : enable
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetStreamOverflowSwitch(aclrtStream stream, uint32_t flag);

/**
 * @ingroup AscendCL
 * @brief get overflow switch on some stream
 * @param stream [IN]   get overflow switch on this stream
 * @param flag [OUT]  current overflow switch, 0 : disable others : enable
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetStreamOverflowSwitch(aclrtStream stream, uint32_t *flag);

/**
 * @ingroup AscendCL
 * @brief set saturation mode
 * @param mode [IN]   target saturation mode
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSetDeviceSatMode(aclrtFloatOverflowMode mode);

/**
 * @ingroup AscendCL
 * @brief get saturation mode
 * @param mode [OUT]   get saturation mode
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetDeviceSatMode(aclrtFloatOverflowMode *mode);

/**
 * @ingroup AscendCL
 * @brief get overflow status asynchronously
 *
 * @par Restriction
 * After calling the aclrtGetOverflowStatus interface,
 * you need to call the aclrtSynchronizeStream interface
 * to ensure that the tasks in the stream have been completed.
 * @param outputAddr [IN/OUT]  output device addr to store overflow status
 * @param outputSize [IN]  output addr size
 * @param outputSize [IN]  stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetOverflowStatus(void *outputAddr, size_t outputSize, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief reset overflow status asynchronously
 *
 * @par Restriction
 * After calling the aclrtResetOverflowStatus interface,
 * you need to call the aclrtSynchronizeStream interface
 * to ensure that the tasks in the stream have been completed.
 * @param outputSize [IN]  stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtResetOverflowStatus(aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief The thread that handles the hostFunc function on the Stream
 *
 * @param hostFuncThreadId [IN] thread ID
 * @param exeStream        [IN] stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtSubscribeHostFunc(uint64_t hostFuncThreadId, aclrtStream exeStream);

/**
 * @ingroup AscendCL
 * @brief After waiting for a specified time, trigger hostFunc callback function processing
 *
 * @par Function
 *  The thread processing callback specified by the aclrtSubscribeHostFunc interface
 *
 * @param timeout [IN]   timeout value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclrtSubscribeHostFunc
 */
ACL_FUNC_VISIBILITY aclError aclrtProcessHostFunc(int32_t timeout);

/**
 * @ingroup AscendCL
 * @brief Cancel thread registration,
 *        the hostFunc function on the specified Stream
 *        is no longer processed by the specified thread
 *
 * @param hostFuncThreadId [IN]   thread ID
 * @param exeStream        [IN]   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtUnSubscribeHostFunc(uint64_t hostFuncThreadId, aclrtStream exeStream);

/**
 * @ingroup AscendCL
 * @brief Get device status
 *
 * @param deviceId       [IN]   device ID
 * @param deviceStatus   [OUT]  device status
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtQueryDeviceStatus(int32_t deviceId, aclrtDeviceStatus *deviceStatus);

/**
 * @ingroup AscendCL
 * @brief Create data of type aclrtBinary
 *
 * @param [in] data   binary data
 * @param [in] dataLen   binary length
 *
 * @retval the aclrtBinary
 */
ACL_FUNC_VISIBILITY aclrtBinary aclrtCreateBinary(const void *data, size_t dataLen);

/**
 * @ingroup AscendCL
 * @brief Destroy data of type aclrtBinary
 *
 * @param modelDesc [IN]   aclrtBinary to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtDestroyBinary(aclrtBinary binary);


/**
 * @ingroup AscendCL
 * @brief Registers and parses the bin file and loads it to the device.
 *
 * @param [in] binary   device binary description
 * @param [out] binHandle   device binary handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtBinaryLoad(const aclrtBinary binary, aclrtBinHandle *binHandle);


/**
 * @ingroup AscendCL
 * @brief UnLoad binary
 *
 * @param [in] binHandle  binary handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtBinaryUnLoad(aclrtBinHandle binHandle);

/**
 * @ingroup AscendCL
 * @brief Find funcHandle based on binHandle and kernel name
 *
 * @param [in] binHandle  binHandle
 * @param [in] kernelName   kernel name
 * @param [out] funcHandle   funcHandle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtBinaryGetFunction(const aclrtBinHandle binHandle, const char *kernelName,
                                                    aclrtFuncHandle *funcHandle);

/**
 * @ingroup AscendCL
 * @brief Kernel Launch to device
 * @param [in] funcHandle  function handle
 * @param [in] blockDim  block dimentions
 * @param [in] argsData  args data
 * @param [in] argsSize  args size
 * @param [in] stream   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtLaunchKernel(aclrtFuncHandle funcHandle, uint32_t blockDim,
                                               const void *argsData, size_t argsSize, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief share the handle that created by the process itself to other process
 * @param [in] handle   mem handle created by aclrtMallocPhysical
 * @param [in] handleType  reserved param, must be MEM_HANDLE_TYPE_NONE
 * @param [in] flags  reserved param, must be 0
 * @param [out] shareableHandle  shareable Handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemExportToShareableHandle(aclrtDrvMemHandle handle,
                                                             aclrtMemHandleType handleType, uint64_t flags,
                                                             uint64_t *shareableHandle);

/**
 * @ingroup AscendCL
 * @brief import a mem allocation from a shareable Handle
 * @param [in] shareableHandle  shareable Handle
 * @param [in] deviceId  used to generate the handle in the specified Device Id
 * @param [out] handle handle in the process
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemImportFromShareableHandle(uint64_t shareableHandle,
                                                               int32_t deviceId, aclrtDrvMemHandle *handle);

/**
 * @ingroup AscendCL
 * @brief set the process whitelist, only the process configured in the whitelist can use this shareableHandle
 * @param [in] shareableHandle  shareable Handle
 * @param [in] deviceId  used to generate the handle in the specified Device Id
 * @param [out] handle handle in the process
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemSetPidToShareableHandle(uint64_t shareableHandle,
                                                             int32_t *pid, size_t pidNum);

/**`
 * @ingroup AscendCL
 * @brief get the mem allocation granularity by the option
 * @param [in] prop  aclrtPhysicalMemProp
 * @param [in] option  mem granularity option
 * @param [out] granularity granularity
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemGetAllocationGranularity(aclrtPhysicalMemProp *prop,
                                                              aclrtMemGranularityOptions option,
                                                              size_t *granularity);

/**
 * @ingroup AscendCL
 * @brief Get the pid for the current process on the physical device
 * @param [out] pid value of pid
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtDeviceGetBareTgid(int32_t *pid);

/**
 * @ingroup AscendCL
 * @brief cache manager operation
 * @param [in] src  device memory address
 * @param [in] size  memory size
 * @param [in] cmoType  type of operation, currently, only ACL_RT_CMO_TYPE_PREFETCH is supported
 * @param [in] stream   stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtCmoAsync(void *src, size_t size, aclrtCmoType cmoType, aclrtStream stream);

/**`
 * @ingroup AscendCL
 * @brief get the mem uce info
 * @param [in] deviceId
 * @param [in/out] memUceInfoArray
 * @param [in] arraySize
 * @param [out] retSize
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetMemUceInfo(int32_t deviceId, aclrtMemUceInfo *memUceInfoArray,
                                                size_t arraySize, size_t *retSize);

/**`
 * @ingroup AscendCL
 * @brief stop the task on specified device
 * @param [in] deviceId
 * @param [in] timeout
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtDeviceTaskAbort(int32_t deviceId, uint32_t timeout);

/**`
 * @ingroup AscendCL
 * @brief repair the mem uce
 * @param [in] deviceId
 * @param [in/out] memUceInfoArray
 * @param [in] arraySize
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtMemUceRepair(int32_t deviceId, aclrtMemUceInfo *memUceInfoArray, size_t arraySize);

/**`
 * @ingroup AscendCL
 * @brief abort unexecuted tasks and pause executing tasks on the stream
 * @param [in] stream  stream to be aborted, cannot be null
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtStreamAbort(aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_RT_H_

// End content from: acl/acl_rt.h

// Begin content from: acl/acl_mdl.h
/**
* @file acl_mdl.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2023. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/

#ifndef INC_EXTERNAL_ACL_ACL_MODEL_H_
#define INC_EXTERNAL_ACL_ACL_MODEL_H_

#include <stddef.h>
#include <stdint.h>

// #include "acl_base.h"
// #include "acl_rt.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACL_DIM_ENDPOINTS        2
#define ACL_MAX_DIM_CNT          128
#define ACL_MAX_TENSOR_NAME_LEN  128
#define ACL_MAX_BATCH_NUM        128
#define ACL_MAX_HW_NUM           128
#define ACL_MAX_SHAPE_COUNT      128
#define ACL_INVALID_NODE_INDEX   0xFFFFFFFF

#define ACL_MDL_LOAD_FROM_FILE            1
#define ACL_MDL_LOAD_FROM_FILE_WITH_MEM   2
#define ACL_MDL_LOAD_FROM_MEM             3
#define ACL_MDL_LOAD_FROM_MEM_WITH_MEM    4
#define ACL_MDL_LOAD_FROM_FILE_WITH_Q     5
#define ACL_MDL_LOAD_FROM_MEM_WITH_Q      6

#define ACL_DYNAMIC_TENSOR_NAME "ascend_mbatch_shape_data"
#define ACL_DYNAMIC_AIPP_NAME "ascend_dynamic_aipp_data"
#define ACL_ATTR_NAME_DATA_DUMP_ORIGIN_OP_NAMES "_datadump_original_op_names"

/* used for ACL_MDL_WORKSPACE_MEM_OPTIMIZE */
#define ACL_WORKSPACE_MEM_OPTIMIZE_DEFAULT 0
#define ACL_WORKSPACE_MEM_OPTIMIZE_INPUTOUTPUT 1

typedef struct aclmdlDataset aclmdlDataset;
typedef struct aclmdlDesc aclmdlDesc;
typedef struct aclmdlAIPP aclmdlAIPP;
typedef struct aclAippExtendInfo aclAippExtendInfo;
typedef struct aclmdlConfigHandle aclmdlConfigHandle;
typedef struct aclmdlExecConfigHandle aclmdlExecConfigHandle;

typedef enum {
    ACL_YUV420SP_U8 = 1,
    ACL_XRGB8888_U8 = 2,
    ACL_RGB888_U8 = 3,
    ACL_YUV400_U8 = 4,
    ACL_NC1HWC0DI_FP16 = 5,
    ACL_NC1HWC0DI_S8 = 6,
    ACL_ARGB8888_U8 = 7,
    ACL_YUYV_U8 = 8,
    ACL_YUV422SP_U8 = 9,
    ACL_AYUV444_U8 = 10,
    ACL_RAW10 = 11,
    ACL_RAW12 = 12,
    ACL_RAW16 = 13,
    ACL_RAW24 = 14,
    ACL_AIPP_RESERVED = 0xFFFF,
} aclAippInputFormat;

typedef enum {
    ACL_MDL_PRIORITY_INT32 = 0,
    ACL_MDL_LOAD_TYPE_SIZET,
    ACL_MDL_PATH_PTR, /**< pointer to model load path with deep copy */
    ACL_MDL_MEM_ADDR_PTR, /**< pointer to model memory with shallow copy */
    ACL_MDL_MEM_SIZET,
    ACL_MDL_WEIGHT_ADDR_PTR, /**< pointer to weight memory of model with shallow copy */
    ACL_MDL_WEIGHT_SIZET,
    ACL_MDL_WORKSPACE_ADDR_PTR, /**< pointer to worksapce memory of model with shallow copy */
    ACL_MDL_WORKSPACE_SIZET,
    ACL_MDL_INPUTQ_NUM_SIZET,
    ACL_MDL_INPUTQ_ADDR_PTR, /**< pointer to inputQ with shallow copy */
    ACL_MDL_OUTPUTQ_NUM_SIZET,
    ACL_MDL_OUTPUTQ_ADDR_PTR, /**< pointer to outputQ with shallow copy */
    ACL_MDL_WORKSPACE_MEM_OPTIMIZE,
    ACL_MDL_WEIGHT_PATH_PTR, /**< pointer to weight path with deep copy */
    ACL_MDL_MODEL_DESC_PTR, /**< pointer to model desc of model with shallow copy */
    ACL_MDL_MODEL_DESC_SIZET,
    ACL_MDL_KERNEL_PTR, /**< pointer to kernel bin of model with shallow copy */
    ACL_MDL_KERNEL_SIZET,
    ACL_MDL_KERNEL_ARGS_PTR, /**< pointer to kernel args of model with shallow copy */
    ACL_MDL_KERNEL_ARGS_SIZET,
    ACL_MDL_STATIC_TASK_PTR, /**< pointer to static task desc of model with shallow copy */
    ACL_MDL_STATIC_TASK_SIZET,
    ACL_MDL_DYNAMIC_TASK_PTR, /**< pointer to dynamic task desc of model with shallow copy */
    ACL_MDL_DYNAMIC_TASK_SIZET,
    ACL_MDL_MEM_MALLOC_POLICY_SIZET
} aclmdlConfigAttr;

typedef enum {
    ACL_MDL_STREAM_SYNC_TIMEOUT = 0,
    ACL_MDL_EVENT_SYNC_TIMEOUT,
    ACL_MDL_WORK_ADDR_PTR, /**< param */
    ACL_MDL_WORK_SIZET, /**< param */
    ACL_MDL_MPAIMID_SIZET, /**< param reserved */
    ACL_MDL_AICQOS_SIZET, /**< param reserved */
    ACL_MDL_AICOST_SIZET, /**< param reserved */
    ACL_MDL_MEC_TIMETHR_SIZET /**< param reserved */
} aclmdlExecConfigAttr;

typedef enum {
    ACL_DATA_WITHOUT_AIPP = 0,
    ACL_DATA_WITH_STATIC_AIPP,
    ACL_DATA_WITH_DYNAMIC_AIPP,
    ACL_DYNAMIC_AIPP_NODE
} aclmdlInputAippType;

typedef struct aclmdlIODims {
    char name[ACL_MAX_TENSOR_NAME_LEN]; /**< tensor name */
    size_t dimCount; /**< dim array count */
    int64_t dims[ACL_MAX_DIM_CNT]; /**< dim data array */
} aclmdlIODims;

typedef struct aclmdlIODimsRange {
    size_t rangeCount; /**< dim range array count */
    int64_t range[ACL_MAX_DIM_CNT][ACL_DIM_ENDPOINTS]; /**< range data array */
} aclmdlIODimsRange;

typedef struct aclAippDims {
    aclmdlIODims srcDims; /**< input dims before model transform */
    size_t srcSize; /**< input size before model transform */
    aclmdlIODims aippOutdims; /**< aipp output dims */
    size_t aippOutSize; /**< aipp output size */
} aclAippDims;

typedef struct aclmdlBatch {
    size_t batchCount; /**< batch array count */
    uint64_t batch[ACL_MAX_BATCH_NUM]; /**< batch data array */
} aclmdlBatch;

typedef struct aclmdlHW {
    size_t hwCount; /**< height&width array count */
    uint64_t hw[ACL_MAX_HW_NUM][2]; /**< height&width data array */
} aclmdlHW;

typedef struct aclAippInfo {
    aclAippInputFormat inputFormat;
    int32_t srcImageSizeW;
    int32_t srcImageSizeH;
    int8_t cropSwitch;
    int32_t loadStartPosW;
    int32_t loadStartPosH;
    int32_t cropSizeW;
    int32_t cropSizeH;
    int8_t resizeSwitch;
    int32_t resizeOutputW;
    int32_t resizeOutputH;
    int8_t paddingSwitch;
    int32_t leftPaddingSize;
    int32_t rightPaddingSize;
    int32_t topPaddingSize;
    int32_t bottomPaddingSize;
    int8_t cscSwitch;
    int8_t rbuvSwapSwitch;
    int8_t axSwapSwitch;
    int8_t singleLineMode;
    int32_t matrixR0C0;
    int32_t matrixR0C1;
    int32_t matrixR0C2;
    int32_t matrixR1C0;
    int32_t matrixR1C1;
    int32_t matrixR1C2;
    int32_t matrixR2C0;
    int32_t matrixR2C1;
    int32_t matrixR2C2;
    int32_t outputBias0;
    int32_t outputBias1;
    int32_t outputBias2;
    int32_t inputBias0;
    int32_t inputBias1;
    int32_t inputBias2;
    int32_t meanChn0;
    int32_t meanChn1;
    int32_t meanChn2;
    int32_t meanChn3;
    float minChn0;
    float minChn1;
    float minChn2;
    float minChn3;
    float varReciChn0;
    float varReciChn1;
    float varReciChn2;
    float varReciChn3;
    aclFormat srcFormat;
    aclDataType srcDatatype;
    size_t srcDimNum;
    size_t shapeCount;
    aclAippDims outDims[ACL_MAX_SHAPE_COUNT];
    aclAippExtendInfo *aippExtend; /**< reserved parameters, current version needs to be null */
} aclAippInfo;

typedef struct aclmdlExeOMDesc {
  size_t workSize;
  size_t weightSize;
  size_t modelDescSize;
  size_t kernelSize;
  size_t kernelArgsSize;
  size_t staticTaskSize;
  size_t dynamicTaskSize;
  size_t reserved[9];
} aclmdlExeOMDesc;

/**
 * @ingroup AscendCL
 * @brief Create data of type aclmdlDesc
 *
 * @retval the aclmdlDesc pointer
 */
ACL_FUNC_VISIBILITY aclmdlDesc *aclmdlCreateDesc();

/**
 * @ingroup AscendCL
 * @brief destroy data of type aclmdlDesc
 *
 * @param modelDesc [IN]   Pointer to almdldlDesc to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlDestroyDesc(aclmdlDesc *modelDesc);

/**
 * @ingroup AscendCL
 * @brief Get aclmdlDesc data of the model according to the model ID
 *
 * @param  modelDesc [OUT]   aclmdlDesc pointer
 * @param  modelId [IN]      model id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetDesc(aclmdlDesc *modelDesc, uint32_t modelId);

/**
 * @ingroup AscendCL
 * @brief Get aclmdlDesc data of the model according to the model path
 *
 * @param  modelDesc [OUT]   aclmdlDesc pointer
 * @param  modelPath [IN]    model path
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetDescFromFile(aclmdlDesc *modelDesc, const char *modelPath);

/**
 * @ingroup AscendCL
 * @brief Get aclmdlDesc data of the model according to the model and modelSize
 *
 * @param  modelDesc [OUT]   aclmdlDesc pointer
 * @param  model [IN]        model pointer
 * @param  modelSize [IN]    model size
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetDescFromMem(aclmdlDesc *modelDesc, const void *model, size_t modelSize);

/**
 * @ingroup AscendCL
 * @brief Get the number of the inputs of
 *        the model according to data of aclmdlDesc
 *
 * @param  modelDesc [IN]   aclmdlDesc pointer
 *
 * @retval input size with aclmdlDesc
 */
ACL_FUNC_VISIBILITY size_t aclmdlGetNumInputs(aclmdlDesc *modelDesc);

/**
 * @ingroup AscendCL
 * @brief Get the number of the output of
 *        the model according to data of aclmdlDesc
 *
 * @param  modelDesc [IN]   aclmdlDesc pointer
 *
 * @retval output size with aclmdlDesc
 */
ACL_FUNC_VISIBILITY size_t aclmdlGetNumOutputs(aclmdlDesc *modelDesc);

/**
 * @ingroup AscendCL
 * @brief Get the size of the specified input according to
 *        the data of type aclmdlDesc
 *
 * @param  modelDesc [IN]  aclmdlDesc pointer
 * @param  index [IN] the size of the number of inputs to be obtained,
 *         the index value starts from 0
 *
 * @retval Specify the size of the input
 */
ACL_FUNC_VISIBILITY size_t aclmdlGetInputSizeByIndex(aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief Get the size of the specified output according to
 *        the data of type aclmdlDesc
 *
 * @param modelDesc [IN]   aclmdlDesc pointer
 * @param index [IN]  the size of the number of outputs to be obtained,
 *        the index value starts from 0
 *
 * @retval Specify the size of the output
 */
ACL_FUNC_VISIBILITY size_t aclmdlGetOutputSizeByIndex(aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief Create config handle of execute
 *
 * @retval the aclmdlCreateExecConfigHandle pointer
 */
ACL_FUNC_VISIBILITY aclmdlExecConfigHandle *aclmdlCreateExecConfigHandle();

/**
 * @ingroup AscendCL
 * @brief Destroy config handle of model execute
 *
 * @param  handle [IN]  Pointer to aclmdlExecConfigHandle to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlDestroyExecConfigHandle(const aclmdlExecConfigHandle *handle);

/**
 * @ingroup AscendCL
 * @brief Create data of type aclmdlDataset
 *
 * @retval the aclmdlDataset pointer
 */
ACL_FUNC_VISIBILITY aclmdlDataset *aclmdlCreateDataset();

/**
 * @ingroup AscendCL
 * @brief destroy data of type aclmdlDataset
 *
 * @param  dataset [IN]  Pointer to aclmdlDataset to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlDestroyDataset(const aclmdlDataset *dataset);

/**
 * @ingroup AscendCL
 * @brief Add aclDataBuffer to aclmdlDataset
 *
 * @param dataset [OUT]    aclmdlDataset address of aclDataBuffer to be added
 * @param dataBuffer [IN]  aclDataBuffer address to be added
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlAddDatasetBuffer(aclmdlDataset *dataset, aclDataBuffer *dataBuffer);

/**
 * @ingroup AscendCL
 * @brief Set aclTensorDesc to aclmdlDataset
 *
 * @param dataset [OUT]    aclmdlDataset address of aclDataBuffer to be added
 * @param tensorDesc [IN]  aclTensorDesc address to be added
 * @param index [IN]       index of tensorDesc which to be added
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetDatasetTensorDesc(aclmdlDataset *dataset,
                                                        aclTensorDesc *tensorDesc,
                                                        size_t index);

/**
 * @ingroup AscendCL
 * @brief Get aclTensorDesc from aclmdlDataset
 *
 * @param dataset [IN]    aclmdlDataset pointer;
 * @param index [IN]      index of tensorDesc
 *
 * @retval Get address of aclTensorDesc when executed successfully.
 * @retval Failure return NULL
 */
ACL_FUNC_VISIBILITY aclTensorDesc *aclmdlGetDatasetTensorDesc(const aclmdlDataset *dataset, size_t index);

/**
 * @ingroup AscendCL
 * @brief Get the number of aclDataBuffer in aclmdlDataset
 *
 * @param dataset [IN]   aclmdlDataset pointer
 *
 * @retval the number of aclDataBuffer
 */
ACL_FUNC_VISIBILITY size_t aclmdlGetDatasetNumBuffers(const aclmdlDataset *dataset);

/**
 * @ingroup AscendCL
 * @brief Get the aclDataBuffer in aclmdlDataset by index
 *
 * @param dataset [IN]   aclmdlDataset pointer
 * @param index [IN]     the index of aclDataBuffer
 *
 * @retval Get successfully, return the address of aclDataBuffer
 * @retval Failure return NULL
 */
ACL_FUNC_VISIBILITY aclDataBuffer *aclmdlGetDatasetBuffer(const aclmdlDataset *dataset, size_t index);

/**
 * @ingroup AscendCL
 * @brief Load offline model data from files
 * and manage memory internally by the system
 *
 * @par Function
 * After the system finishes loading the model,
 * the model ID returned is used as a mark to identify the model
 * during subsequent operations
 *
 * @param modelPath [IN]   Storage path for offline model files
 * @param modelId [OUT]    Model ID generated after
 *        the system finishes loading the model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromFile(const char *modelPath, uint32_t *modelId);

/**
 * @ingroup AscendCL
 * @brief Load offline bundle model data from file
 * and manage memory internally by the system
 *
 * @par Function
 * After the system finishes loading the bundle model,
 * the bundle model ID returned is used as a mark to identify the bundle model
 * during subsequent operations
 *
 * @param modelPath [IN]   Storage path for offline bundle model file
 * @param bundleId [OUT]   Bundle model id generated after
 *        the system finishes loading the bundle model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlBundleLoadFromFile(const char *modelPath, uint32_t *bundleId);

/**
 * @ingroup AscendCL
 * @brief Load offline bundle model data from memory and manage the memory of
 * model running internally by the system
 *
 * @par Function
 * After the system finishes loading the bundle model,
 * the bundle model ID returned is used as a mark to identify the bundle  model
 * during subsequent operations
 *
 * @param model [IN]      Bundle model data stored in memory
 * @param modelSize [IN]  model data size
 * @param bundleId [OUT]  Bundle model id generated after
 *        the system finishes loading the model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlBundleLoadFromMem(const void *model,  size_t modelSize, uint32_t *bundleId);

/**
 * @ingroup AscendCL
 * @brief unload bundle model with bundle model id
 *
 * @param  bundleId [IN]   bundle model id to be unloaded
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlBundleUnload(uint32_t bundleId);

/**
 * @ingroup AscendCL
 * @brief get bundle model inner model nums
 *
 * @param bundleId [IN] bundle id acquired by aclmdlBundleLoadFromFile or aclmdlBundleLoadFromMem
 * @param modelNum [OUT]    the pointer to model num
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 */
ACL_FUNC_VISIBILITY aclError aclmdlBundleGetModelNum(uint32_t bundleId, size_t *modelNum);

/**
 * @ingroup AscendCL
 * @brief get inner model id by index
 *
 * @param bundleId [IN] bundle id acquired by aclmdlBundleLoadFromFile or aclmdlBundleLoadFromMem
 * @param index [IN] index of bundle models
 * @param modelId [OUT]    the pointer to inner model id which to be executed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 */
ACL_FUNC_VISIBILITY aclError aclmdlBundleGetModelId(uint32_t bundleId, size_t index, uint32_t *modelId);

/**
 * @ingroup AscendCL
 * @brief Load offline model data from memory and manage the memory of
 * model running internally by the system
 *
 * @par Function
 * After the system finishes loading the model,
 * the model ID returned is used as a mark to identify the model
 * during subsequent operations
 *
 * @param model [IN]      Model data stored in memory
 * @param modelSize [IN]  model data size
 * @param modelId [OUT]   Model ID generated after
 *        the system finishes loading the model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromMem(const void *model,  size_t modelSize, uint32_t *modelId);

/**
 * @ingroup AscendCL
 * @brief Load offline model data from a file,
 * and the user manages the memory of the model run by itself
 *
 * @par Function
 * After the system finishes loading the model,
 * the model ID returned is used as a mark to identify the model
 * during subsequent operations.
 * @param modelPath [IN]   Storage path for offline model files
 * @param modelId [OUT]    Model ID generated after finishes loading the model
 * @param workPtr [IN]     A pointer to the working memory
 *                         required by the model on the Device,can be null
 * @param workSize [IN]    The amount of working memory required by the model
 * @param weightPtr [IN]   Pointer to model weight memory on Device
 * @param weightSize [IN]  The amount of weight memory required by the model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromFileWithMem(const char *modelPath,
                                                       uint32_t *modelId, void *workPtr, size_t workSize,
                                                       void *weightPtr, size_t weightSize);

/**
 * @ingroup AscendCL
 * @brief Load offline model data from memory,
 * and the user can manage the memory of model running
 *
 * @par Function
 * After the system finishes loading the model,
 * the model ID returned is used as a mark to identify the model
 * during subsequent operations
 * @param model [IN]      Model data stored in memory
 * @param modelSize [IN]  model data size
 * @param modelId [OUT]   Model ID generated after finishes loading the model
 * @param workPtr [IN]    A pointer to the working memory
 *                        required by the model on the Device,can be null
 * @param workSize [IN]   work memory size
 * @param weightPtr [IN]  Pointer to model weight memory on Device,can be null
 * @param weightSize [IN] The amount of weight memory required by the model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromMemWithMem(const void *model, size_t modelSize,
                                                      uint32_t *modelId, void *workPtr, size_t workSize,
                                                      void *weightPtr, size_t weightSize);

/**
 * @ingroup AscendCL
 * @brief load model from file with async queue
 *
 * @param modelPath  [IN] model path
 * @param modelId [OUT]   return model id if load success
 * @param inputQ [IN]     input queue pointer
 * @param inputQNum [IN]  input queue num
 * @param outputQ [IN]    output queue pointer
 * @param outputQNum [IN] output queue num
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromFileWithQ(const char *modelPath, uint32_t *modelId, const uint32_t *inputQ,
                                                     size_t inputQNum, const uint32_t *outputQ, size_t outputQNum);

/**
 * @ingroup AscendCL
 * @brief load model from memory with async queue
 *
 * @param model [IN]      model memory which user manages
 * @param modelSize [IN]  model size
 * @param modelId [OUT]   return model id if load success
 * @param inputQ [IN]     input queue pointer
 * @param inputQNum [IN]  input queue num
 * @param outputQ [IN]    output queue pointer
 * @param outputQNum [IN] output queue num
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlLoadFromMemWithQ(const void *model, size_t modelSize, uint32_t *modelId,
                                                    const uint32_t *inputQ, size_t inputQNum,
                                                    const uint32_t *outputQ, size_t outputQNum);

/**
 * @ingroup AscendCL
 * @brief Execute model synchronous inference until the inference result is returned
 *
 * @param  modelId [IN]   ID of the model to perform inference
 * @param  input [IN]     Input data for model inference
 * @param  output [OUT]   Output data for model inference
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlExecute(uint32_t modelId, const aclmdlDataset *input, aclmdlDataset *output);

/**
 * @ingroup AscendCL
 * @brief Execute model synchronous inference until the inference result is returned
 *
 * @param  modelId [IN]   ID of the model to perform inference
 * @param  input [IN]     Input data for model inference
 * @param  output [OUT]   Output data for model inference
 * @param  stream [IN]   stream
 * @param  handle [IN]   config of model execute
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlExecuteV2(uint32_t modelId, const aclmdlDataset *input, aclmdlDataset *output,
                                             aclrtStream stream, const aclmdlExecConfigHandle *handle);

/**
 * @ingroup AscendCL
 * @brief Execute model asynchronous inference until the inference result is returned
 *
 * @param  modelId [IN]   ID of the model to perform inference
 * @param  input [IN]     Input data for model inference
 * @param  output [OUT]   Output data for model inference
 * @param  stream [IN]   stream
 * @param  handle [IN]   config of model execute
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY  aclError aclmdlExecuteAsyncV2(uint32_t modelId, const aclmdlDataset *input, aclmdlDataset *output,
                                                   aclrtStream stream, const aclmdlExecConfigHandle *handle);
/**
 * @ingroup AscendCL
 * @brief Execute model asynchronous inference until the inference result is returned
 *
 * @param  modelId [IN]   ID of the model to perform inference
 * @param  input [IN]     Input data for model inference
 * @param  output [OUT]   Output data for model inference
 * @param  stream [IN]    stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem
 */
ACL_FUNC_VISIBILITY aclError aclmdlExecuteAsync(uint32_t modelId, const aclmdlDataset *input,
                                                aclmdlDataset *output, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief unload model with model id
 *
 * @param  modelId [IN]   model id to be unloaded
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlUnload(uint32_t modelId);

/**
 * @ingroup AscendCL
 * @brief Get the weight memory size and working memory size
 * required for model execution according to the model file
 *
 * @param  fileName [IN]     Model path to get memory information
 * @param  workSize [OUT]    The amount of working memory for model executed
 * @param  weightSize [OUT]  The amount of weight memory for model executed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlQuerySize(const char *fileName, size_t *workSize, size_t *weightSize);

/**
 * @ingroup AscendCL
 * @brief Get the size of each partition and working memory size
 * required for model execution according to the model file
 *
 * @param  fileName [IN]          Model path to get memory information
 * @param  aclmdlExeOMDesc [OUT]  The size of each partition and working memory size
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlQueryExeOMDesc(const char *fileName, aclmdlExeOMDesc *mdlPartitionSize);

/**
 * @ingroup AscendCL
 * @brief Obtain the weights required for
 * model execution according to the model data in memory
 *
 * @par Restriction
 * The execution and weight memory is Device memory,
 * and requires user application and release.
 * @param  model [IN]        model memory which user manages
 * @param  modelSize [IN]    model data size
 * @param  workSize [OUT]    The amount of working memory for model executed
 * @param  weightSize [OUT]  The amount of weight memory for model executed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlQuerySizeFromMem(const void *model, size_t modelSize, size_t *workSize,
                                                    size_t *weightSize);

/**
 * @ingroup AscendCL
 * @brief In dynamic batch scenarios,
 * it is used to set the number of images processed
 * at one time during model inference
 *
 * @param  modelId [IN]     model id
 * @param  dataset [IN|OUT] data for model inference
 * @param  index [IN]       index of dynamic tensor
 * @param  batchSize [IN]   Number of images processed at a time during model
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetDynamicBatchSize(uint32_t modelId, aclmdlDataset *dataset, size_t index,
                                                       uint64_t batchSize);

/**
 * @ingroup AscendCL
 * @brief Sets the H and W of the specified input of the model
 *
 * @param  modelId [IN]     model id
 * @param  dataset [IN|OUT] data for model inference
 * @param  index [IN]       index of dynamic tensor
 * @param  height [IN]      model height
 * @param  width [IN]       model width
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetDynamicHWSize(uint32_t modelId, aclmdlDataset *dataset, size_t index,
                                                    uint64_t height, uint64_t width);

/**
 * @ingroup AscendCL
 * @brief Sets the dynamic dims of the specified input of the model
 *
 * @param  modelId [IN]     model id
 * @param  dataset [IN|OUT] data for model inference
 * @param  index [IN]       index of dynamic dims
 * @param  dims [IN]        value of dynamic dims
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetInputDynamicDims(uint32_t modelId, aclmdlDataset *dataset, size_t index,
                                                       const aclmdlIODims *dims);

/**
 * @ingroup AscendCL
 * @brief get input dims info
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  input tensor index
 * @param dims [OUT]  dims info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlGetInputDimsV2
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputDims(const aclmdlDesc *modelDesc, size_t index, aclmdlIODims *dims);

/**
 * @ingroup AscendCL
 * @brief get input dims info(version 2), especially for static aipp
 * it is the same with aclmdlGetInputDims while model without static aipp
 *
 * @param modelDesc [IN] model description
 * @param index [IN]     input tensor index
 * @param dims [OUT]     dims info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlGetInputDims
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputDimsV2(const aclmdlDesc *modelDesc, size_t index, aclmdlIODims *dims);

/**
 * @ingroup AscendCL
 * @brief get input dims range info
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  input tensor index
 * @param dimsRange [OUT]  dims range info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputDimsRange(const aclmdlDesc *modelDesc, size_t index,
                                                     aclmdlIODimsRange *dimsRange);

/**
 * @ingroup AscendCL
 * @brief get output dims info
 *
 * @param modelDesc [IN] model description
 * @param index [IN]     output tensor index
 * @param dims [OUT]     dims info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetOutputDims(const aclmdlDesc *modelDesc, size_t index, aclmdlIODims *dims);

/**
 * @ingroup AscendCL
 * @brief get current output dims info
 *
 * @par Function
 * The following use cases are supported:
 * @li Get current output shape when model is dynamic and
 * dynamic shape info is set
 * @li Get max output shape when model is dynamic and
 * dynamic shape info is not set
 * @li Get actual output shape when model is static
 *
 * @param modelDesc [IN] model description
 * @param index [IN]     output tensor index
 * @param dims [OUT]     dims info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetCurOutputDims(const aclmdlDesc *modelDesc, size_t index, aclmdlIODims *dims);

/**
 * @ingroup AscendCL
 * @brief get attr value by op name
 *
 * @param modelDesc [IN]   model description
 * @param opName [IN]      op name
 * @param attr [IN]        attr name
 *
 * @retval the attr value
 */
ACL_FUNC_VISIBILITY const char *aclmdlGetOpAttr(aclmdlDesc *modelDesc, const char *opName, const char *attr);

/**
 * @ingroup AscendCL
 * @brief get input name by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]      intput tensor index
 *
 * @retval input tensor name,the same life cycle with modelDesc
 */
ACL_FUNC_VISIBILITY const char *aclmdlGetInputNameByIndex(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get output name by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]      output tensor index
 *
 * @retval output tensor name,the same life cycle with modelDesc
 */
ACL_FUNC_VISIBILITY const char *aclmdlGetOutputNameByIndex(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get input format by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]      intput tensor index
 *
 * @retval input tensor format
 */
ACL_FUNC_VISIBILITY aclFormat aclmdlGetInputFormat(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get output format by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]      output tensor index
 *
 * @retval output tensor format
 */
ACL_FUNC_VISIBILITY aclFormat aclmdlGetOutputFormat(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get input data type by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  intput tensor index
 *
 * @retval input tensor data type
 */
ACL_FUNC_VISIBILITY aclDataType aclmdlGetInputDataType(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get output data type by index
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  output tensor index
 *
 * @retval output tensor data type
 */
ACL_FUNC_VISIBILITY aclDataType aclmdlGetOutputDataType(const aclmdlDesc *modelDesc, size_t index);

/**
 * @ingroup AscendCL
 * @brief get input tensor index by name
 *
 * @param modelDesc [IN]  model description
 * @param name [IN]    intput tensor name
 * @param index [OUT]  intput tensor index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputIndexByName(const aclmdlDesc *modelDesc, const char *name, size_t *index);

/**
 * @ingroup AscendCL
 * @brief get output tensor index by name
 *
 * @param modelDesc [IN]  model description
 * @param name [IN]  output tensor name
 * @param index [OUT]  output tensor index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetOutputIndexByName(const aclmdlDesc *modelDesc, const char *name, size_t *index);

/**
 * @ingroup AscendCL
 * @brief get dynamic batch info
 *
 * @param modelDesc [IN]  model description
 * @param batch [OUT]  dynamic batch info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetDynamicBatch(const aclmdlDesc *modelDesc, aclmdlBatch *batch);

/**
 * @ingroup AscendCL
 * @brief get dynamic height&width info
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  input tensor index
 * @param hw [OUT]  dynamic height&width info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetDynamicHW(const aclmdlDesc *modelDesc, size_t index, aclmdlHW *hw);

/**
 * @ingroup AscendCL
 * @brief get dynamic gear count
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  unused, must be -1
 * @param gearCount [OUT]  dynamic gear count
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputDynamicGearCount(const aclmdlDesc *modelDesc, size_t index,
                                                            size_t *gearCount);

/**
 * @ingroup AscendCL
 * @brief get dynamic dims info
 *
 * @param modelDesc [IN]  model description
 * @param index [IN]  unused, must be -1
 * @param dims [OUT]  value of dynamic dims
 * @param gearCount [IN]  dynamic gear count
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetInputDynamicDims(const aclmdlDesc *modelDesc, size_t index, aclmdlIODims *dims,
                                                       size_t gearCount);

/**
 * @ingroup AscendCL
 * @brief Create data of type aclmdlAIPP
 *
 * @param batchSize [IN]    batchsizes of model
 *
 * @retval the aclmdlAIPP pointer
 */
ACL_FUNC_VISIBILITY aclmdlAIPP *aclmdlCreateAIPP(uint64_t batchSize);

/**
 * @ingroup AscendCL
 * @brief destroy data of type aclmdlAIPP
 *
 * @param aippParmsSet [IN]    Pointer for aclmdlAIPP to be destroyed
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlDestroyAIPP(const aclmdlAIPP *aippParmsSet);

/**
 * @ingroup AscendCL
 * @brief Get dynamic aipp data need size according to batchSize
 *
 * @param batchSize [IN]    batchsizes of model
 * @param size [OUT]    Pointer of aipp data need size according to batchSize
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlGetAippDataSize(uint64_t batchSize, size_t *size);

/**
 * @ingroup AscendCL
 * @brief set InputFormat of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param inputFormat [IN]    The inputFormat of aipp
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPInputFormat(aclmdlAIPP *aippParmsSet, aclAippInputFormat inputFormat);

/**
 * @ingroup AscendCL
 * @brief set cscParms of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]    Pointer for aclmdlAIPP
 * @param csc_switch [IN]       Csc switch
 * @param cscMatrixR0C0 [IN]    Csc_matrix_r0_c0
 * @param cscMatrixR0C1 [IN]    Csc_matrix_r0_c1
 * @param cscMatrixR0C2 [IN]    Csc_matrix_r0_c2
 * @param cscMatrixR1C0 [IN]    Csc_matrix_r1_c0
 * @param cscMatrixR1C1 [IN]    Csc_matrix_r1_c1
 * @param cscMatrixR1C2 [IN]    Csc_matrix_r1_c2
 * @param cscMatrixR2C0 [IN]    Csc_matrix_r2_c0
 * @param cscMatrixR2C1 [IN]    Csc_matrix_r2_c1
 * @param cscMatrixR2C2 [IN]    Csc_matrix_r2_c2
 * @param cscOutputBiasR0 [IN]  Output Bias for RGB to YUV, element of row 0, unsigned number
 * @param cscOutputBiasR1 [IN]  Output Bias for RGB to YUV, element of row 1, unsigned number
 * @param cscOutputBiasR2 [IN]  Output Bias for RGB to YUV, element of row 2, unsigned number
 * @param cscInputBiasR0 [IN]   Input Bias for YUV to RGB, element of row 0, unsigned number
 * @param cscInputBiasR1 [IN]   Input Bias for YUV to RGB, element of row 1, unsigned number
 * @param cscInputBiasR2 [IN]   Input Bias for YUV to RGB, element of row 2, unsigned number
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPCscParams(aclmdlAIPP *aippParmsSet, int8_t cscSwitch,
                                                    int16_t cscMatrixR0C0, int16_t cscMatrixR0C1, int16_t cscMatrixR0C2,
                                                    int16_t cscMatrixR1C0, int16_t cscMatrixR1C1, int16_t cscMatrixR1C2,
                                                    int16_t cscMatrixR2C0, int16_t cscMatrixR2C1, int16_t cscMatrixR2C2,
                                                    uint8_t cscOutputBiasR0, uint8_t cscOutputBiasR1,
                                                    uint8_t cscOutputBiasR2, uint8_t cscInputBiasR0,
                                                    uint8_t cscInputBiasR1, uint8_t cscInputBiasR2);

/**
 * @ingroup AscendCL
 * @brief set rb/ub swap switch of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param rbuvSwapSwitch [IN] rb/ub swap switch
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPRbuvSwapSwitch(aclmdlAIPP *aippParmsSet, int8_t rbuvSwapSwitch);

/**
 * @ingroup AscendCL
 * @brief set RGBA->ARGB, YUVA->AYUV swap switch of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param axSwapSwitch [IN]   RGBA->ARGB, YUVA->AYUV swap switch
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPAxSwapSwitch(aclmdlAIPP *aippParmsSet, int8_t axSwapSwitch);

/**
 * @ingroup AscendCL
 * @brief set source image of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param srcImageSizeW [IN]  Source image width
 * @param srcImageSizeH [IN]  Source image height
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPSrcImageSize(aclmdlAIPP *aippParmsSet, int32_t srcImageSizeW,
                                                       int32_t srcImageSizeH);

/**
 * @ingroup AscendCL
 * @brief set resize switch of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param scfSwitch [IN]      Resize switch
 * @param scfInputSizeW [IN]  Input width of scf
 * @param scfInputSizeH [IN]  Input height of scf
 * @param scfOutputSizeW [IN] Output width of scf
 * @param scfOutputSizeH [IN] Output height of scf
 * @param batchIndex [IN]     Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPScfParams(aclmdlAIPP *aippParmsSet,
                                                    int8_t scfSwitch,
                                                    int32_t scfInputSizeW,
                                                    int32_t scfInputSizeH,
                                                    int32_t scfOutputSizeW,
                                                    int32_t scfOutputSizeH,
                                                    uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set cropParams of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]  Pointer for aclmdlAIPP
 * @param cropSwitch [IN]     Crop switch
 * @param cropStartPosW [IN]  The start horizontal position of cropping
 * @param cropStartPosH [IN]  The start vertical position of cropping
 * @param cropSizeW [IN]      Crop width
 * @param cropSizeH [IN]      Crop height
 * @param batchIndex [IN]     Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPCropParams(aclmdlAIPP *aippParmsSet,
                                                     int8_t cropSwitch,
                                                     int32_t cropStartPosW,
                                                     int32_t cropStartPosH,
                                                     int32_t cropSizeW,
                                                     int32_t cropSizeH,
                                                     uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set paddingParams of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]      Pointer for aclmdlAIPP
 * @param paddingSwitch [IN]      Padding switch
 * @param paddingSizeTop [IN]     Top padding size
 * @param paddingSizeBottom [IN]  Bottom padding size
 * @param paddingSizeLeft [IN]    Left padding size
 * @param paddingSizeRight [IN]   Right padding size
 * @param batchIndex [IN]         Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPPaddingParams(aclmdlAIPP *aippParmsSet, int8_t paddingSwitch,
                                                        int32_t paddingSizeTop, int32_t paddingSizeBottom,
                                                        int32_t paddingSizeLeft, int32_t paddingSizeRight,
                                                        uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set DtcPixelMean of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]      Pointer for aclmdlAIPP
 * @param dtcPixelMeanChn0 [IN]   Mean value of channel 0
 * @param dtcPixelMeanChn1 [IN]   Mean value of channel 1
 * @param dtcPixelMeanChn2 [IN]   Mean value of channel 2
 * @param dtcPixelMeanChn3 [IN]   Mean value of channel 3
 * @param batchIndex [IN]         Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPDtcPixelMean(aclmdlAIPP *aippParmsSet,
                                                       int16_t dtcPixelMeanChn0,
                                                       int16_t dtcPixelMeanChn1,
                                                       int16_t dtcPixelMeanChn2,
                                                       int16_t dtcPixelMeanChn3,
                                                       uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set DtcPixelMin of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]    Pointer for aclmdlAIPP
 * @param dtcPixelMinChn0 [IN]  Min value of channel 0
 * @param dtcPixelMinChn1 [IN]  Min value of channel 1
 * @param dtcPixelMinChn2 [IN]  Min value of channel 2
 * @param dtcPixelMinChn3 [IN]  Min value of channel 3
 * @param batchIndex [IN]       Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPDtcPixelMin(aclmdlAIPP *aippParmsSet,
                                                      float dtcPixelMinChn0,
                                                      float dtcPixelMinChn1,
                                                      float dtcPixelMinChn2,
                                                      float dtcPixelMinChn3,
                                                      uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set PixelVarReci of type aclmdlAIPP
 *
 * @param aippParmsSet [OUT]       Pointer for aclmdlAIPP
 * @param dtcPixelVarReciChn0 [IN] sfr_dtc_pixel_variance_reci_ch0
 * @param dtcPixelVarReciChn1 [IN] sfr_dtc_pixel_variance_reci_ch1
 * @param dtcPixelVarReciChn2 [IN] sfr_dtc_pixel_variance_reci_ch2
 * @param dtcPixelVarReciChn3 [IN] sfr_dtc_pixel_variance_reci_ch3
 * @param batchIndex [IN]          Batch parameter index
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPPixelVarReci(aclmdlAIPP *aippParmsSet,
                                                       float dtcPixelVarReciChn0,
                                                       float dtcPixelVarReciChn1,
                                                       float dtcPixelVarReciChn2,
                                                       float dtcPixelVarReciChn3,
                                                       uint64_t batchIndex);

/**
 * @ingroup AscendCL
 * @brief set aipp parameters to model
 *
 * @param modelId [IN]        model id
 * @param dataset [IN]        Pointer of dataset
 * @param index [IN]          index of input for aipp data(ACL_DYNAMIC_AIPP_NODE)
 * @param aippParmsSet [IN]   Pointer for aclmdlAIPP
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName | aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetInputAIPP(uint32_t modelId,
                                                aclmdlDataset *dataset,
                                                size_t index,
                                                const aclmdlAIPP *aippParmsSet);

/**
 * @ingroup AscendCL
 * @brief set aipp parameters to model
 *
 * @param modelId [IN]        model id
 * @param dataset [IN]        Pointer of dataset
 * @param index [IN]          index of input for data which linked dynamic aipp(ACL_DATA_WITH_DYNAMIC_AIPP)
 * @param aippParmsSet [IN]   Pointer for aclmdlAIPP
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName | aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetAIPPByInputIndex(uint32_t modelId,
                                                       aclmdlDataset *dataset,
                                                       size_t index,
                                                       const aclmdlAIPP *aippParmsSet);

/**
 * @ingroup AscendCL
 * @brief get input aipp type
 *
 * @param modelId [IN]        model id
 * @param index [IN]          index of input
 * @param type [OUT]          aipp type for input.refrer to aclmdlInputAippType(enum)
 * @param dynamicAttachedDataIndex [OUT]     index for dynamic attached data(ACL_DYNAMIC_AIPP_NODE)
 *        valid when type is ACL_DATA_WITH_DYNAMIC_AIPP, invalid value is ACL_INVALID_NODE_INDEX
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName | aclmdlCreateAIPP
*/
ACL_FUNC_VISIBILITY aclError aclmdlGetAippType(uint32_t modelId,
                                               size_t index,
                                               aclmdlInputAippType *type,
                                               size_t *dynamicAttachedDataIndex);

/**
 * @ingroup AscendCL
 * @brief get static aipp parameters from model
 *
 * @param modelId [IN]        model id
 * @param index [IN]          index of tensor
 * @param aippInfo [OUT]      Pointer for static aipp info
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval ACL_ERROR_MODEL_AIPP_NOT_EXIST The tensor of index is not configured with aipp
 * @retval OtherValues Failure
 *
 * @see aclmdlLoadFromFile | aclmdlLoadFromMem | aclmdlLoadFromFileWithMem |
 * aclmdlLoadFromMemWithMem | aclmdlGetInputIndexByName
*/
ACL_FUNC_VISIBILITY aclError aclmdlGetFirstAippInfo(uint32_t modelId, size_t index, aclAippInfo *aippInfo);

/**
 * @ingroup AscendCL
 * @brief get op description info
 *
 * @param deviceId [IN]       device id
 * @param streamId [IN]       stream id
 * @param taskId [IN]         task id
 * @param opName [OUT]        pointer to op name
 * @param opNameLen [IN]      the length of op name
 * @param inputDesc [OUT]     pointer to input description
 * @param numInputs [OUT]     the number of input tensor
 * @param outputDesc [OUT]    pointer to output description
 * @param numOutputs [OUT]    the number of output tensor
 *
 * @retval ACL_SUCCESS The function is successfully executed
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclmdlCreateAndGetOpDesc(uint32_t deviceId, uint32_t streamId,
    uint32_t taskId, char *opName, size_t opNameLen, aclTensorDesc **inputDesc, size_t *numInputs,
    aclTensorDesc **outputDesc, size_t *numOutputs);

/**
 * @ingroup AscendCL
 * @brief init dump
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclmdlInitDump();

/**
 * @ingroup AscendCL
 * @brief set param of dump
 *
 * @param dumpCfgPath [IN]   the path of dump config
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclmdlSetDump(const char *dumpCfgPath);

/**
 * @ingroup AscendCL
 * @brief finalize dump.
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclmdlFinalizeDump();

/**
 * @ingroup AscendCL
 * @brief load model with config
 *
 * @param handle [IN]    pointer to model config handle
 * @param modelId [OUT]  pointer to model id
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclmdlLoadWithConfig(const aclmdlConfigHandle *handle, uint32_t *modelId);

/**
 * @ingroup AscendCL
 * @brief create model config handle of type aclmdlConfigHandle
 *
 * @retval the aclmdlConfigHandle pointer
 *
 * @see aclmdlDestroyConfigHandle
*/
ACL_FUNC_VISIBILITY aclmdlConfigHandle *aclmdlCreateConfigHandle();

/**
 * @ingroup AscendCL
 * @brief destroy data of type aclmdlConfigHandle
 *
 * @param handle [IN]   pointer to model config handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclmdlCreateConfigHandle
 */
ACL_FUNC_VISIBILITY aclError aclmdlDestroyConfigHandle(aclmdlConfigHandle *handle);

/**
 * @ingroup AscendCL
 * @brief set config for model load
 *
 * @param handle [OUT]    pointer to model config handle
 * @param attr [IN]       config attr in model config handle to be set
 * @param attrValue [IN]  pointer to model config value
 * @param valueSize [IN]  memory size of attrValue
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetConfigOpt(aclmdlConfigHandle *handle, aclmdlConfigAttr attr,
    const void *attrValue, size_t valueSize);

/**
 * @ingroup AscendCL
 * @brief set config for model execute
 *
 * @param handle [OUT]    pointer to model execute config handle
 * @param attr [IN]       config attr in model execute config handle to be set
 * @param attrValue [IN]  pointer to model execute config value
 * @param valueSize [IN]  memory size of attrValue
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclmdlSetExecConfigOpt(aclmdlExecConfigHandle *handle, aclmdlExecConfigAttr attr,
                                                    const void *attrValue, size_t valueSize);

/**
 * @ingroup AscendCL
 * @brief get real tensor name from modelDesc
 *
 * @param modelDesc [IN]  pointer to modelDesc
 * @param name [IN]       tensor name
 *
 * @retval the pointer of real tensor name
 * @retval Failure return NULL
 */
ACL_FUNC_VISIBILITY const char *aclmdlGetTensorRealName(const aclmdlDesc *modelDesc, const char *name);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_MODEL_H_
// End content from: acl/acl_mdl.h

// Begin content from: acl/acl_op.h
/**
* @file acl_op.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/
#ifndef INC_EXTERNAL_ACL_ACL_OP_H_
#define INC_EXTERNAL_ACL_ACL_OP_H_

// #include "acl_base.h"
// #include "acl_rt.h"

#ifdef __cplusplus
extern "C" {
#endif

typedef struct aclopHandle aclopHandle;
typedef struct aclopAttr aclopAttr;
typedef struct aclopKernelDesc aclopKernelDesc;

typedef void (*aclDataDeallocator)(void *data, size_t length);

static const int ACL_COMPILE_FLAG_BIN_SELECTOR = 1;

typedef enum aclEngineType {
    ACL_ENGINE_SYS,
    ACL_ENGINE_AICORE,
    ACL_ENGINE_VECTOR,
} aclopEngineType;

/**
 * @ingroup AscendCL
 * @brief Set base directory that contains single op models
 *
 * @par Restriction
 * The aclopSetModelDir interface can be called only once in a process.
 * @param  modelDir [IN]   path of the directory
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetModelDir(const char *modelDir);

/**
 * @ingroup AscendCL
 * @brief load single op models from memory
 *
 * @par Restriction
 * The aclopLoad interface can be called more than one times in a process.
 * @param model [IN]        address of single op models
 * @param modelSize [IN]    size of single op models
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopLoad(const void *model, size_t modelSize);

/**
 * @ingroup AscendCL
 * @brief create data of type aclopAttr
 *
 * @retval pointer to created instance.
 * @retval nullptr if run out of memory
 */
ACL_FUNC_VISIBILITY aclopAttr *aclopCreateAttr();

/**
 * @ingroup AscendCL
 * @brief destroy data of typ aclopAttr
 *
 * @param attr [IN]   pointer to the instance of aclopAttr
 */
ACL_FUNC_VISIBILITY void aclopDestroyAttr(const aclopAttr *attr);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is bool
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param attrValue [IN]   attribute value
 *                         false if attrValue is 0, true otherwise.
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrBool(aclopAttr *attr, const char *attrName, uint8_t attrValue);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is int64_t
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param attrValue [IN]   attribute value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrInt(aclopAttr *attr, const char *attrName, int64_t attrValue);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is float
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param attrValue [IN]   attribute value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrFloat(aclopAttr *attr, const char *attrName, float attrValue);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is string
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param attrValue [IN]   attribute value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrString(aclopAttr *attr, const char *attrName, const char *attrValue);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is aclDataType
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param attrValue [IN]   attribute value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrDataType(aclopAttr *attr, const char *attrName, aclDataType attrValue);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of aclDataType
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numValues [IN]   number of values. false if attrValue is 0, true otherwise.
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListDataType(aclopAttr *attr, const char *attrName, int numValues,
    const aclDataType values[]);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of bools
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numValues [IN]   number of values. false if attrValue is 0, true otherwise.
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListBool(aclopAttr *attr, const char *attrName, int numValues,
    const uint8_t *values);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of ints
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numValues [IN]   number of values
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListInt(aclopAttr *attr, const char *attrName, int numValues,
    const int64_t *values);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of floats
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numValues [IN]   number of values
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListFloat(aclopAttr *attr, const char *attrName, int numValues,
    const float *values);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of strings
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numValues [IN]   number of values
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListString(aclopAttr *attr, const char *attrName, int numValues,
    const char **values);

/**
 * @ingroup AscendCL
 * @brief set an attribute. the type of the attribute is list of list of ints
 *
 * @param attr [OUT]       pointer to the instance of aclopAttr
 * @param attrName [IN]    attribute name
 * @param numLists [IN]    number of lists
 * @param numValues [IN]   pointer to number of values of each list
 * @param values [IN]      pointer to values
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetAttrListListInt(aclopAttr *attr,
                                                     const char *attrName,
                                                     int numLists,
                                                     const int *numValues,
                                                     const int64_t *const values[]);

/**
 * @ingroup AscendCL
 * @brief Load and execute the specified operator asynchronously
 *
 * @par Restriction
 * @li The input and output organization of each operator is different,
 * and the application needs to organize the operator strictly
 * according to the operator input and output parameters when calling.
 * @li When the user calls aclopExecute,
 * the ACL finds the corresponding task according to the optype,
 * the description of the input tesnsor,
 * the description of the output tesnsor, and attr, and issues the execution.
 *
 * @param opType [IN]      type of op
 * @param numInputs [IN]   number of inputs
 * @param inputDesc [IN]   pointer to array of input tensor descriptions
 * @param inputs [IN]      pointer to array of input buffers
 * @param numOutputs [IN]  number of outputs
 * @param outputDesc [IN]  pointer to array of output tensor descriptions
 * @param outputs [OUT]    pointer to array of output buffers
 * @param attr [IN]        pointer to instance of aclopAttr.
 *                         may pass nullptr if the op has no attribute
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_DEPRECATED_MESSAGE("aclopExecute is deprecated, use aclopExecuteV2 instead")
ACL_FUNC_VISIBILITY aclError aclopExecute(const char *opType,
                                          int numInputs,
                                          const aclTensorDesc *const inputDesc[],
                                          const aclDataBuffer *const inputs[],
                                          int numOutputs,
                                          const aclTensorDesc *const outputDesc[],
                                          aclDataBuffer *const outputs[],
                                          const aclopAttr *attr,
                                          aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief Load and execute the specified operator
 *        The difference with aclopExecute is that aclopExecuteV2 will refresh outputDesc
 *
 * @par Restriction
 * @li The input and output organization of each operator is different,
 * and the application needs to organize the operator strictly
 * according to the operator input and output parameters when calling.
 * @li When the user calls aclopExecuteV2,
 * the ACL finds the corresponding task according to the optype,
 * the description of the input tesnsor,
 * the description of the output tesnsor, and attr, and issues the execution.
 *
 * @param opType [IN]      type of op
 * @param numInputs [IN]   number of inputs
 * @param inputDesc [IN]   pointer to array of input tensor descriptions
 * @param inputs [IN]      pointer to array of input buffers
 * @param numOutputs [IN]  number of outputs
 * @param outputDesc [IN|OUT]  pointer to array of output tensor descriptions
 * @param outputs [OUT]    pointer to array of output buffers
 * @param attr [IN]        pointer to instance of aclopAttr.
 *                         may pass nullptr if the op has no attribute
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopExecuteV2(const char *opType,
                                            int numInputs,
                                            aclTensorDesc *inputDesc[],
                                            aclDataBuffer *inputs[],
                                            int numOutputs,
                                            aclTensorDesc *outputDesc[],
                                            aclDataBuffer *outputs[],
                                            aclopAttr *attr,
                                            aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a instance of aclopHandle.
 *
 * @param opType [IN]      type of op
 * @param numInputs [IN]   number of inputs
 * @param inputDesc [IN]   pointer to array of input tensor descriptions
 * @param numOutputs [IN]  number of outputs
 * @param outputDesc [IN]  pointer to array of output tensor descriptions
 * @param opAttr [IN]      pointer to instance of aclopAttr.
 *                         may pass nullptr if the op has no attribute
 * @param handle [OUT]     pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCreateHandle(const char *opType,
                                               int numInputs,
                                               const aclTensorDesc *const inputDesc[],
                                               int numOutputs,
                                               const aclTensorDesc *const outputDesc[],
                                               const aclopAttr *opAttr,
                                               aclopHandle **handle);

/**
 * @ingroup AscendCL
 * @brief destroy aclopHandle instance
 *
 * @param handle [IN]   pointer to the instance of aclopHandle
 */
ACL_FUNC_VISIBILITY void aclopDestroyHandle(aclopHandle *handle);

/**
 * @ingroup AscendCL
 * @brief execute an op with the handle.
 *        can save op model matching cost compared with aclopExecute
 *
 * @param handle [IN]      pointer to the instance of aclopHandle.
 *                         The aclopCreateHandle interface has been called
 *                         in advance to create aclopHandle type data.
 * @param numInputs [IN]   number of inputs
 * @param inputs [IN]      pointer to array of input buffers.
 *                         The aclCreateDataBuffer interface has been called
 *                         in advance to create aclDataBuffer type data.
 * @param numOutputs [IN]  number of outputs
 * @param outputs [OUT]    pointer to array of output buffers
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclopCreateHandle | aclCreateDataBuffer
 */
ACL_FUNC_VISIBILITY aclError aclopExecWithHandle(aclopHandle *handle,
                                                 int numInputs,
                                                 const aclDataBuffer *const inputs[],
                                                 int numOutputs,
                                                 aclDataBuffer *const outputs[],
                                                 aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief cast data type
 *
 * @param srcDesc [IN]     source tensor desc
 * @param srcBuffer [IN]   source tensor buffer
 * @param dstDesc [IN]     destination tensor desc
 * @param dstBuffer [OUT]  destination tensor buffer
 * @param truncate [IN]    do not truncate if value is 0, truncate otherwise
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCast(const aclTensorDesc *srcDesc,
                                       const aclDataBuffer *srcBuffer,
                                       const aclTensorDesc *dstDesc,
                                       aclDataBuffer *dstBuffer,
                                       uint8_t truncate,
                                       aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a handle for casting datatype
 *
 * @param srcDesc [IN]    source tensor desc
 * @param dstDesc [IN]    destination tensor desc
 * @param truncate [IN]   do not truncate if value is 0, truncate otherwise
 * @param handle [OUT]    pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCreateHandleForCast(aclTensorDesc *srcDesc,
                                                      aclTensorDesc *dstDesc,
                                                      uint8_t truncate,
                                                      aclopHandle **handle);


/**
 * @ingroup AscendCL
 * @brief create kernel
 *
 * @param opType [IN]           op type
 * @param kernelId [IN]         kernel id
 * @param kernelName [IN]       kernel name
 * @param binData [IN]          kernel bin data
 * @param binSize [IN]          kernel bin size
 * @param enginetype [IN]       enigne type
 * @param deallocator [IN]      callback function for deallocating bin data,
 *                              null if bin data to be deallocated by caller
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclopCompile
 */
ACL_FUNC_VISIBILITY aclError aclopCreateKernel(const char *opType,
                                               const char *kernelId,
                                               const char *kernelName,
                                               void *binData,
                                               int binSize,
                                               aclopEngineType enginetype,
                                               aclDataDeallocator deallocator);


/**
 * @ingroup AscendCL
 * @brief create kernel
 *
 * @param numInputs [IN]            number of inputs
 * @param inputDesc [IN]            pointer to array of input tensor descriptions
 * @param numOutputs [IN]           number of outputs
 * @param outputDesc [IN]           pointer to array of output tensor descriptions
 * @param opAttr [IN]               pointer to instance of aclopAttr
 * @param aclopKernelDesc [IN]      pointer to instance of aclopKernelDesc
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
typedef aclError (*aclopCompileFunc)(int numInputs,
                                     const aclTensorDesc *const inputDesc[],
                                     int numOutputs,
                                     const aclTensorDesc *const outputDesc[],
                                     const aclopAttr *opAttr,
                                     aclopKernelDesc *aclopKernelDesc);

/**
 * @ingroup AscendCL
 * @brief register compile function
 *
 * @param opType [IN]         op type
 * @param func [IN]           compile function
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclopUnregisterCompileFunc
 */
ACL_FUNC_VISIBILITY aclError aclopRegisterCompileFunc(const char *opType, aclopCompileFunc func);

/**
 * @ingroup AscendCL
 * @brief unregister compile function
 *
 * @param opType [IN]         op type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopUnregisterCompileFunc(const char *opType);

/**
 * @ingroup AscendCL
 * @brief set kernel args
 *
 * @param kernelDesc [IN]               pointer to instance of aclopKernelDesc
 * @param kernelId [IN]                 kernel id
 * @param blockDim [IN]                 block dim
 * @param args [IN]                     args
 * @param argSize [IN]                  size in bytes of args
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetKernelArgs(aclopKernelDesc *kernelDesc,
                                                const char *kernelId,
                                                uint32_t blockDim,
                                                const void *args,
                                                uint32_t argSize);

/**
 * @ingroup AscendCL
 * @brief set workspace sizes
 *
 * @param kernelDesc [IN]               pointer to instance of aclopKernelDesc
 * @param numWorkspaces [IN]            number of workspaces
 * @param workspaceSizes [IN]           pointer to array of sizes of workspaces
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetKernelWorkspaceSizes(aclopKernelDesc *kernelDesc, int numWorkspaces,
                                                          size_t *workspaceSizes);

/**
 * @ingroup AscendCL
 * @brief compile op with dynamic shape
 *
 * @param opType [IN]       op type
 * @param numInputs [IN]    number of inputs
 * @param inputDesc [IN]    pointer to array of input tensor descriptions
 * @param numOutputs [IN]   number of outputs
 * @param outputDesc [IN]   pointer to array of output tensor descriptions
 * @param attr [IN]         pointer to instance of aclopAttr.
 *                          may pass nullptr if the op has no attribute
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopUpdateParams(const char *opType,
                                               int numInputs,
                                               const aclTensorDesc *const inputDesc[],
                                               int numOutputs,
                                               const aclTensorDesc *const outputDesc[],
                                               const aclopAttr *attr);

/**
 * @ingroup AscendCL
 * @brief set max op queue num
 *
 * @param maxOpNum [IN]   number of max op queue
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetMaxOpQueueNum(uint64_t maxOpNum);

/**
 * @ingroup AscendCL
 * @brief inferShape the specified operator synchronously
 *
 * @param opType [IN]       type of op
 * @param numInputs [IN]    number of inputs
 * @param inputDesc [IN]    pointer to array of input tensor descriptions
 * @param inputs [IN]       pointer to array of input buffers
 * @param numOutputs [IN]   number of outputs
 * @param outputDesc [OUT]  pointer to array of output tensor descriptions
 * @param attr [IN]         pointer to instance of aclopAttr.
 *                          may pass nullptr if the op has no attribute
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopInferShape(const char *opType,
                                             int numInputs,
                                             aclTensorDesc *inputDesc[],
                                             aclDataBuffer *inputs[],
                                             int numOutputs,
                                             aclTensorDesc *outputDesc[],
                                             aclopAttr *attr);

#define ACL_OP_DUMP_OP_AICORE_ARGS 0x00000001U

/**
 * @ingroup AscendCL
 * @brief Enable the dump function of the corresponding dump type.
 *
 * @param dumpType [IN]       type of dump
 * @param path     [IN]       dump path
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopStartDumpArgs(uint32_t dumpType, const char *path);

/**
 * @ingroup AscendCL
 * @brief Disable the dump function of the corresponding dump type.
 *
 * @param dumpType [IN]       type of dump
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopStopDumpArgs(uint32_t dumpType);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_OP_H_
// End content from: acl/acl_op.h

// Begin content from: aclnn/acl_meta.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_OP_API_COMMON_INC_EXTERNAL_ACL_META_H
#define OP_API_OP_API_COMMON_INC_EXTERNAL_ACL_META_H

#include <cstdint>
#include <cstdlib>
// #include <acl/acl_base.h>

#ifdef __cplusplus
extern "C" {
#endif

#if defined(_MSC_VER)
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY _declspec(dllexport)
#else
#define ACL_FUNC_VISIBILITY
#endif
#else
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY __attribute__((visibility("default")))
#else
#define ACL_FUNC_VISIBILITY
#endif
#endif

#ifdef __GNUC__
#define ACL_DEPRECATED __attribute__((deprecated))
#define ACL_DEPRECATED_MESSAGE(message) __attribute__((deprecated(message)))
#elif defined(_MSC_VER)
#define ACL_DEPRECATED __declspec(deprecated)
#define ACL_DEPRECATED_MESSAGE(message) __declspec(deprecated(message))
#else
#define ACL_DEPRECATED
#define ACL_DEPRECATED_MESSAGE(message)
#endif

#ifndef ACLNN_META
#define ACLNN_META
typedef struct aclOpExecutor aclOpExecutor;
typedef struct aclTensor aclTensor;
typedef struct aclScalar aclScalar;
typedef struct aclIntArray aclIntArray;
typedef struct aclFloatArray aclFloatArray;
typedef struct aclBoolArray aclBoolArray;
typedef struct aclTensorList aclTensorList;
typedef struct aclScalarList aclScalarList;

typedef int32_t aclnnStatus;
constexpr aclnnStatus OK = 0;
#endif

ACL_FUNC_VISIBILITY aclTensor *aclCreateTensor(const int64_t *viewDims, uint64_t viewDimsNum, aclDataType dataType,
                                               const int64_t *stride, int64_t offset, aclFormat format,
                                               const int64_t *storageDims, uint64_t storageDimsNum,
                                               void *tensorData);

ACL_FUNC_VISIBILITY aclScalar *aclCreateScalar(void *value, aclDataType dataType);
ACL_FUNC_VISIBILITY aclIntArray *aclCreateIntArray(const int64_t *value, uint64_t size);
ACL_FUNC_VISIBILITY aclFloatArray *aclCreateFloatArray(const float *value, uint64_t size);
ACL_FUNC_VISIBILITY aclBoolArray *aclCreateBoolArray(const bool *value, uint64_t size);
ACL_FUNC_VISIBILITY aclTensorList *aclCreateTensorList(const aclTensor *const *value, uint64_t size);
ACL_FUNC_VISIBILITY aclScalarList *aclCreateScalarList(const aclScalar *const *value, uint64_t size);

ACL_FUNC_VISIBILITY aclnnStatus aclDestroyTensor(const aclTensor *tensor);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyScalar(const aclScalar *scalar);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyIntArray(const aclIntArray *array);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyFloatArray(const aclFloatArray *array);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyBoolArray(const aclBoolArray *array);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyTensorList(const aclTensorList *array);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyScalarList(const aclScalarList *array);
ACL_FUNC_VISIBILITY aclnnStatus aclGetViewShape(const aclTensor *tensor, int64_t **viewDims, uint64_t *viewDimsNum);
ACL_FUNC_VISIBILITY aclnnStatus aclGetStorageShape(const aclTensor *tensor,
                                                   int64_t **storageDims,
                                                   uint64_t *storageDimsNum);
ACL_FUNC_VISIBILITY aclnnStatus aclGetViewStrides(const aclTensor *tensor,
                                                  int64_t **stridesValue,
                                                  uint64_t *stridesNum);
ACL_FUNC_VISIBILITY aclnnStatus aclGetViewOffset(const aclTensor *tensor, int64_t *offset);
ACL_FUNC_VISIBILITY aclnnStatus aclGetFormat(const aclTensor *tensor, aclFormat *format);
ACL_FUNC_VISIBILITY aclnnStatus aclGetDataType(const aclTensor *tensor, aclDataType *dataType);
ACL_FUNC_VISIBILITY aclnnStatus aclGetIntArraySize(const aclIntArray *array, uint64_t *size);
ACL_FUNC_VISIBILITY aclnnStatus aclGetFloatArraySize(const aclFloatArray *array, uint64_t *size);
ACL_FUNC_VISIBILITY aclnnStatus aclGetBoolArraySize(const aclBoolArray *array, uint64_t *size);
ACL_FUNC_VISIBILITY aclnnStatus aclGetTensorListSize(const aclTensorList *tensorList, uint64_t *size);
ACL_FUNC_VISIBILITY aclnnStatus aclGetScalarListSize(const aclScalarList *scalarList, uint64_t *size);

ACL_FUNC_VISIBILITY aclnnStatus aclInitTensor(aclTensor *tensor, const int64_t *viewDims, uint64_t viewDimsNum,
                                              aclDataType dataType, const int64_t *stride, int64_t offset,
                                              aclFormat format, const int64_t *storageDims, uint64_t storageDimsNum,
                                              void *tensorDataAddr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetAclOpExecutorRepeatable(aclOpExecutor *executor);
ACL_FUNC_VISIBILITY aclnnStatus aclDestroyAclOpExecutor(aclOpExecutor *executor);
ACL_FUNC_VISIBILITY aclnnStatus AclSetInputTensorAddr(aclOpExecutor *executor, const size_t index,
                                                      aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus AclSetOutputTensorAddr(aclOpExecutor *executor, const size_t index,
                                                       aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus AclSetDynamicInputTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                             const size_t relativeIndex,
                                                             aclTensorList *tensors, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus AclSetDynamicOutputTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                              const size_t relativeIndex,
                                                              aclTensorList *tensors, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus AclSetTensorAddr(aclOpExecutor *executor, const size_t index,
                                                 aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus AclSetDynamicTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                        const size_t relativeIndex,
                                                        aclTensorList *tensors, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetInputTensorAddr(aclOpExecutor *executor, const size_t index,
                                                      aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetOutputTensorAddr(aclOpExecutor *executor, const size_t index,
                                                       aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetDynamicInputTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                             const size_t relativeIndex,
                                                             aclTensorList *tensors, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetDynamicOutputTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                              const size_t relativeIndex,
                                                              aclTensorList *tensors, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetTensorAddr(aclOpExecutor *executor, const size_t index,
                                                 aclTensor *tensor, void *addr);
ACL_FUNC_VISIBILITY aclnnStatus aclSetDynamicTensorAddr(aclOpExecutor *executor, size_t irIndex,
                                                        const size_t relativeIndex,
                                                        aclTensorList *tensors, void *addr);
#ifdef __cplusplus
}
#endif

#endif // OP_API_OP_API_COMMON_INC_EXTERNAL_ACL_META_H
// End content from: aclnn/acl_meta.h

// Begin content from: acl/acl.h
/**
* @file acl.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/

#ifndef INC_EXTERNAL_ACL_ACL_H_
#define INC_EXTERNAL_ACL_ACL_H_

// #include "acl_rt.h"
// #include "acl_op.h"
// #include "acl_mdl.h"

#ifdef __cplusplus
extern "C" {
#endif

// Current version is 1.12.0
#define ACL_MAJOR_VERSION    1
#define ACL_MINOR_VERSION    12
#define ACL_PATCH_VERSION    0

/**
 * @ingroup AscendCL
 * @brief acl initialize
 *
 * @par Restriction
 * The aclInit interface can be called only once in a process
 * @param configPath [IN]    the config path,it can be NULL
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclInit(const char *configPath);

/**
 * @ingroup AscendCL
 * @brief acl finalize
 *
 * @par Restriction
 * Need to call aclFinalize before the process exits.
 * After calling aclFinalize,the services cannot continue to be used normally.
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclFinalize();

/**
 * @ingroup AscendCL
 * @brief query ACL interface version
 *
 * @param majorVersion[OUT] ACL interface major version
 * @param minorVersion[OUT] ACL interface minor version
 * @param patchVersion[OUT] ACL interface patch version
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclrtGetVersion(int32_t *majorVersion, int32_t *minorVersion, int32_t *patchVersion);

/**
 * @ingroup AscendCL
 * @brief get recent error message
 *
 * @retval null for failed
 * @retval OtherValues success
*/
ACL_FUNC_VISIBILITY const char *aclGetRecentErrMsg();

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_H_
// End content from: acl/acl.h

// Begin content from: hccl/hccl_types.h
/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef HCCL_TYPES_H_
#define HCCL_TYPES_H_

#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif // __cplusplus

/**
 * @brief HCCL functions return value definition
 */
typedef enum {
    HCCL_SUCCESS = 0,               /**< success */
    HCCL_E_PARA = 1,                /**< parameter error */
    HCCL_E_PTR = 2,                 /**< empty pointer */
    HCCL_E_MEMORY = 3,              /**< memory error */
    HCCL_E_INTERNAL = 4,            /**< internal error */
    HCCL_E_NOT_SUPPORT = 5,         /**< not support feature */
    HCCL_E_NOT_FOUND = 6,           /**< not found specific resource */
    HCCL_E_UNAVAIL = 7,             /**< resource unavailable */
    HCCL_E_SYSCALL = 8,             /**< call system interface error */
    HCCL_E_TIMEOUT = 9,             /**< timeout */
    HCCL_E_OPEN_FILE_FAILURE = 10,  /**< open file fail */
    HCCL_E_TCP_CONNECT = 11,        /**< tcp connect fail */
    HCCL_E_ROCE_CONNECT = 12,       /**< roce connect fail */
    HCCL_E_TCP_TRANSFER = 13,       /**< tcp transfer fail */
    HCCL_E_ROCE_TRANSFER = 14,      /**< roce transfer fail */
    HCCL_E_RUNTIME = 15,            /**< call runtime api fail */
    HCCL_E_DRV = 16,                /**< call driver api fail */
    HCCL_E_PROFILING = 17,          /**< call profiling api fail */
    HCCL_E_CCE = 18,                /**< call cce api fail */
    HCCL_E_NETWORK = 19,            /**< call network api fail */
    HCCL_E_AGAIN = 20,              /**< try again */
    HCCL_E_REMOTE = 21,             /**< error cqe */
    HCCL_E_SUSPENDING = 22,         /**< error communicator suspending */
    HCCL_E_RESERVED                 /**< reserved */
} HcclResult;

/**
 * @brief handle to HCCL communicator
 */
typedef void *HcclComm;

/**
 * @brief HCCL Reduction opperation
 */
typedef enum {
    HCCL_REDUCE_SUM = 0,    /**< sum */
    HCCL_REDUCE_PROD = 1,   /**< prod */
    HCCL_REDUCE_MAX = 2,    /**< max */
    HCCL_REDUCE_MIN = 3,    /**< min */
    HCCL_REDUCE_RESERVED    /**< reserved */
} HcclReduceOp;

/**
 * @brief HCCL data type
 */
typedef enum {
    HCCL_DATA_TYPE_INT8 = 0,    /**< int8 */
    HCCL_DATA_TYPE_INT16 = 1,   /**< int16 */
    HCCL_DATA_TYPE_INT32 = 2,   /**< int32 */
    HCCL_DATA_TYPE_FP16 = 3,    /**< fp16 */
    HCCL_DATA_TYPE_FP32 = 4,    /**< fp32 */
    HCCL_DATA_TYPE_INT64 = 5,    /**< int64 */
    HCCL_DATA_TYPE_UINT64 = 6,    /**< uint64 */
    HCCL_DATA_TYPE_UINT8 = 7,    /**< uint8 */
    HCCL_DATA_TYPE_UINT16 = 8,   /**< uint16 */
    HCCL_DATA_TYPE_UINT32 = 9,   /**< uint32 */
    HCCL_DATA_TYPE_FP64 = 10, /**< fp64 */
    HCCL_DATA_TYPE_BFP16 = 11,    /**< bfp16 */
    HCCL_DATA_TYPE_INT128 = 12,   /**< int128 */
    HCCL_DATA_TYPE_RESERVED     /**< reserved */
} HcclDataType;

typedef enum {
    HCCL_DETERMINISTIC = 0,     /**< 0: non-deterministic, 1: deterministic */
    HCCL_CONFIG_RESERVED
} HcclConfig;


union HcclConfigValue {
    int32_t value;
};

const uint32_t HCCL_ROOT_INFO_BYTES =  4108; // 4108: root info length
const uint32_t COMM_NAME_MAX_LENGTH = 128; // group name max length
const uint32_t UDI_MAX_LENGTH = 128; // UDI max length
/**
 * @brief HCCL root info
 */
typedef struct HcclRootInfoDef {
    char internal[HCCL_ROOT_INFO_BYTES];
} HcclRootInfo;

const uint32_t HCCL_COMM_CONFIG_INFO_BYTES = 24;
const uint32_t HCCL_COMM_CONFIG_MAGIC_WORD = 0xf0f0f0f0;
const uint32_t HCCL_COMM_CONFIG_VERSION = 3;
const uint32_t HCCL_COMM_DEFAULT_BUFFSIZE = 200;
const uint32_t HCCL_COMM_DEFAULT_DETERMINISTIC = 0;

typedef struct HcclCommConfigDef {
    char reserved[HCCL_COMM_CONFIG_INFO_BYTES];
    uint32_t hcclBufferSize;
    uint32_t hcclDeterministic;
    char hcclCommName[COMM_NAME_MAX_LENGTH];
    char hcclUdi[UDI_MAX_LENGTH];
} HcclCommConfig;

typedef enum {
    HCCL_COMM_CONFIG_BUFFER_SIZE= 0,
    HCCL_COMM_CONFIG_DETERMINISTIC = 1,
    HCCL_COMM_CONFIG_COMM_NAME = 2,
    HCCL_COMM_CONFIG_RESERVED
} HcclCommConfigCapability;

typedef enum {
    HCCL_SEND = 0,
    HCCL_RECV = 1,
    HCCL_SEND_RECV_RESERVED
} HcclSendRecvType;

typedef struct HcclSendRecvItemDef {
    HcclSendRecvType sendRecvType;
    void *buf;
    uint64_t count;
    HcclDataType dataType;
    uint32_t remoteRank;
} HcclSendRecvItem;

#ifdef __cplusplus
}
#endif // __cplusplus
#endif // HCCL_TYPES_H_
// End content from: hccl/hccl_types.h

// Begin content from: aclnn_util.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_UTIL_H
#define OP_API_INC_ACLNN_UTIL_H
 
#define ACLNN_API __attribute__((visibility("default")))
#endif // OP_API_INC_ACLNN_UTIL_H// End content from: aclnn_util.h

// Begin content from: aclnn/aclnn_base.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_OP_API_COMMON_INC_EXTERNAL_ACLNN_BASE_H
#define OP_API_OP_API_COMMON_INC_EXTERNAL_ACLNN_BASE_H

#include <cstdint>
#include <cstdlib>
// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

#if defined(_MSC_VER)
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY _declspec(dllexport)
#else
#define ACL_FUNC_VISIBILITY
#endif
#else
#ifdef FUNC_VISIBILITY
#define ACL_FUNC_VISIBILITY __attribute__((visibility("default")))
#else
#define ACL_FUNC_VISIBILITY
#endif
#endif

#ifdef __GNUC__
#define ACL_DEPRECATED __attribute__((deprecated))
#define ACL_DEPRECATED_MESSAGE(message) __attribute__((deprecated(message)))
#elif defined(_MSC_VER)
#define ACL_DEPRECATED __declspec(deprecated)
#define ACL_DEPRECATED_MESSAGE(message) __declspec(deprecated(message))
#else
#define ACL_DEPRECATED
#define ACL_DEPRECATED_MESSAGE(message)
#endif

ACL_FUNC_VISIBILITY aclnnStatus aclnnInit(const char *configPath);
ACL_FUNC_VISIBILITY aclnnStatus aclnnFinalize();

#ifdef __cplusplus
}
#endif

#endif // OP_API_OP_API_COMMON_INC_EXTERNAL_ACLNN_BASE_H
// End content from: aclnn/aclnn_base.h

// Begin content from: hccl/hccl.h
/*
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef HCCL_H_
#define HCCL_H_

// #include <hccl/hccl_types.h>
// #include <acl/acl.h>

#ifdef __cplusplus
extern "C" {
#endif // __cplusplus

/**
 * @brief Initialize HCCL.
 *
 * @param clusterInfo A string identifying the cluster info file path, include file name.
 * @param rank A integer identifying the identify for the rank.
 * @param comm A pointer identifying the initialized communication resource.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclCommInitClusterInfo(const char *clusterInfo, uint32_t rank, HcclComm *comm);

/**
 * @brief Initialize HCCL with config params.
 *
 * @param clusterInfo A string identifying the cluster info file path, include file name.
 * @param rank A integer identifying the identify for the rank.
 * @param config A pointer identifying config params about the current comm.
 * @param comm A pointer identifying the initialized communication resource.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclCommInitClusterInfoConfig(const char *clusterInfo, uint32_t rank,
    HcclCommConfig *config, HcclComm *comm);

/**
 * @brief Initialize HCCL sub communication based on global communication with config params.
 *
 * @param comm A pointer identifying the global communication resource.
 * @param rankNum A integer identifying the rank size of the sub communication.
 * @param rankIds A array identifying the identifies for the ranks in the sub communication.
 * @param subCommId A integer identifying the identify of sub communication in global communication.
 * @param subCommRankId A array identifying the identify for the rank in the sub communication.
 * @param config A pointer identifying config params about the current comm.
 * @param comm A pointer identifying the initialized communication resource.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclCreateSubCommConfig(HcclComm *comm, uint32_t rankNum, uint32_t *rankIds,
    uint64_t subCommId, uint32_t subCommRankId, HcclCommConfig *config, HcclComm *subComm);

/**
 * @brief Get hccl root info.
 *
 * @param rootInfo A pointer identifying the hccl root info.
 * @return HcclResult
 */
extern HcclResult HcclGetRootInfo(HcclRootInfo *rootInfo);

/**
 * @brief Initialize HCCL with root info.
 *
 * @param nRanks A integer identifying the rank size of the cluster.
 * @param rootInfo A struct identifying the hccl root info.
 * @param rank A integer identifying the identify for the rank.
 * @param comm A pointer identifying the initialized communication resource.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclCommInitRootInfo(uint32_t nRanks, const HcclRootInfo *rootInfo, uint32_t rank, HcclComm *comm);

/**
 * @brief Initialize HCCL with root info and config params.
 *
 * @param nRanks A integer identifying the rank size of the cluster.
 * @param rootInfo A struct identifying the hccl root info.
 * @param rank A integer identifying the identify for the rank.
 * @param config A pointer identifying config params about the current comm.
 * @param comm A pointer identifying the initialized communication resource.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclCommInitRootInfoConfig(uint32_t nRanks, const HcclRootInfo *rootInfo, uint32_t rank,
    const HcclCommConfig *config, HcclComm *comm);

/* *

 * @brief Set deterministic calculate
 *
 * @param config A struct identifying the Config
 * @param configValue An interger identifying the identify for the config.
 */

extern HcclResult HcclSetConfig(HcclConfig config, HcclConfigValue configValue);
extern HcclResult HcclGetConfig(HcclConfig config, HcclConfigValue *configValue);

/**

 * @brief get commName.
 *
 * @param commhandle A pointer identifying the initialized communication resource.
 * @param commName The name of commhandle.
 * @return HcclResult
 * @see HcclCommDestroy()
 */
extern HcclResult HcclGetCommName(HcclComm comm, char* commName);


/**
 * @brief AllReduce operator.
 *
 * @param sendBuf A pointer identifying the input data address of the operator.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param count An integer(u64) identifying the number of the output data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32,
float16, float32, bfloat16.
 * @param op The reduction type of the operator, must be one of the following types: sum, min, max, prod.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclAllReduce(void *sendBuf, void *recvBuf, uint64_t count, HcclDataType dataType,
    HcclReduceOp op, HcclComm comm, aclrtStream stream);

/**
 * @brief Broadcast operator.
 *
 * @param buf A pointer identifying the data address of the operator.
 * @param count An integer(u64) identifying the number of the data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param root An integer(u32) identifying the the root rank in the operator.
 * @param comm A pointer identifying the communication resource based on
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclBroadcast(void *buf, uint64_t count, HcclDataType dataType, uint32_t root, HcclComm comm,
    aclrtStream stream);

/**
 * @brief ReduceScatter operator.
 *
 * @param sendBuf A pointer identifying the input data address of the operator.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param recvCount An integer(u64) identifying the number of the output data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int32,
 float16, float32, bfloat16.
 * @param op The reduction type of the operator, must be one of the following types: sum, min, max, prod.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclReduceScatter(void *sendBuf, void *recvBuf, uint64_t recvCount, HcclDataType dataType,
    HcclReduceOp op, HcclComm comm, aclrtStream stream);

/**
 * @brief Scatter operator.
 *
 * @param sendBuf A pointer identifying the input data address of the operator.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param recvCount An integer(u64) identifying the number of the data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int32, float16, float32.
 * @param root An integer(u32) identifying the the root rank in the operator.
 * @param comm A pointer identifying the communication resource based on
 * @param stream A pointer identifying the stream information.
 * @return HcclResult 
 */
extern HcclResult HcclScatter(void *sendBuf, void *recvBuf, uint64_t recvCount, HcclDataType dataType, uint32_t root,
    HcclComm comm, aclrtStream stream);

/**
 * @brief AllGather operator.
 *
 * @param sendBuf A pointer identifying the input data address of the operator.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param sendCount An integer(u64) identifying the number of the input data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclAllGather(void *sendBuf, void *recvBuf, uint64_t sendCount, HcclDataType dataType,
    HcclComm comm, aclrtStream stream);
/**
 * @brief Get the rank size of this comm.
 *
 * @param comm A pointer identifying the communication resource based on.
 * @param rankSize  A pointer identifying the rank size.
 * @return HcclResult
 */
extern HcclResult HcclGetRankSize(HcclComm comm, uint32_t *rankSize);

/**
 * @brief Get the rank id of this comm.
 *
 * @param comm A pointer identifying the communication resource based on.
 * @param rankSize  A pointer identifying the rank id.
 * @return HcclResult
 */
extern HcclResult HcclGetRankId(HcclComm comm, uint32_t *rank);
/**
 * @brief Barrier operator.
 *
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclBarrier(HcclComm comm, aclrtStream stream);

/**
 * @brief Send operator.
 *
 * @param sendBuff A pointer identifying the input data address of the operator.
 * @param count An integer(u64) identifying the number of the send data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param destRank An integer identifying the destination rank.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclSend(void* sendBuf, uint64_t count, HcclDataType dataType, uint32_t destRank,
                           HcclComm comm, aclrtStream stream);
/**
 * @brief Recv operator.
 *
 * @param recvBuff A pointer identifying the output data address of the operator.
 * @param count An integer(u64) identifying the number of the receive data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param srcRank An integer identifying the source rank.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclRecv(void* recvBuf, uint64_t count, HcclDataType dataType, uint32_t srcRank,
                           HcclComm comm, aclrtStream stream);

/**
 * @brief AlltoAllV operator.
 *
 * @param sendBuff A pointer identifying the input data address of the operator.
 * @param sendCounts Integer array, where entry i specifies the number of elements to send to rank i.
 * @param sdispls Integer array, where entry i specifies the displacement (offset from sendbuf, in units of sendtype)
from which to send data to rank i.
 * @param sendType Datatype of send buffer elements, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param recvCounts Integer array, where entry j specifies the number of elements to receive from rank j.
 * @param rdispls Integer array, where entry j specifies the displacement (offset from recvbuf, in units of recvtype)
 to which data from rank j should be written.
 * @param recvType Datatype of receive buffer elements, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclAlltoAllV(const void *sendBuf, const void *sendCounts, const void *sdispls, HcclDataType sendType,
                         const void *recvBuf, const void *recvCounts, const void *rdispls, HcclDataType recvType,
                         HcclComm comm, aclrtStream stream);

/**
 * @brief AlltoAll operator.
 *
 * @param sendBuff A pointer identifying the input data address of the operator.
 * @param sendCount Integer, number of elements to send to each proces.
 * @param sendType Datatype of send buffer elements, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64, bfloat16.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param recvCount Integer, number of elements received from any process.
 * @param recvType Datatype of receive buffer elements, must be one of the following types: int8, int16, int32, int64,
uint8, uint16, uint32, uint64, float16, float32, float64.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclAlltoAll(const void *sendBuf, uint64_t sendCount, HcclDataType sendType,
                               const void *recvBuf, uint64_t recvCount, HcclDataType recvType,
                               HcclComm comm, aclrtStream stream);

/**
 * @brief Reduce operator.
 *
 * @param sendBuf A pointer identifying the input data address of the operator.
 * @param recvBuf A pointer identifying the output data address of the operator.
 * @param count An integer(u64) identifying the number of the output data.
 * @param dataType The data type of the operator, must be one of the following types: int8, int16, int32, float16,
 float32, bfloat16.
 * @param op The reduction type of the operator, must be one of the following types: sum, min, max, prod.
 * @param root An integer(u32) identifying the the root rank in the operator.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
 * @return HcclResult
 */
extern HcclResult HcclReduce(void *sendBuf, void *recvBuf, uint64_t count, HcclDataType dataType,
                             HcclReduceOp op, uint32_t root, HcclComm comm, aclrtStream stream);

/**
 * @brief Destroy HCCL comm
 *
 * @param comm A pointer identifying the communication resource targetting
 * @return HcclResult
 * @see HcclCommInitClusterInfo()
 */
extern HcclResult HcclCommDestroy(HcclComm comm);

/**
 * @brief Create a single-process multi-npu communication domain. Cross-machine is not supported.
 *
 * @param ndev: the number of NPUs in a communication domain.
 * @param devices: Indicates the NPU list in the communication domain. The value is the device logic ID.
 The communication library creates communication domains in the sequence of devices.
 * @param comms: Generated communication domain handle, size: ndev * sizeof(HcclComm)
 * @return HcclResult
 */
extern HcclResult HcclCommInitAll(uint32_t ndev, int32_t* devices, HcclComm* comms);

/**
 * @brief Get hccl error.
 * @param comm A pointer identifying the communication resource based on.
 * @param asyncError A pointer identifying the communication error.
*/
extern HcclResult HcclGetCommAsyncError(HcclComm comm, HcclResult *asyncError);

/**
 * @brief  convert a hccl errorCode to a string.
 * @param code enum HcclResult.
*/
extern const char *HcclGetErrorString(HcclResult code);

/**
 * @brief  Batch SEND/RECV
 * @param sendRecvInfo A pointer to an send/recv item array.
 * @param itemNum The size of the send/recv item array.
 * @param comm A pointer identifying the communication resource based on.
 * @param stream A pointer identifying the stream information.
*/
extern HcclResult HcclBatchSendRecv(HcclSendRecvItem* sendRecvInfo, uint32_t itemNum, HcclComm comm, aclrtStream stream);


/**
 * @brief Initialize the comm configuration.
 * @param config Pointer to the comm configuration that needs to be initialized.
*/
inline void HcclCommConfigInit(HcclCommConfig *config)
{
    if (config == nullptr) {
        return;
    }

    typedef struct {
        size_t size;
        uint32_t magicWord;
        uint32_t version;
        uint64_t reserved;
    } configInfo_t;

    configInfo_t *info = (configInfo_t *)config;

    info->size = sizeof(HcclCommConfig);
    info->magicWord = HCCL_COMM_CONFIG_MAGIC_WORD;
    info->version = HCCL_COMM_CONFIG_VERSION;
    info->reserved = 0;

    config->hcclBufferSize = HCCL_COMM_DEFAULT_BUFFSIZE;
    config->hcclDeterministic = HCCL_COMM_DEFAULT_DETERMINISTIC;
    config->hcclCommName[0] = '\0';
}

/**
 * @brief Suspend communication.
 * @param comm A pointer identifying the communication resource based on.
*/
extern HcclResult HcclCommSuspend(HcclComm comm);
 
/**
 * @brief Clear and recover communication.
 * @param comm A pointer identifying the communication resource based on.
*/
extern HcclResult HcclCommResume(HcclComm comm);

/**
 * @brief Get a number that represents the capability of comm configuration.
*/
extern uint32_t HcclGetCommConfigCapability();

#ifdef __cplusplus
}
#endif // __cplusplus
#endif // HCCL_H
// End content from: hccl/hccl.h

// Begin content from: aclnn_dropout_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DROPOUT_BACKWARD_H_
#define OP_API_INC_DROPOUT_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDropoutBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnDropoutBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* mask,
                                                           double scale, aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnDropoutBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DROPOUT_BACKWARD_H_
// End content from: aclnn_dropout_backward.h

// Begin content from: aclnn_dropout.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DROPOUT_H_
#define OP_API_INC_DROPOUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDropout的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnDropoutGetWorkspaceSize(const aclTensor* input, double p, bool train, int64_t seed,
                                                   int64_t offset, aclTensor* out, aclTensor* maskOut,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnDropout(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DROPOUT_H_
// End content from: aclnn_dropout.h

// Begin content from: aclnn_normal_out.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NORMAL_OUT_H_
#define OP_API_INC_NORMAL_OUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnNormal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 * @param [in] mean: npu
 * device侧的aclTensor，数据类型支持浮点数据类型，且数据类型需要与std构成互相推导关系，shape需要与std满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与std一致。
 * @param [in] std: npu
 * device侧的aclTensor，数据类型支持浮点类型，且数据类型需要与mean构成互相推导关系，shape需要与mean满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与mean一致。
 * @param [in] seed: host侧的int64_t，伪随机数生成器的种子值。
 * @param [in] offset: host侧的int64_t， 伪随机数生成器的偏移量。
 * @param [out] out: host侧的int64_t, 输出张量。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNormalTensorTensorGetWorkspaceSize(const aclTensor* mean, const aclTensor* std, int64_t seed,
                                                              int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                              aclOpExecutor** executor);

/**
 * @brief aclnnNormalTensorFloat的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnNormalTensorFloatGetWorkspaceSize(const aclTensor* mean, float std, int64_t seed,
                                                             int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnNormalFloatTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnNormalFloatTensorGetWorkspaceSize(float mean, const aclTensor* std, int64_t seed,
                                                             int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnNormalFloatFloat的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnNormalFloatFloatGetWorkspaceSize(float mean, float std, int64_t seed, int64_t offset,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnNormal的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNormalTensorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

ACLNN_API aclnnStatus aclnnNormalTensorFloat(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

ACLNN_API aclnnStatus aclnnNormalFloatTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

ACLNN_API aclnnStatus aclnnNormalFloatFloat(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NORMAL_OUT_H_
// End content from: aclnn_normal_out.h

// Begin content from: aclnn_bernoulli.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BERNOULLI_H_
#define OP_API_INC_BERNOULLI_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBernoulli的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(self_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Out)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] prob: host侧的aclScalar，浮点类型，需要满足$ 0≤p≤1 $
 * @param [in] seed: host侧的aclScalar
 * @param [in] offset: host侧的aclScalar
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBernoulliGetWorkspaceSize(const aclTensor* self, const aclScalar* prob, int64_t seed,
                                                     int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnBernoulli的第二段接口，用于执行计算。
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(input_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Out)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBernoulli(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnBernoulliTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(self_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Out)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] prob: npu
 * device侧的aclTensor，数据类型支持浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] seed: host侧的aclScalar
 * @param [in] offset: host侧的aclScalar
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBernoulliTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* prob, int64_t seed,
                                                           int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnBernoulliTensor的第二段接口，用于执行计算。
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(input_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Out)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBernoulliTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

/**
 * @brief aclnnInplaceBernoulli的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(selfRef_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Self)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] prob: host侧的aclScalar，浮点类型，需要满足$ 0≤p≤1 $
 * @param [in] seed: host侧的aclScalar
 * @param [in] offset: host侧的aclScalar
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBernoulliGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* prob, int64_t seed,
                                                            int64_t offset, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBernoulli的第二段接口，用于执行计算。
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(input_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Self)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBernoulli(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceBernoulliTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(selfRef_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Self)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] prob: npu
 * device侧的aclTensor，数据类型支持浮点类型，支持非连续的Tensor，数据格式支持ND
 * @param [in] seed: host侧的aclScalar
 * @param [in] offset: host侧的aclScalar
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBernoulliTensorGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* prob,
                                                                  int64_t seed, int64_t offset, uint64_t* workspaceSize,
                                                                  aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBernoulliTensor的第二段接口，用于执行计算。
 *
 * 算子功能：从伯努利分布中提取二进制随机数
 * 计算公式：
 * $$ out_i∼Bernoulli(input_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]  --> B([l0::Contiguous]) -->D([l0op::StatelessBernoulli]) --> I([l0op::ViewCopy]) --> J[(Self)]
 * K((p)) --> K0([ConvertToTensor]) --> D
 * E((seed)) --> D
 * F((offset)) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBernoulliTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BERNOULLI_H_
// End content from: aclnn_bernoulli.h

// Begin content from: aclnn_multinomial.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_MULTINOMIAL_H_
#define OP_API_INC_LEVEL2_ACLNN_MULTINOMIAL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMultinomial的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnMultinomialGetWorkspaceSize(const aclTensor* self, int64_t numsamples, bool replacement,
                                                       int64_t seed, int64_t offset, aclTensor* out,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMultinomial的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMultinomial(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MULTINOMIAL_H_
// End content from: aclnn_multinomial.h

// Begin content from: aclnn_dropout_do_mask.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DROPOUT_DO_MASK_H_
#define OP_API_INC_DROPOUT_DO_MASK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDropoutDoMask的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnDropoutDoMaskGetWorkspaceSize(const aclTensor* self, const aclTensor* mask, double prob,
                                                         aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnDropoutDoMask(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DROPOUT_DO_MASK_H_
// End content from: aclnn_dropout_do_mask.h

// Begin content from: aclnn_random.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_RANDOM_H_
#define OP_API_INC_LEVEL2_ACLNN_RANDOM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceRandom的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、INT32、INT64、BFLOAT16、FLOAT16、
 * INT16、INT8、UINT8、BOOL、DOUBLE。数据格式支持ND。支持非连续的Tensor。
 * @param [in] from: host侧的浮点型，进行离散均匀分布取值的左边界，输入为DOUBLE数据类型。
 * @param [in] to: host侧的浮点型，进行离散均匀分布取值的右边界，输入为DOUBLE数据类型。
 * @param [in] seed: 随机数生成器的种子,它影响生成的随机数序列，输入为UINT64_T数据类型。
 * @param [in] offset: 随机数生成器的偏移量,它影响生成的随机数序列的位置。设置偏移量后，
 * 生成的随机数序列会从指定位置开始。输入为UINT64_T数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRandomGetWorkspaceSize(const aclTensor* selfRef, int64_t from, int64_t to,
                                                         int64_t seed, int64_t offset, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnInplaceRandom的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceRandom获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnInplaceRandom(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_RANDOM_H_
// End content from: aclnn_random.h

// Begin content from: aclnn_uniform.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNIFORM_H_
#define OP_API_INC_UNIFORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceUniform的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 *
 * 算子功能：生成[from, to)区间内离散均匀分布的随机数，并将其填充到self张量中
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -.->B([l0op::Contiguous])
 *  B --> D([l0op::StatelessRandomUniformV2])
 *  E[(seed)] -->D
 *  X[(offset)] -->D
 *  P[(alg)] -->D
 *  D --> F([l0op::Muls])
 *  G[(to)] --> F
 *  D --> H([l0op::Muls])
 *  I[(from)] --> H
 *  H --> J([l0op::Sub])
 *  K[(from)] --> J
 *  F --> L([l0op::Sub])
 *  J --> L
 *  L --> Q([l0op::Cast])
 *  Q -.-> N([l0op::ViewCopy])
 *  N --> O[(Out)]
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT、INT32、INT64、FLOAT16、INT16、INT8、UINT8、BOOL、DOUBLE。数据格式支持ND。支持非连续的Tensor。
 * @param [in] from: host侧的浮点型，进行离散均匀分布取值的左边界，输入为DOUBLE数据类型。
 * @param [in] to: host侧的浮点型，进行离散均匀分布取值的右边界，输入为DOUBLE数据类型。
 * @param [in] seed: 随机数生成器的种子,它影响生成的随机数序列，输入为UINT64_T数据类型。
 * @param [in] offset:
 * 随机数生成器的偏移量,它影响生成的随机数序列的位置。设置偏移量后，生成的随机数序列会从指定位置开始。输入为UINT64_T数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceUniformGetWorkspaceSize(const aclTensor* selfRef, double from, double to,
                                                          uint64_t seed, uint64_t offset, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnInplaceUniform的第二段接口，用于执行计算。
 *
 * 算子功能：生成[from, to)区间内离散均匀分布的随机数，并将其填充到self张量中
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -.->B([l0op::Contiguous])
 *  B --> D([l0op::StatelessRandomUniformV2])
 *  E[(seed)] -->D
 *  X[(offset)] -->D
 *  P[(alg)] -->D
 *  D --> F([l0op::Muls])
 *  G[(to)] --> F
 *  D --> H([l0op::Muls])
 *  I[(from)] --> H
 *  H --> J([l0op::Sub])
 *  K[(from)] --> J
 *  F --> L([l0op::Sub])
 *  J --> L
 *  L --> Q([l0op::Cast])
 *  Q -.-> N([l0op::ViewCopy])
 *  N --> O[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnUniformGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceUniform(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                          const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNIFORM_H_
// End content from: aclnn_uniform.h

// Begin content from: aclnn_dropout_gen_mask.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DROPOUT_GEN_MASK_H_
#define OP_API_INC_DROPOUT_GEN_MASK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDropoutGenMask的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnDropoutGenMaskGetWorkspaceSize(const aclIntArray* shape, double prob, int64_t seed,
                                                          int64_t offset, aclTensor* out, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnDropoutGenMask(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

/**
 * @brief aclnnDropoutGenMaskV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnDropoutGenMaskV2GetWorkspaceSize(const aclIntArray* shape, double prob, int64_t seed,
                                                            int64_t offset, aclDataType probDataType, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnDropoutGenMaskV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DROPOUT_GEN_MASK_H_
// End content from: aclnn_dropout_gen_mask.h

// Begin content from: aclnn_randperm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_RANDPERM_H_
#define OP_API_INC_LEVEL2_ACLNN_RANDPERM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRandperm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 */
ACLNN_API aclnnStatus aclnnRandpermGetWorkspaceSize(int64_t n, int64_t seed, int64_t offset, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRandperm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnRandperm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_RANDPERM_H_
// End content from: aclnn_randperm.h

// Begin content from: aclnn_normal.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NORMAL_H_
#define OP_API_INC_NORMAL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnInplaceNormal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_rand
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点、复数数据类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点、复数数据类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW，且数据格式需要与self一致。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNormalGetWorkspaceSize(const aclTensor* selfRef, float mean, float std, int64_t seed,
                                                         int64_t offset, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnInplaceNormal的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNormal(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NORMAL_H_
// End content from: aclnn_normal.h

// Begin content from: acl/ops/acl_cblas.h
/**
* @file acl_cblas.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/
#ifndef INC_EXTERNAL_ACL_OPS_ACL_CBLAS_H_
#define INC_EXTERNAL_ACL_OPS_ACL_CBLAS_H_

// #include "acl/acl.h"

#ifdef __cplusplus
extern "C" {
#endif

typedef enum aclTransType {
    ACL_TRANS_N,
    ACL_TRANS_T,
    ACL_TRANS_NZ,
    ACL_TRANS_NZ_T
} aclTransType;

typedef enum aclComputeType {
    ACL_COMPUTE_HIGH_PRECISION,
    ACL_COMPUTE_LOW_PRECISION
} aclComputeType;

/**
 * @ingroup AscendCL
 * @brief perform the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param alpha [IN]       pointer to scalar used for multiplication.
 *                         of same type as dataTypeC
 * @param a [IN]           pointer to matrix A
 * @param lda [IN]         leading dimension used to store the matrix A
 * @param dataTypeA [IN]   datatype of matrix A
 * @param x [IN]           pointer to vector x
 * @param incx [IN]        stride between consecutive elements of vector x
 * @param dataTypeX [IN]   datatype of vector x
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         of same type as dataTypeC If beta == 0,
 *                         then y does not have to be a valid input
 * @param y [IN|OUT]       pointer to vector y
 * @param incy [IN]        stride between consecutive elements of vector y
 * @param dataTypeY [IN]   datatype of vector y
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclblasGemvEx(aclTransType transA, int m, int n,
    const void *alpha, const void *a, int lda, aclDataType dataTypeA,
    const void *x, int incx, aclDataType dataTypeX,
    const void *beta, void *y, int incy, aclDataType dataTypeY,
    aclComputeType type, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param dataTypeA [IN]   datatype of matrix A
 * @param dataTypeX [IN]   datatype of vector x
 * @param dataTypeY [IN]   datatype of vector y
 * @param type [IN]        computation type
 * @param handle [OUT]     pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
*/
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForGemvEx(aclTransType transA,
                                                          int m,
                                                          int n,
                                                          aclDataType dataTypeA,
                                                          aclDataType dataTypeX,
                                                          aclDataType dataTypeY,
                                                          aclComputeType type,
                                                          aclopHandle **handle);

/**
 * @ingroup AscendCL
 * @brief perform the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param alpha [IN]       pointer to scalar used for multiplication
 * @param a [IN]           pointer to matrix A
 * @param lda [IN]         leading dimension used to store the matrix A
 * @param x [IN]           pointer to vector x
 * @param incx [IN]        stride between consecutive elements of vector x
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         If beta value == 0,
 *                         then y does not have to be a valid input
 * @param y [IN|OUT]       pointer to vector y
 * @param incy [IN]        stride between consecutive elements of vector y
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasHgemv(aclTransType transA,
                                          int m,
                                          int n,
                                          const aclFloat16 *alpha,
                                          const aclFloat16 *a,
                                          int lda,
                                          const aclFloat16 *x,
                                          int incx,
                                          const aclFloat16 *beta,
                                          aclFloat16 *y,
                                          int incy,
                                          aclComputeType type,
                                          aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param type [IN]        computation type
 * @param handle [OUT]     pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForHgemv(aclTransType transA,
                                                         int m,
                                                         int n,
                                                         aclComputeType type,
                                                         aclopHandle **handle);

/**
 * @ingroup AscendCL
 * @brief perform the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param alpha [IN]       pointer to scalar used for multiplication
 * @param a [IN]           pointer to matrix A
 * @param lda [IN]         leading dimension used to store the matrix A
 * @param x [IN]           pointer to vector x
 * @param incx [IN]        stride between consecutive elements of vector x
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         If beta value == 0,
 *                         then y does not have to be a valid input
 * @param y [IN|OUT]       pointer to vector y
 * @param incy [IN]        stride between consecutive elements of vector y
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasS8gemv(aclTransType transA,
                                           int m,
                                           int n,
                                           const int32_t *alpha,
                                           const int8_t *a,
                                           int lda,
                                           const int8_t *x,
                                           int incx,
                                           const int32_t *beta,
                                           int32_t *y,
                                           int incy,
                                           aclComputeType type,
                                           aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-vector multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param m [IN]           number of rows of matrix A
 * @param n [IN]           number of columns of matrix A
 * @param handle [OUT]     pointer to the pointer to the handle
 * @param type [IN]        computation type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForS8gemv(aclTransType transA,
                                                          int m,
                                                          int n,
                                                          aclComputeType type,
                                                          aclopHandle **handle);

/**
 * @ingroup AscendCL
 * @brief perform the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param alpha [IN]       pointer to scalar used for multiplication. of same type as dataTypeC
 * @param matrixA [IN]     pointer to matrix A
 * @param lda [IN]         leading dimension array used to store  matrix A
 * @param dataTypeA [IN]   datatype of matrix A
 * @param matrixB [IN]     pointer to matrix B
 * @param ldb [IN]         leading dimension array used to store  matrix B
 * @param dataTypeB [IN]   datatype of matrix B
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         of same type as dataTypeC If beta == 0,
 *                         then matrixC does not have to be a valid input
 * @param matrixC [IN|OUT] pointer to matrix C
 * @param ldc [IN]         leading dimension array used to store  matrix C
 * @param dataTypeC [IN]   datatype of matrix C
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasGemmEx(aclTransType transA,
                                           aclTransType transB,
                                           aclTransType transC,
                                           int m,
                                           int n,
                                           int k,
                                           const void *alpha,
                                           const void *matrixA,
                                           int lda,
                                           aclDataType dataTypeA,
                                           const void *matrixB,
                                           int ldb,
                                           aclDataType dataTypeB,
                                           const void *beta,
                                           void *matrixC,
                                           int ldc,
                                           aclDataType dataTypeC,
                                           aclComputeType type,
                                           aclrtStream stream);


/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param dataTypeA [IN]   datatype of matrix A
 * @param dataTypeB [IN]   datatype of matrix B
 * @param dataTypeC [IN]   datatype of matrix C
 * @param type [IN]        computation type
 * @param handle [OUT]     pointer to the pointer to the handle
 * @param type [IN]        computation type
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForGemmEx(aclTransType transA,
                                                          aclTransType transB,
                                                          aclTransType transC,
                                                          int m,
                                                          int n,
                                                          int k,
                                                          aclDataType dataTypeA,
                                                          aclDataType dataTypeB,
                                                          aclDataType dataTypeC,
                                                          aclComputeType type,
                                                          aclopHandle **handle);


/**
 * @ingroup AscendCL
 * @brief perform the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param alpha [IN]       pointer to scalar used for multiplication
 * @param matrixA [IN]     pointer to matrix A
 * @param lda [IN]         leading dimension used to store the matrix A
 * @param matrixB [IN]     pointer to matrix B
 * @param ldb [IN]         leading dimension used to store the matrix B
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         If beta value == 0,
 *                         then matrixC does not have to be a valid input
 * @param matrixC [IN|OUT] pointer to matrix C
 * @param ldc [IN]         leading dimension used to store the matrix C
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasHgemm(aclTransType transA,
                                          aclTransType transB,
                                          aclTransType transC,
                                          int m,
                                          int n,
                                          int k,
                                          const aclFloat16 *alpha,
                                          const aclFloat16 *matrixA,
                                          int lda,
                                          const aclFloat16 *matrixB,
                                          int ldb,
                                          const aclFloat16 *beta,
                                          aclFloat16 *matrixC,
                                          int ldc,
                                          aclComputeType type,
                                          aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param type [IN]        computation type
 * @param handle [OUT]     pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForHgemm(aclTransType transA,
                                                         aclTransType transB,
                                                         aclTransType transC,
                                                         int m,
                                                         int n,
                                                         int k,
                                                         aclComputeType type,
                                                         aclopHandle **handle);

/**
 * @ingroup AscendCL
 * @brief perform the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param alpha [IN]       pointer to scalar used for multiplication
 * @param matrixA [IN]     pointer to matrix A
 * @param lda [IN]         leading dimension used to store the matrix A
 * @param matrixB [IN]     pointer to matrix B
 * @param ldb [IN]         leading dimension used to store the matrix B
 * @param beta [IN]        pointer to scalar used for multiplication.
 *                         If beta value == 0,
 *                         then matrixC does not have to be a valid input
 * @param matrixC [IN|OUT] pointer to matrix C
 * @param ldc [IN]         leading dimension used to store the matrix C
 * @param type [IN]        computation type
 * @param stream [IN]      stream
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasS8gemm(aclTransType transA,
                                           aclTransType transB,
                                           aclTransType transC,
                                           int m,
                                           int n,
                                           int k,
                                           const int32_t *alpha,
                                           const int8_t *matrixA,
                                           int lda,
                                           const int8_t *matrixB,
                                           int ldb,
                                           const int32_t *beta,
                                           int32_t *matrixC,
                                           int ldc,
                                           aclComputeType type,
                                           aclrtStream stream);


/**
 * @ingroup AscendCL
 * @brief create a handle for performing the matrix-matrix multiplication
 *
 * @param transA [IN]      transpose type of matrix A
 * @param transB [IN]      transpose type of matrix B
 * @param transC [IN]      transpose type of matrix C
 * @param m [IN]           number of rows of matrix A and matrix C
 * @param n [IN]           number of columns of matrix B and matrix C
 * @param k [IN]           number of columns of matrix A and rows of matrix B
 * @param type [IN]        computation type
 * @param handle [OUT]     pointer to the pointer to the handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclblasCreateHandleForS8gemm(aclTransType transA,
                                                          aclTransType transB,
                                                          aclTransType transC,
                                                          int m,
                                                          int n,
                                                          int k,
                                                          aclComputeType type,
                                                          aclopHandle **handle);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_OPS_ACL_CBLAS_H_
// End content from: acl/ops/acl_cblas.h

// Begin content from: acl/acl_op_compiler.h
/**
* @file acl_op_compiler.h
*
* Copyright (c) Huawei Technologies Co., Ltd. 2019-2020. All rights reserved.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
*/
#ifndef INC_EXTERNAL_ACL_ACL_OP_COMPILER_H_
#define INC_EXTERNAL_ACL_ACL_OP_COMPILER_H_

// #include "acl_base.h"
// #include "acl_op.h"

#ifdef __cplusplus
extern "C" {
#endif

typedef enum aclCompileType {
    ACL_COMPILE_SYS,
    ACL_COMPILE_UNREGISTERED
} aclopCompileType;

typedef enum {
    ACL_PRECISION_MODE,
    ACL_AICORE_NUM,
    ACL_AUTO_TUNE_MODE, // The auto_tune_mode has been discarded
    ACL_OP_SELECT_IMPL_MODE,
    ACL_OPTYPELIST_FOR_IMPLMODE,
    ACL_OP_DEBUG_LEVEL,
    ACL_DEBUG_DIR,
    ACL_OP_COMPILER_CACHE_MODE,
    ACL_OP_COMPILER_CACHE_DIR,
    ACL_OP_PERFORMANCE_MODE,
    ACL_OP_JIT_COMPILE,
    ACL_OP_DETERMINISTIC,
    ACL_CUSTOMIZE_DTYPES,
    ACL_OP_PRECISION_MODE,
    ACL_ALLOW_HF32,
    ACL_PRECISION_MODE_V2,
    ACL_OP_DEBUG_OPTION
} aclCompileOpt;

typedef enum aclCompileFlag {
    ACL_OP_COMPILE_DEFAULT,
    ACL_OP_COMPILE_FUZZ
} aclOpCompileFlag;

typedef struct aclGraphDumpOption aclGraphDumpOption;

/**
 * @ingroup AscendCL
 * @brief compile op
 *
 * @param opType [IN]           op type
 * @param numInputs [IN]        number of inputs
 * @param inputDesc [IN]        pointer to array of input tensor descriptions
 * @param numOutputs [IN]       number of outputs
 * @param outputDesc [IN]       pointer to array of output tensor descriptions
 * @param attr [IN]           pointer to instance of aclopAttr.
 *                              may pass nullptr if the op has no attribute
 * @param engineType [IN]       engine type
 * @param compileFlag [IN]      compile flag
 * @param opPath [IN]           path of op
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCompile(const char *opType,
                                          int numInputs,
                                          const aclTensorDesc *const inputDesc[],
                                          int numOutputs,
                                          const aclTensorDesc *const outputDesc[],
                                          const aclopAttr *attr,
                                          aclopEngineType engineType,
                                          aclopCompileType compileFlag,
                                          const char *opPath);

/**
 * @ingroup AscendCL
 * @brief compile and execute op
 *
 * @param opType [IN]           op type
 * @param numInputs [IN]        number of inputs
 * @param inputDesc [IN]        pointer to array of input tensor descriptions
 * @param inputs [IN]           pointer to array of input buffers
 * @param numOutputs [IN]       number of outputs
 * @param outputDesc [IN]       pointer to array of output tensor descriptions
 * @param outputs [IN]          pointer to array of outputs buffers
 * @param attr [IN]             pointer to instance of aclopAttr.
 *                              may pass nullptr if the op has no attribute
 * @param engineType [IN]       engine type
 * @param compileFlag [IN]      compile flag
 * @param opPath [IN]           path of op
 * @param stream [IN]           stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCompileAndExecute(const char *opType,
    int numInputs, const aclTensorDesc *const inputDesc[], const aclDataBuffer *const inputs[],
    int numOutputs, const aclTensorDesc *const outputDesc[], aclDataBuffer *const outputs[],
    const aclopAttr *attr, aclopEngineType engineType, aclopCompileType compileFlag,
    const char *opPath, aclrtStream stream);


/**
 * @ingroup AscendCL
 * @brief compile and execute op
 *
 * @param opType [IN]           op type
 * @param numInputs [IN]        number of inputs
 * @param inputDesc [IN]        pointer to array of input tensor descriptions
 * @param inputs [IN]           pointer to array of input buffers
 * @param numOutputs [IN]       number of outputs
 * @param outputDesc [IN|OUT]   pointer to array of output tensor descriptions
 * @param outputs [IN]          pointer to array of outputs buffers
 * @param attr [IN]             pointer to instance of aclopAttr.
 *                              may pass nullptr if the op has no attribute
 * @param engineType [IN]       engine type
 * @param compileFlag [IN]      compile flag
 * @param opPath [IN]           path of op
 * @param stream [IN]           stream handle
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopCompileAndExecuteV2(const char *opType,
    int numInputs, aclTensorDesc *inputDesc[], aclDataBuffer *inputs[],
    int numOutputs, aclTensorDesc *outputDesc[], aclDataBuffer *outputs[],
    aclopAttr *attr, aclopEngineType engineType, aclopCompileType compileFlag,
    const char *opPath, aclrtStream stream);

/**
 * @ingroup AscendCL
 * @brief set compile option
 *
 * @param aclCompileOpt [IN]      compile option
 * @param value [IN]              pointer for the option value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclSetCompileopt(aclCompileOpt opt, const char *value);

/**
 * @ingroup AscendCL
 * @brief get compile option value size
 *
 * @param aclCompileOpt [IN]      compile option
 *
 * @retval size of compile option value
 */
ACL_FUNC_VISIBILITY size_t aclGetCompileoptSize(aclCompileOpt opt);

/**
 * @ingroup AscendCL
 * @brief get compile option
 *
 * @param aclCompileOpt [IN]      compile option
 * @param value [OUT]             pointer for the option value
 * @param length [IN]             length of value
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclGetCompileopt(aclCompileOpt opt, char *value, size_t length);

/**
 * @ingroup AscendCL
 * @brief set compile flag
 *
 * @param flag [IN]    compile flag, ACL_OP_COMPILE_DEFAULT means compile with default mode
 *                     ACL_OP_COMPILE_FUZZ means compile with fuzz mode
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclopSetCompileFlag(aclOpCompileFlag flag);

/**
 * @ingroup AscendCL
 * @brief generate graph and dump
 *
 * @param opType [IN]           op type
 * @param numInputs [IN]        number of inputs
 * @param inputDesc [IN]        pointer to array of input tensor descriptions
 * @param inputs [IN]           pointer to array of input buffers
 * @param numOutputs [IN]       number of outputs
 * @param outputDesc [IN]       pointer to array of output tensor descriptions
 * @param outputs [IN]          pointer to array of outputs buffers
 * @param attr [IN]             pointer to instance of aclopAttr.
 *                              may pass nullptr if the op has no attribute
 * @param engineType [IN]       engine type
 * @param graphDumpPath [IN]    dump path, if the suffix is ".txt", it means file path, else it means directory path
 * @param graphDumpOpt [IN]     dump option, nullptr is supported
 *
 * @retval ACL_SUCCESS The function is successfully executed.
 * @retval OtherValues Failure
 */
ACL_FUNC_VISIBILITY aclError aclGenGraphAndDumpForOp(const char *opType,
    int numInputs, const aclTensorDesc *const inputDesc[], const aclDataBuffer *const inputs[],
    int numOutputs, const aclTensorDesc *const outputDesc[], aclDataBuffer *const outputs[],
    const aclopAttr *attr, aclopEngineType engineType,
    const char *graphDumpPath, const aclGraphDumpOption *graphDumpOpt);

/**
 * @ingroup AscendCL
 * @brief Create the graph dump option
 *
 * @retval null for failed
 * @retval OtherValues success
 *
 * @see aclDestroyGraphDumpOpt
 */
ACL_FUNC_VISIBILITY aclGraphDumpOption *aclCreateGraphDumpOpt();

/**
 * @ingroup AscendCL
 * @brief Destroy graph dump option
 *
 * @param graphDumpOpt [IN]  pointer to the graph dump option
 *
 * @retval ACL_SUCCESS  The function is successfully executed.
 * @retval OtherValues Failure
 *
 * @see aclCreateGraphDumpOpt
 */
ACL_FUNC_VISIBILITY aclError aclDestroyGraphDumpOpt(const aclGraphDumpOption *graphDumpOpt);

#ifdef __cplusplus
}
#endif

#endif // INC_EXTERNAL_ACL_ACL_OP_COMPILER_H_
// End content from: acl/acl_op_compiler.h

// Begin content from: aclnn_incre_flash_attention_v2.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_INCRE_FLASH_ATTENTION_V2_H_
#define ACLNN_INCRE_FLASH_ATTENTION_V2_H_

// #include "aclnn/aclnn_base.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIncreFlashAttentionV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * funtion: aclnnIncreFlashAttentionV2GetWorkspaceSize
 * @param [in] query : required
 * @param [in] key : dynamic
 * @param [in] value : dynamic
 * @param [in] pseShift : optional
 * @param [in] attenMask : optional
 * @param [in] actualSeqLengths : optional
 * @param [in] dequantScale1 : optional
 * @param [in] quantScale1 : optional
 * @param [in] dequantScale2 : optional
 * @param [in] quantScale2 : optional
 * @param [in] quantOffset2 : optional
 * @param [in] numHeads : required
 * @param [in] scaleValue : optional
 * @param [in] inputLayout : optional
 * @param [in] numKeyValueHeads : optional
 * @param [out] attentionOut : required
 * @param [out] workspaceSize : size of workspace(output).
 * @param [out] executor : executor context(output).
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV2GetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
    const aclTensor *attenMask, const aclIntArray *actualSeqLengths, const aclTensor *dequantScale1,
    const aclTensor *quantScale1, const aclTensor *dequantScale2, const aclTensor *quantScale2,
    const aclTensor *quantOffset2, int64_t numHeads, double scaleValue, char *inputLayout, int64_t numKeyValueHeads,
    const aclTensor *attentionOut, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnIncreFlashAttentionV2的第二段接口，，用于执行计算。
 * funtion: aclnnIncreFlashAttentionV2
 * @param [in] workspace : workspace memory addr(input).
 * @param [in] workspaceSize : size of workspace(input).
 * @param [in] executor : executor context(input).
 * @param [in] stream : acl stream.
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV2(void *workspace, uint64_t workspaceSize,
                                                                              aclOpExecutor *executor,
                                                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_incre_flash_attention_v2.h

// Begin content from: aclnn_batch_norm_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BATCH_NORM_BACKWARD_H_
#define OP_API_INC_BATCH_NORM_BACKWARD_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormBackwardBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* input,
                                                             const aclTensor* weight, const aclTensor* runningMean,
                                                             const aclTensor* runningVar, const aclTensor* saveMean,
                                                             const aclTensor* saveInvstd, bool training, double eps,
                                                             const aclBoolArray* outputMask, aclTensor* gradInput,
                                                             aclTensor* gradWeight, aclTensor* gradBias,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BATCH_NORM_BACKWARD_H_
// End content from: aclnn_batch_norm_backward.h

// Begin content from: aclnn_foreach_pow_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_POW_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_POW_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachPowScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素进行scalar次方运算后输出。
 * 计算公式：
 * out_{i}=x_{i}^scalar
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT16。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachPowScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachPowScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素进行scalar次方运算后输出。
 * 计算公式：
 * out_{i}=x_{i}^scalar
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachPowScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachPowScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_pow_scalar_v2.h

// Begin content from: aclnn_aminmax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_AMINMAX_H_
#define OP_API_INC_AMINMAX_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAminmax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] dim: host侧的aclIntArray，指定要缩减的维度。
 * @param [in] keepDim: host侧的bool，reduce轴的维度是否保留。
 * @param [in] minOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] maxOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminmaxGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                   aclTensor* minOut, aclTensor* maxOut, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnAminmax的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAminmaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminmax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AMINMAX_H_
// End content from: aclnn_aminmax.h

// Begin content from: aclnn_dynamic_quant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef ACLNN_DYNAMIC_QUANT_H_
#define ACLNN_DYNAMIC_QUANT_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDynamicQuantGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
__attribute__((visibility("default"))) aclnnStatus aclnnDynamicQuantGetWorkspaceSize(
    const aclTensor* x, const aclTensor* smoothScalesOptional, const aclTensor* yOut, const aclTensor* scaleOut,
    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDynamicQuant的第二段接口，用于执行计算。
 */
__attribute__((visibility("default"))) aclnnStatus aclnnDynamicQuant(void* workspace, uint64_t workspaceSize,
                                                                       aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // ACLNN_DYNAMIC_QUANT_H_// End content from: aclnn_dynamic_quant.h

// Begin content from: aclnn_max_pool3d_with_argmax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_WITH_ARGMAX_H_
#define OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_WITH_ARGMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxPool3dWithArgmax First segment interface. Calculate the workspace size based on the specific calculation process.
 * Function description: Calculates the aclnnMaxPool3dWithArgmax from the input tensor, at the output contains 2 tensors: out and indices
 * @domain aclnn_ops_infer
 * @param [in] self: aclTensor on the NPU device. the data type can be float32/float16/bfloat16. The data format supports ND.
 * @param [in] kernelSize: aclIntArray type, indicating the maxpooling window size.
 * @param [in] stride: aclIntArray type, the step size of the window movement.
 * @param [in] padding: aclIntArray type, number of padding layers for each edge. The value is negative infinity.
 * @param [in] dilation: aclIntArray type, controls the stride of elements in the window.
 * @param [in] ceilMode: bool type, when true, the output shape is calculated using round-up method. By default is rounding down.
 * @param [in] out: aclTensor on the NPU device. the data type can be float32/float16/bfloat16. The data format supports ND.
 * @param [in] indices: aclTensor on the NPU device. the data type can be int32. The data format supports ND.
 * @param [out] workspaceSize: Returns the workspace size that the user needs to apply for on the npu device side.
 * @param [out] executor: Return the op executor, including the operator calculation process.
 * @return aclnnStatus: Return the status code.
 */

ACLNN_API aclnnStatus aclnnMaxPool3dWithArgmaxGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                               const aclIntArray* stride, const aclIntArray* padding,
                                                               const aclIntArray* dilation, bool ceilMode, aclTensor* out,
                                                               aclTensor* indices, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);
/**
 * @brief A second interface of aclnnMaxPool3dWithArgmax, used to perform calculation.
 * @param [in] workspace: start address of the workspace memory allocated on the NPU device.
 * @param [in] workspaceSize: size of the workspace applied on the NPU device, which is obtained by calling the first segment interface aclnnMaxPool3dWithArgmaxGetWorkspaceSize.
 * @param [in] exector: op executor, including the operator calculation process.
 * @param [in] stream: acl stream.
 * @return aclnnStatus: returned status code
 */

ACLNN_API aclnnStatus aclnnMaxPool3dWithArgmax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_WITH_ARGMAX_H_// End content from: aclnn_max_pool3d_with_argmax.h

// Begin content from: aclnn_sign.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SIGN_H_
#define OP_API_INC_LEVEL2_ACLNN_SIGN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSign的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnSignGetWorkspaceSize(const aclTensor* self, aclTensor* result, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnSign的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSign(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_sign.h

// Begin content from: aclnn_moe_init_routing_quant_v2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_INIT_ROUTING_QUANT_V2_H_
#define ACLNN_MOE_INIT_ROUTING_QUANT_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeInitRoutingQuantV2GetWorkspaceSize
 * parameters :
 * x : required
 * expertIdx : required
 * scaleOptional : optional
 * offsetOptional : optional
 * activeNum : optional
 * expertCapacity : optional
 * expertNum : optional
 * dropPadMode : optional
 * expertTokensCountOrCumsumFlag : optional
 * expertTokensBeforeCapacityFlag : optional
 * quantMode : optional
 * expandedXOut : required
 * expandedRowIdxOut : required
 * expertTokensCountOrCumsumOutOptional : optional
 * expertTokensBeforeCapacityOutOptional : optional
 * dynamicQuantScaleOutOptional : optional
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingQuantV2GetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *expertIdx,
    const aclTensor *scaleOptional,
    const aclTensor *offsetOptional,
    int64_t activeNum,
    int64_t expertCapacity,
    int64_t expertNum,
    int64_t dropPadMode,
    int64_t expertTokensCountOrCumsumFlag,
    bool expertTokensBeforeCapacityFlag,
    int64_t quantMode,
    const aclTensor *expandedXOut,
    const aclTensor *expandedRowIdxOut,
    const aclTensor *expertTokensCountOrCumsumOutOptional,
    const aclTensor *expertTokensBeforeCapacityOutOptional,
    const aclTensor *dynamicQuantScaleOutOptional,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeInitRoutingQuantV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingQuantV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_init_routing_quant_v2.h

// Begin content from: aclnn_logsigmoid_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOGSIGMOID_BACKWARD_H_
#define OP_API_INC_LOGSIGMOID_BACKWARD_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogSigmoidBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnLogSigmoidBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                              const aclTensor* buffer, aclTensor* gradInput,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogSigmoidBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLogSigmoidBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOGSIGMOID_BACKWARD_H_
// End content from: aclnn_logsigmoid_backward.h

// Begin content from: aclnn_matmul_reduce_scatter.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MATMUL_REDUCE_SCATTER_
#define OP_API_INC_MATMUL_REDUCE_SCATTER_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现mm + reduceScatter融合计算
 * @brief aclnnMatmulReduceScatter的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bf16。
 * @param [in] x2: matmul右矩阵，数据类型支持：float16, bf16。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：0/1。
 * @param [out] output: 计算+通信的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulReduceScatterGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                               const aclTensor* bias, const char* group,
                                                               const char* reduceOp, int64_t commTurn,
                                                               int64_t streamMode, const aclTensor* output,
                                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMatmulReduceScatter的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAbsGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulReduceScatter(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MATMUL_REDUCE_SCATTER_// End content from: aclnn_matmul_reduce_scatter.h

// Begin content from: aclnn_moe_finalize_routing_v2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_FINALIZE_ROUTING_V2_H_
#define ACLNN_MOE_FINALIZE_ROUTING_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeFinalizeRoutingV2GetWorkspaceSize
 * parameters :
 * expandedX : required
 * expandedRowIdx : required
 * x1Optional : optional
 * x2Optional : optional
 * biasOptional : optional
 * scalesOptional : optional
 * expertIdxOptional : optional
 * dropPadMode : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRoutingV2GetWorkspaceSize(
    const aclTensor *expandedX,
    const aclTensor *expandedRowIdx,
    const aclTensor *x1Optional,
    const aclTensor *x2Optional,
    const aclTensor *biasOptional,
    const aclTensor *scalesOptional,
    const aclTensor *expertIdxOptional,
    int64_t dropPadMode,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeFinalizeRoutingV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRoutingV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_finalize_routing_v2.h

// Begin content from: aclnn_trans_matmul_weight.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRANS_MATMUL_WEIGHT_H_
#define OP_API_INC_TRANS_MATMUL_WEIGHT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief
 * aclnnCalculateMatmulWeightSizeV2用于计算调用aclnnMatMul、aclnnMm、aclnnWeightQuantBatchMatmulV2、
 * aclnnWeightQuantBatchMatmulV3、aclnnQuantMatmulV3传入的weight tensor需要占用的元素大小
 * @domain aclnn_ops_infer
 *
 * @param [in] tensorShape: 用于表达该次Matmul载入矩阵的weight的Shape
 * @param [in] dataType: 输入Weight的Datatype, 支持INT8和Float16
 * @param [out] weightTensorSize: 根据MatMul内部处理逻辑，计算该输入下Weight需要多少个元素的数据量
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCalculateMatmulWeightSizeV2(const aclIntArray* tensorShape, aclDataType dataType,
                                                       uint64_t* weightTensorSize);

/**
 * @brief aclnnCalculateMatmulWeightSize用于计算调用aclnnMatMul或aclnnMm传入的weight tensor需要占用的元素大小
 *
 *
 * @param [in] tensorShape: 用于表达该次Matmul载入矩阵的weight的Shape
 * @param [in] weightTensorSize: 根据MatMul内部处理逻辑，计算该输入下Weight需要多少个元素的数据量
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCalculateMatmulWeightSize(const aclIntArray* tensorShape, uint64_t* weightTensorSize);

/**
 * @brief aclnnTransMatmulWeight的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将输入tensor转换为指定的dtype类型。
 *
 * @param [in] mmWeightRef: 输入是一个待处理的matmul的weightTensor，格式是正常的ND输入，数据类型支持Float16
 * 经过此接口处理后此tensor被刷新为预处理后的matmul weightTensor格式根据亲和性进行ND或者私有格式的转换
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTransMatmulWeightGetWorkspaceSize(aclTensor* mmWeightRef, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);
/**
 * @brief aclnnTransMatmulWeight的第二段接口，用于执行计算。
 *
 * 算子功能：将输入tensor转换为指定的dtype类型。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnTransMatmulWeightGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTransMatmulWeight(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRANS_MATMUL_WEIGHT_H_
// End content from: aclnn_trans_matmul_weight.h

// Begin content from: aclnn_upsample_nearest_2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_2D_H_
#define OP_API_INC_UNAMPLE_NEAREST_2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest2D的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：对由多个输入通道组成的输入信号应用最近邻插值算法进行上采样。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> D([l0op::TransData])
 *  D --> I([l0op::ResizeNearestNeighborV2])
 *  C[(outputSize)] --> I
 *  I --> J([l0op::TransData])
 *  J -.-> P([l0op::ViewCopy])
 *  P --> G[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE、UIN8。数据格式支持NCHW、NHWC。支持非连续的Tensor。
 * @param [in] outputSize: npu device侧的aclIntArray，指定输出空间大小。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE、UIN8。数据格式支持NCHW、NHWC。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest2D的第二段接口，用于执行计算。
 *
 * 算子功能：对由多个输入通道组成的输入信号应用最近邻插值算法进行上采样。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> D([l0op::TransData])
 *  D --> I([l0op::ResizeNearestNeighborV2])
 *  C[(outputSize)] --> I
 *  I --> J([l0op::TransData])
 *  J -.-> P([l0op::ViewCopy])
 *  P --> G[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnUpsampleNearest2dGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_2D_H_
// End content from: aclnn_upsample_nearest_2d.h

// Begin content from: aclnn_complex.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_COMPLEX_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_COMPLEX_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnComplex的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] real: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT数据类型，
 * shape需要与imag满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] imag: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT数据类型，
 * shape需要与real满足broadcast关系，支持非连续的Tensor，数据格式支持ND，数据类型需要与real相同。
 * @param [in] out: npu device侧的aclTensor，数据格式支持ND。
 * @param [in] Tout: host侧的int，表示
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnComplexGetWorkspaceSize(const aclTensor* real, const aclTensor* imag, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnComplex的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnComplexGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnComplex(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_COMPLEX_TENSOR_H_// End content from: aclnn_complex.h

// Begin content from: aclnn_reciprocal.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_RECIPROCAL_H_
#define OP_API_INC_LEVEL2_ACLNN_RECIPROCAL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReciprocal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnReciprocalGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnReciprocal的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnReciprocal(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceReciprocal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceReciprocalGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnInplaceReciprocal的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceReciprocal(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_RECIPROCAL_H_// End content from: aclnn_reciprocal.h

// Begin content from: aclnn_fused_infer_attention_score.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_FUSED_INFER_ATTENTION_SCORE_H_
#define ACLNN_FUSED_INFER_ATTENTION_SCORE_H_
// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief The first interface of aclnnFusedInferAttentionScore calculates the workspace size based on the specific calculation process.
 * @domain aclnn_ops_infer
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFusedInferAttentionScoreGetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
    const aclTensor *attenMask, const aclIntArray *actualSeqLengths, const aclIntArray *actualSeqLengthsKv,
    const aclTensor *deqScale1, const aclTensor *quantScale1, const aclTensor *deqScale2, const aclTensor *quantScale2,
    const aclTensor *quantOffset2, const aclTensor *antiquantScale, const aclTensor *antiquantOffset,
    const aclTensor *blockTable, const aclTensor *queryPaddingSize, const aclTensor *kvPaddingSize, int64_t numHeads,
    double scaleValue, int64_t preTokens, int64_t nextTokens, char *inputLayout, int64_t numKeyValueHeads,
    int64_t sparseMode, int64_t innerPrecise, int64_t blockSize, int64_t antiquantMode, bool softmaxLseFlag,
    const aclTensor *attentionOut, const aclTensor *softmaxLse, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief The second interface of aclnnFusedInferAttentionScore is used to perform calculations.
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFusedInferAttentionScore(void *workspace,
                                                                                 uint64_t workspaceSize,
                                                                                 aclOpExecutor *executor,
                                                                                 const aclrtStream stream);


#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_fused_infer_attention_score.h

// Begin content from: aclnn_var_mean.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_VAR_MEAN_H_
#define OP_API_INC_VAR_MEAN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnVarMean的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnVarMeanGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, int64_t correction,
                                                   bool keepdim, aclTensor* varOut, aclTensor* meanOut,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnVarMean的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnVarMean(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_var_mean.h

// Begin content from: aclnn_amin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AMIN_H_
#define OP_API_INC_AMIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAmin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回张量在指定维度上每个切片的最小值。
 *
 * @param [in] self:
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL。数据格式支持ND。
 * 支持非连续的Tensor。
 * @param [in] dim: host侧aclIntArray，指定了要进行最小值计算的维度， 数据类型支持INT32和INT64。
 * @param [in] keepDim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] out: device侧的aclTensor，数据类型需要与self相同。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAmin的第二段接口，用于执行计算。
 *
 * 算子功能：返回张量在指定维度上的每个切片的最小值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAminGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAmin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AMIN_H_// End content from: aclnn_amin.h

// Begin content from: aclnn_grouped_matmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_MATMUL_H
#define OP_API_INC_GROUPED_MATMUL_H
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupedMatmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 表示公式中的x，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] weight:
 * 表示公式中的weight，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] biasOptional:
 * 表示公式中的bias，数据类型支持FLOAT16、FLOAT32、INT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] scaleOptional: 表示量化参数，数据类型支持UINT64数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] offsetOptional: 表示量化参数，数据类型支持FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantScaleOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantOffsetOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] groupListOptional: 可选参数，代表输入和输出M轴上的索引情况，数据类型支持INT64，支持的最大长度为128个。
 * @param [in] splitItem:
 * 整数型参数，代表输出是否要做tensor切分，0/1代表输出为多tensor；2/3代表输出为单tensor，默认值为0。
 * @param [out] y: 表示公式中的y，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulGetWorkspaceSize(
    const aclTensorList* x, const aclTensorList* weight, const aclTensorList* biasOptional,
    const aclTensorList* scaleOptional, const aclTensorList* offsetOptional,
    const aclTensorList* antiquantScaleOptional, const aclTensorList* antiquantOffsetOptional,
    const aclIntArray* groupListOptional, int64_t splitItem, const aclTensorList* y, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnGroupedMatmul的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_grouped_matmul.h

// Begin content from: aclnn_foreach_addcdiv_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCDIV_LIST_H_
#define ACLNN_FOREACH_ADDCDIV_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcdivListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcdivList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcdiv_list.h

// Begin content from: aclnn_gather_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GATHER_V2_H_
#define OP_API_INC_LEVEL2_ACLNN_GATHER_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：从输入Tensor的指定维度dim，按index中的下标序号提取元素，保存到out Tensor中。
 * 计算公式：
 * 以输入为三维张量为例：
 *   x=$\begin{bmatrix}[[1,&2],&[3,&4]], \\ [[5,&6],&[7,&8]], \\ [[9,&10],&[11,&12]]\end{bmatrix}$
 *   idx=[1, 0],
 * dim为0：   I=index[i];  &nbsp;&nbsp;   y$[i][m][n]$ = x$[I][m][n]$
 * dim为1：   J=index[j];  &nbsp;&nbsp;&nbsp;    y$[l][j][n]$ = x$[l][J][n]$
 * dim为2：   K=index[k]; &nbsp;  y$[l][m][k]$ = x$[l][m][K]$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> In_0([l0op::cast]) --> Op([GatherV2])
 *   In_1[(index)] --> con([l0op::Contiguous])--> Op
 *   In_2(dim) --> a(dimVec) --> Op
 *   Op --> C([l0op::cast]) --> D([l0op::ViewCopy]) --> Out[(out)]
 * ```
 */

/**
 * @brief aclnnGatherV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT64、INT32、INT16、INT8、UINT8、BOOL、DOUBLE、
 * COMPLEX64、COMPLEX128、BFLOAT16，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] dim: host侧的整数，数据类型支持INT64。
 * @param [in] index: npu
 * device侧的aclTensor，数据类型支持INT64、INT32，支持非连续的Tensor，数据格式支持ND，数据维度不支持 8维以上。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT64、INT32、INT16、INT8、UINT8、BOOL、DOUBLE、
 * COMPLEX64、COMPLEX128、BFLOAT16，数据类型需要与self一致，维数等于self维数与index维数之和减一，除dim维扩展为跟index的shape
 * 一样外，其他维长度与self相应维一致，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGatherV2GetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* index,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGatherV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGatherV2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGatherV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GATHER_V2_H_
// End content from: aclnn_gather_v2.h

// Begin content from: aclnn_smooth_l1_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SILU_H_
#define OP_API_INC_LEVEL2_ACLNN_SILU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSmoothL1Loss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnSmoothL1LossGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                        int64_t reduction, float beta, aclTensor* result,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSmoothL1Loss的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSmoothL1Loss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_smooth_l1_loss.h

// Begin content from: aclnn_max_pool2d_with_indices.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_H_
#define OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxPool2dWithMask的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 对于输入信号的输入通道，提供2维池化操作：
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持float16、float32、bfloat16（仅昇腾910B AI处理器、
 * 昇腾910_93 AI处理器支持），数据格式支持NCHW(ND)， 支持非连续的Tensor。
 * @param [in] kernelSize: aclIntArray类型， 表示最大池化的窗口大小。
 * @param [in] stride: aclIntArray类型， 窗口移动的步长。
 * @param [in] padding: aclIntArray类型， 每一条边补充的层数，补充的位置填写负无穷。
 * @param [in] dilation: aclIntArray类型， 控制窗口中元素的步幅。
 * @param [in] ceilMode: aclIntArray类型， 为true时用上取整的方法计算输出形状，默认向下取整。
 * @param [in] out: npu device侧的aclTensor，数据类型支持float16、float32、bfloat16（仅昇腾910B
 * AI处理器、昇腾910_93 AI处理器支持）， 数据格式支持NCHW(ND)， 支持非连续的Tensor。
 * @param [in] indices: npu
 * device侧的aclTensor，最大值在求mask的kernel位置的bit值组成的Tensor，数据类型支持int8，数据格式支持NCHW(ND)，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithMaskGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                             const aclIntArray* stride, const aclIntArray* padding,
                                                             const aclIntArray* dilation, bool ceilMode,
                                                             aclTensor* out, aclTensor* indices,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxPool2dWithMask的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMaxPool2dWithMaskGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithMask(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

/**
 * @brief aclnnMaxPool2dWithIndices的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 对于输入信号的输入通道，提供2维池化操作：
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持float32（仅昇腾910B AI处理器支持、昇腾910_93 AI处理器支持），
 * 数据格式支持NCHW(ND)，支持非连续的Tensor。
 * @param [in] kernelSize: aclIntArray类型， 表示最大池化的窗口大小。
 * @param [in] stride: aclIntArray类型， 窗口移动的步长。
 * @param [in] padding: aclIntArray类型， 每一条边补充的层数，补充的位置填写负无穷。
 * @param [in] dilation: aclIntArray类型， 控制窗口中元素的步幅。
 * @param [in] ceilMode: aclIntArray类型， 为true时用上取整的方法计算输出形状，默认向下取整。
 * @param [in] out: npu device侧的aclTensor，数据类型支持float32（仅昇腾910B AI处理器支持），数据格式支持NCHW(ND)，
 * 支持非连续的Tensor。
 * @param [in] indices: npu device侧的aclTensor，最大值索引位置组成的tensor，数据类型支持int32，数据格式支持NCHW(ND)，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithIndicesGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                                const aclIntArray* stride, const aclIntArray* padding,
                                                                const aclIntArray* dilation, bool ceilMode,
                                                                aclTensor* out, aclTensor* indices,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxPool2dWithIndices的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMaxPool2dWithIndicesGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithIndices(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_H_
// End content from: aclnn_max_pool2d_with_indices.h

// Begin content from: aclnn_l1_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_L1_LOSS_BACKWARD_H_
#define OP_API_INC_L1_LOSS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnL1LossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：平均绝对误差函数的反向传播。
 *
 * @param [in] gradOutput：npu device侧的aclTensor，数据类型需要与self、target满足数据类型推导规则，
 * 且推导后数据类型支持FLOAT、FLOAT16，shape需要与self、target满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] self：npu device侧的aclTensor，数据类型需要与gradOutput、target满足数据类型推导规则，
 * 且推导后数据类型支持FLOAT、FLOAT16，shape需要与gradOutput、target满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] target：npu device侧的aclTensor，数据类型需要与self、gradOutput满足数据类型推导规则，
 * 且推导后数据类型支持FLOAT、FLOAT16。shape需要与gradOutput、self满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction：host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。'none'
 * 表示不应用减少， 'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] gradInput：npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。
 * shape需要是target与self、gradOutput broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnL1LossBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                          const aclTensor* target, int64_t reduction,
                                                          aclTensor* gradInput, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnL1LossBackward的第二段接口，用于执行计算。
 *
 * 算子功能：均方误差函数的反向传播。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnL1LossBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnL1LossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_L1_LOSS_BACKWARD_H_
// End content from: aclnn_l1_loss_backward.h

// Begin content from: aclnn_is_inf.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_IS_INF_H_
#define ACLNN_IS_INF_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnIsInfGetWorkspaceSize
 * parameters :
 * x : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnIsInfGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnIsInf
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnIsInf(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_is_inf.h

// Begin content from: aclnn_glu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GLU_BACKWARD_H_
#define OP_API_INC_GLU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：GLU的反向。
 *
 * $$
 * \frac{\partial GLU(a,b)}{\partial(a,b)}=cat(\sigma(b),\sigma(b) \otimes a \otimes (1-\sigma(b)))
 * $$
 *
 * 数学计算表达式：
 * 假设输出的GLUGrad有两部分组成:out=[a_grad, b_grad]，则：
 * sig_b = sigmoid(b)
 * **a_grad** = y_grad * sig_b
 * **b_grad** = a_grad * (a - a * sig_b)
 * 其中：y_grad 为gradOut，a表示的是输入张量根据指定dim进行均分后的前部分张量，b表示后半部分张量。
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *    A0[(gradOut)] -->B0([l0op::Contiguous])-->C1([l0op::Mul])-->C2([l0op::Mul])
 *    A1[(self)] -->B1([l0op::Contiguous])
 *    B1 -->D0([l0op::SplitV])--a--> C0-->G0([l0op::Sub])
 *    D0--a-->G0-->C2--b_grad-->H0([l0op::ConcatD])
 *    E0((dim)) -->D0--b-->D1([l0op::Sigmoid])-->C0([l0op::Mul])
 *    D1-->C1--a_grad-->H0
 *    E0-->H0
 *    H0 -->F0([l0op::ViewCopy])--> J0[(out)]
 * ```
 *
 * @param [in] gradOut: 表示梯度更新系数，数据类型支持DOUBLE,FLOAT,FLOAT16数据类型，数据类型必须与self的数据类型一致，
 * shape为$(*_1,M,*_2)$其中$*$表示self中对应维度，$M = N /2$，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self:
 * 数据类型支持DOUBLE,FLOAT,FLOAT16数据类型，tensor的维度必须大于0，且shape必须在入参dim对应的维度上可以整除2，
 * shape表示为$(*_1,N,*_2)$其中$*$表示任何数量的附加维，$N$表示dim指定的维度大小，支持非连续的Tensor，数据格式支持ND。
 * @param [in] dim: 表示要拆分输入self的维度，数据类型支持INT64，取值范围[-self.dim，self.dim-1]。
 * @param [out] out: 数据类型支持DOUBLE,FLOAT,FLOAT16数据类型，数据类型必须与self的数据类型一致，
 * shape必须与self的shape一致，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGluBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* self, int64_t dim,
                                                       const aclTensor* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnGluBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GLU_BACKWARD_H_
// End content from: aclnn_glu_backward.h

// Begin content from: aclnn_upsample_linear_1d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_LINEAR_1D_BACKWARD_H_
#define OP_API_INC_UNAMPLE_LINEAR_1D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleLinear1dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleLinear1dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    aclrtStream stream);

/**
 * @brief aclnnUpsampleLinear1dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleLinear1dBackwardGetWorkspaceSize(const aclTensor* gradOut,
                                                                    const aclIntArray* outputSize,
                                                                    const aclIntArray* inputSize, bool alignCorners,
                                                                    double scales, aclTensor* out,
                                                                    uint64_t* workspaceSize, aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_LINEAR_1D_BACKWARD_H_// End content from: aclnn_upsample_linear_1d_backward.h

// Begin content from: aclnn_erfinv.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ERFINV_H_
#define OP_API_INC_LEVEL2_ACLNN_ERFINV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnErfinv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：erfinv是高斯误差函数erf的反函数。返回输入Tensor中每个元素对应在标准正态分布函数的分位数。
 * 计算公式：如下
 * $$
 * y = erfinv(x) \\
 * x = erf(y)=\frac{2}{\sqrt{\pi } } \int_{0}^{y} e^{-t^{2} } \mathrm{d}t
 * $$
 *
 * 计算图：如下
 * 场景：当输入类型在Erfinv算子支持的范围之内（FLOAT32、FLOAT16、BFLOAT16）时，使用Erfinv算子完成计算。
 * ```mermaid
 * graph LR
 * A[(Self)] --> B([l0op::Contiguous]) --> C([l0op::Erfinv])
 * C --> D([l0op::Cast]) --E D([l0op::ViewCopy]) --> F[(out)]
 * ```
 *
 * 整数类型：BOOL、INT8、INT16、INT32、INT64、UINT8，先转为FLOAT，再计算：
 * ```mermaid
 * graph LR
 * A[(Self)] --> B([l0op::Contiguous]) --> C([l0op::Cast]) --> D([l0op::Erfinv])
 * D --> E([l0op::Cast]) --> F([l0op::ViewCopy]) --> G[(out)]
 * ```
 *
 * @param [in] self: 待进行erfinv计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT32、FLOAT16、BFLOAT16、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND， 支持非连续的Tensor。
 * @param [in] out: erfinv计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，shape同self一致，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErfinvGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnErfinv的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnErfinvGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErfinv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                  const aclrtStream stream);

/**
 * @brief aclnnInplaceErfinv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素对应在标准正态分布函数的分位数
 *
 * @param [in] selfRef: 待进行erfinv计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErfinvGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnInplaceinv的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnErfinvGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErfinv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ERFINV_H_// End content from: aclnn_erfinv.h

// Begin content from: aclnn_upsample_nearest_exact3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_EXACT3D_GRAD_H_
#define OP_API_INC_UNAMPLE_NEAREST_EXACT3D_GRAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearestExact3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact3dBackwardGetWorkspaceSize(
    const aclTensor *gradOut, const aclIntArray *outputSize, const aclIntArray *inputSize, double scalesD,
    double scalesH, double scalesW, aclTensor *gradInput, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleNearestExact3dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact3dBackward(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_EXACT3D_GRAD_H_
// End content from: aclnn_upsample_nearest_exact3d_backward.h

// Begin content from: aclnn_hardsigmoid.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDSIGMOID_H_
#define OP_API_INC_HARDSIGMOID_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardsigmoid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数计算
 * $$
 * Hardsigmoid(x)=\begin{cases}
 * 1, & x\gt3 \\
 * 0, &  x\le -3 \\
 * x/6 + 1/2 , & otherwise
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardsigmoid])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、FLOAT16、INT32，支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持 FLOAT、FLOAT16、INT32，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardsigmoidGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);
/**
 * @brief aclnnHardsigmoid的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数计算
 * $$
 * Hardsigmoid(x)=\begin{cases}
 * 1, & x\gt3 \\
 * 0, &  x\le -3 \\
 * x/6 + 1/2 , & otherwise
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardsigmoid])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnHardsigmoidGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardsigmoid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

/**
 * @brief aclnnInplaceHardsigmoid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数计算
 * $$
 * Hardsigmoid(x)=\begin{cases}
 * 1, & x\gt3 \\
 * 0, &  x\le -3 \\
 * x/6 + 1/2 , & otherwise
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardsigmoid])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、FLOAT16、INT32，支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardsigmoidGetWorkspaceSize(const aclTensor* self, uint64_t* workspaceSize,
                                                              aclOpExecutor** executor);

/**
 * @brief aclnnInplaceHardsigmoid的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数计算
 * $$
 * Hardsigmoid(x)=\begin{cases}
 * 1, & x\gt3 \\
 * 0, &  x\le -3 \\
 * x/6 + 1/2 , & otherwise
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardsigmoid])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceHardsigmoidGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardsigmoid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSIGMOID_H
// End content from: aclnn_hardsigmoid.h

// Begin content from: aclnn_quant_matmul_weight_nz.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_QUANT_MATMUL_NZ
#define OP_API_INC_QUANT_MATMUL_NZ

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulWeightNz的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：相对于aclnnQuantBatchMatmulV4, 新增了支持x2为nz格式。
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] x1Scale: 量化参数，数据类型支持：float32。
 * @param [in] x2Scale: 量化参数，数据类型支持：uint64_t, float, bfloat16, int64_t。
 * @param [in] yScale: 预留参数，当前接口不支持该参数。
 * @param [in] x1Offset: 预留参数，当前接口不支持该参数。
 * @param [in] x2Offset: 量化参数，数据类型支持：float32。
 * @param [in] yOffset: 预留参数，当前接口不支持该参数。
 * @param [in] bias: 偏置，数据类型支持：int32_t, bfloat16, float16, float32。
 * @param [in] transposeX1: a矩阵是否转置，默认值：false。
 * @param [in] transposeX2: b矩阵是否转置，默认值：false。
 * @param [in] groupSize: 预留参数，当前接口不支持该参数。
 * @param [out] out: 计算结果，数据类型：half, int8, bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulWeightNzGetWorkspaceSize(const aclTensor *x1, const aclTensor *x2,
                                                               const aclTensor *x1Scale, const aclTensor *x2Scale,
                                                               const aclTensor *yScale, const aclTensor *x1Offset,
                                                               const aclTensor *x2Offset, const aclTensor *yOffset,
                                                               const aclTensor *bias, bool transposeX1,
                                                               bool transposeX2, int64_t groupSize, aclTensor *out,
                                                               uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnQuantMatmulWeightNz的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnQuantMatmulNZGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulWeightNz(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_NZ// End content from: aclnn_quant_matmul_weight_nz.h

// Begin content from: aclnn_foreach_sinh.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SINH_H_
#define ACLNN_FOREACH_SINH_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSinhGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSinhGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSinh
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSinh(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sinh.h

// Begin content from: aclnn_max_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_V2_H_
#define OP_API_INC_MAX_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：按指定维度对输入tensor求元素最大值。
 *
 * @param [in] self: device侧的aclTensor，数据类型支持整型、浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] dims: host侧aclIntArray，指定了要进行最大值计算的维度， 数据类型支持INT32和INT64。
 * @param [in] keepDims: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] noopWithEmptyDims: host侧的布尔型，定义dims为空时的行为。
 * @param [in] out: device侧的aclTensor，数据类型支持整型、浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxV2GetWorkspaceSize(const aclTensor* self, const aclIntArray* dims, bool keepDims,
                                                 bool noopWithEmptyDims, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnMaxV2的第二段接口，用于执行计算。
 *
 * 算子功能：按指定维度对输入tensor求元素最大值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMaxV2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_V2_H_
// End content from: aclnn_max_v2.h

// Begin content from: aclnn_foreach_addcdiv_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCDIV_SCALAR_LIST_H_
#define ACLNN_FOREACH_ADDCDIV_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcdivScalarListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivScalarListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcdivScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcdiv_scalar_list.h

// Begin content from: aclnn_trace.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRACE_H_
#define OP_API_INC_TRACE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTrace的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnTraceGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnTrace的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnTrace(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRACE_H_// End content from: aclnn_trace.h

// Begin content from: aclnn_min.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MIN_H_
#define OP_API_INC_MIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，数据类型是self可转化的数据类型。数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnMin的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMinGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MIN_H_// End content from: aclnn_min.h

// Begin content from: aclnn_hardtanh_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDTANH_BACKWARD_H_
#define OP_API_INC_HARDTANH_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardtanhBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成Hardtanh的反向计算
 *
 * 实现说明：api
 * 计算的基本路径：如下所示
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] -->B([l0op::Contiguous])
 *     B -->C([l0op::HardtanhGrad])
 *     D[(self)] -->E([l0op::Contiguous])
 *     E -->C([l0op::HardtanhGrad])
 *     F((min)) --> C([l0op::HardtanhGrad])
 *     G((max)) --> C([l0op::HardtanhGrad])
 *     C --> H([l0op::ViewCopy])
 *     H --> K[(out)]
 * ```
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [in] min: 下界。
 * @param [in] max: 上界。
 * @param [out] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardtanhBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                            const aclScalar* min, const aclScalar* max, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnHardtanhBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：完成Hardtanh的反向计算
 *
 * 实现说明：api
 * 计算的基本路径：如下所示
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] -->B([l0op::Contiguous])
 *     B -->C([l0op::HardtanhGrad])
 *     D[(self)] -->E([l0op::Contiguous])
 *     E -->C([l0op::HardtanhGrad])
 *     F((min)) --> C([l0op::HardtanhGrad])
 *     G((max)) --> C([l0op::HardtanhGrad])
 *     C --> H([l0op::ViewCopy])
 *     H --> K[(out)]
 * ```
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [in] min: 下界。
 * @param [in] max: 上界。
 * @param [out] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16(仅昇腾910B和910_93
 * AI处理器支持)，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardtanhBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDTANH_BACKWARD_H_
// End content from: aclnn_hardtanh_backward.h

// Begin content from: aclnn_avgpool3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AVGPOOL3D_BACKWARD_H_
#define OP_API_INC_AVGPOOL3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAvgPool3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成avgpool3d的反向计算
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持FLOAT, BFLOAT16, FLOAT16。支持4维或者5维。支持非连续的Tensor。支持数据格式为ND。
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT, BFLOAT16, FLOAT16。支持4维或者5维。支持非连续的Tensor。支持数据格式为ND。
 * @param [in] kernelSize: npu
 * device侧的aclIntArray，长度为1(kD=kH=kW)或3(kD,kH,kW)，表示池化窗口大小。数据类型支持INT32和INT64。数值必须大于0。
 * @param [in] stride: npu
 * device侧的aclIntArray，长度为0(默认为kernelSize)或1(sD=sH=sW)或3(sD,sH,sW)，表示池化操作的步长。
 * 数据类型支持INT32和INT64。数值必须大于0。
 * @param [in] padding: npu
 * device侧的aclIntArray，长度为1(padD=padH=padW)或3(padD,padH,padW)，表示在输入的D、H、W方向上padding补0的层数。
 * 数据类型支持INT32和INT64。数值在[0, kernelSize/2]的范围内。
 * @param [in] ceilMode: 数据类型支持BOOL。表示计算输出shape时，向下取整（False），否则向上取整。
 * @param [in] countIncludePad: 数据类型支持BOOL。表示平均计算中包括零填充（True），否则不包括。
 * @param [in] divisorOverride: 数据类型支持INT64。如果指定，它将用作平均计算中的除数，当值为0时，该属性不生效。
 * @param [out] output: npu
 * device侧的aclTensor，输出Tensor，数据类型支持FLOAT16、BFLOAT16和FLOAT。支持4维或5维。支持非连续的Tensor。支持数据格式为ND。
 * 数据类型、数据格式需要与gradOutput一致。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAvgPool3dBackwardGetWorkspaceSize(const aclTensor* gradOuput, const aclTensor* self,
                                                             const aclIntArray* kernelSize, const aclIntArray* stride,
                                                             const aclIntArray* padding, bool ceilMode, bool countIncludePad,
                                                             int64_t divisorOverride, aclTensor* output,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAvgPool3dBackward的第二段接口，用于执行计算。
 *
 * 算子功能：完成avgpool3d的反向计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAvgPool3dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAvgPool3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AVGPOOL3D_BACKWARD_H_
// End content from: aclnn_avgpool3d_backward.h

// Begin content from: aclnn_batch_norm_backward_reduce.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_BATCH_NORM_BACKWARD_REDUCE_H_
#define OP_API_INC_LEVEL2_ACLNN_BATCH_NORM_BACKWARD_REDUCE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormReduceBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormReduceBackwardGetWorkspaceSize(
    const aclTensor* gradOut, const aclTensor* input, const aclTensor* mean, const aclTensor* invstd,
    const aclTensor* weight, const bool inputG, const bool weightG, const bool biasG, aclTensor* sumDy,
    aclTensor* sumDyXmu, aclTensor* gradWeight, aclTensor* gradBias, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormReduceBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormReduceBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BATCH_NORM_BACKWARD_REDUCE_H_
// End content from: aclnn_batch_norm_backward_reduce.h

// Begin content from: aclnn_batchmatmul_quant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BATCHMATMUL_QUANT_H_
#define OP_API_INC_BATCHMATMUL_QUANT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchMatmulQuant的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnBatchMatmulQuantGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                            const aclTensor* quantParam, const aclTensor* bias,
                                                            bool transposeX1, bool transposeX2, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBatchMatmulQuant的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchMatmulQuant(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BATCHMATMUL_QUANT_H// End content from: aclnn_batchmatmul_quant.h

// Begin content from: aclnn_geglu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GEGLU_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_GEGLU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeGluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成GeGlu的反向。
 *
 * 实现说明
 * api计算基本路径：
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous])
 *     C[(self)] --> D([l0op::Contiguous])
 *     E[(gelu)] --> F([l0op::Contiguous])
 *     B --> G([l0op::GeGluV2Grad])
 *     D --> G
 *     F --> G
 *     H((dim)) --> G
 *     I((approximate)) --> G
 *     G --> J([l0op::ViewCopy])
 *     J --> K[(gradInput)]
 * ```
 */

/**
 * @param [in] gradOutput：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape中除dim维外，其它维的大小跟self一样，
 * dim维的大小是self的一半，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape中除dim维外，其它维的大小跟gradOutput一样，
 * dim维的大小是gradOutput的两倍，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gelu：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape需要与gradOutput一样，支持非连续的Tensor， 数据格式支持ND。
 * @param [in] dim: 计算属性，host侧的整数，数据类型支持INT64，当前取值只支持-1。
 * @param [in] approximate: 计算属性，host侧的整数，数据类型支持INT64，取值范围是0('none')、1('tanh') ，当前取值只支持
 * 1('tanh') 。
 * @param [in] activateLeft:
 * 计算属性，host侧的布尔值，表示激活函数操作数据块的方向，默认值为false，表示对右边做activate。
 * @param [out] gradInput：计算输出，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape需要与self一样，支持非连续的Tensor， 数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                         const aclTensor* gelu, int64_t dim, int64_t approximate,
                                                         aclTensor* gradInput, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnGeGluV3Backward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * @param [in] gradOutput：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape中除dim维外，其它维的大小跟self一样，
 * dim维的大小是self的一半，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape中除dim维外，其它维的大小跟gradOutput一样，
 * dim维的大小是gradOutput的两倍，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gelu：计算输入，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape需要与gradOutput一样，支持非连续的Tensor， 数据格式支持ND。
 * @param [in] dim: 计算属性，host侧的整数，数据类型支持INT64，当前取值只支持-1。
 * @param [in] approximate: 计算属性，host侧的整数，数据类型支持INT64，取值范围是0('none')、1('tanh') ，当前取值只支持
 * 1('tanh') 。
 * @param [in] activateLeft:
 * 计算属性，host侧的布尔值，表示激活函数操作数据块的方向，默认值为false，表示对右边做activate。
 * @param [out] gradInput：计算输出，npu
 * device侧的aclTensor，数据类型支持FLOAT16，shape需要与self一样，支持非连续的Tensor， 数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluV3BackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                           const aclTensor* gelu, int64_t dim, int64_t approximate,
                                                           bool activateLeft, aclTensor* gradInput,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGeGluBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeGluBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

/**
 * @brief aclnnGeGluBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeGluBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluV3Backward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GEGLU_BACKWARD_H_
// End content from: aclnn_geglu_backward.h

// Begin content from: aclnn_batch_norm_elemt_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BATCH_NORM_ELEMT_BACKWARD_H_
#define OP_API_INC_BATCH_NORM_ELEMT_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormElemtBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormElemtBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* input,
                                                                  const aclTensor* mean, const aclTensor* invstd,
                                                                  const aclTensor* weight, const aclTensor* sumDy,
                                                                  const aclTensor* sumDyXmu, aclTensor* counter,
                                                                  aclTensor* gradInput, uint64_t* workspaceSize,
                                                                  aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormElemtBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormElemtBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BATCH_NORM_ELEMT_BACKWARD_H_
// End content from: aclnn_batch_norm_elemt_backward.h

// Begin content from: aclnn_silu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SILU_H_
#define OP_API_INC_LEVEL2_ACLNN_SILU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSilu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnSiluGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnSilu的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSilu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_silu.h

// Begin content from: aclnn_replication_pad1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REPLICATION_PAD1D_H_
#define OP_API_INC_REPLICATION_PAD1D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReplicationPad1d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：使用输入边界填充输入tensor。
 * @param [in] self: 数据类型支持FLOAT16, FLOAT32, DOUBLE, INT8, INT16, INT32, INT64, UINT8,
 * COMPLEX64, COMPLEX128，支持非连续的Tensor，数据格式支持ND，维度支持二维或三维，在最后一维做pad。
 * @param [in] padding: 数据类型为INT64，长度为2，数值依次代表左右两边需要填充的值。
 * @param [in] out: 数据类型、数据格式、维度与self一致，最后一维度的数值等于self最后一维度的数值加padding的两个值。
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad1dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad1d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReplicationPad1dGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad1d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REPLICATION_PAD1D_H_// End content from: aclnn_replication_pad1d.h

// Begin content from: aclnn_prompt_flash_attention.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License. 
 */

#ifndef ACLNN_PROMPT_FLASH_ATTENTION_H_
#define ACLNN_PROMPT_FLASH_ATTENTION_H_
// #include "aclnn/acl_meta.h"
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief The first interface of aclnnPromptFlashAttention is used to calculate the workspace size based on the specific calculation process.
 * @domain aclnn_math
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttentionGetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *pseShift,
    const aclTensor *attenMask,
    const aclIntArray *actualSeqLengths,
    int64_t numHeads,
    double scaleValue,
    int64_t preTokens,
    int64_t nextTokens,
    char *inputLayout,
    int64_t numKeyValueHeads,
    const aclTensor *attentionOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief The second interface of aclnnPromptFlashAttention is used to perform calculations.
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttention(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_prompt_flash_attention.h

// Begin content from: aclnn_x_log_y_scalar_other.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_X_LOG_Y_SCALAR_OTHER_H_
#define OP_API_INC_X_LOG_Y_SCALAR_OTHER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnXLogYScalarOther的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnXLogYScalarOtherGetWorkspaceSize(const aclTensor* self, const aclScalar* other,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnXLogYScalarOther的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnXLogYScalarOther(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceXLogYScalarOther的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceXLogYScalarOtherGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceXLogYScalarOther的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceXLogYScalarOther(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_X_LOG_Y_SCALAR_OTHER_H_// End content from: aclnn_x_log_y_scalar_other.h

// Begin content from: aclnn_layer_norm_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_LAYER_NORM_BACKWARD_H_
#define OP_API_INC_LEVEL2_LAYER_NORM_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLayerNormBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnLayerNormBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* input,
                                                             const aclIntArray* normalizedShape, const aclTensor* mean,
                                                             const aclTensor* rstd, const aclTensor* weightOptional,
                                                             const aclTensor* biasOptional,
                                                             const aclBoolArray* outputMask, aclTensor* gradInputOut,
                                                             aclTensor* gradWeightOut, aclTensor* gradBiasOut,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLayerNormBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLayerNormBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_LAYER_NORM_BACKWARD_H_
// End content from: aclnn_layer_norm_backward.h

// Begin content from: aclnn_copy.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_COPY_H_
#define OP_API_INC_COPY_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceCopy 的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将src中的元素复制到selfRef张量中并返回selfRef。
 *
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持int8, int16, int32, int64, uint8, float16, float32, bool,
 * double, complex64, complex128, uint16, uint32, uint64。支持[非连续的Tensor](#)，数据格式支持ND
 * @param [in] src: npu device侧的aclTensor，数据类型支持int8, int16, int32, int64, uint8, float16, float32, bool,
 * double, complex64, complex128, uint16, uint32, uint64。支持[非连续的Tensor](#)，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCopyGetWorkspaceSize(aclTensor* selfRef, const aclTensor* src,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceCopy 的第二段接口，用于执行计算。
 *
 * 算子功能：将src中的元素复制到selfRef张量中并返回selfRef。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceZeroGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCopy(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CONVOLUTION_H_
// End content from: aclnn_copy.h

// Begin content from: aclnn_fmod_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_FMOD_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_FMOD_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFmodScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回self除以other的余数。
 * 计算公式：$$ out_{i} = self_{i} - (other_{i} *\left \lfloor (self_{i}/other_{i}) \right \rfloor) $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] -->B([l0op::Contiguous])
 * B -->C([l0op::Cast])
 * C -->D([l0op::Mod])
 * E[(other)]-->F([l0op::Cast])
 * F --> D
 * D--> G([l0op::Cast])
 * G --> I([l0op::ViewCopy])
 * I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor
 * 数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与other的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclScalar，
 * 数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与self的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8，
 * shape需要是self与other broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnFmodScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFmodScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnFmodScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFmodScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceFmodScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回selfRef除以other的余数。
 * 计算公式：$$ out_{i} = self_{i} - (other_{i} *\left \lfloor (self_{i}/other_{i}) \right \rfloor) $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] -->B([l0op::Contiguous])
 * B -->C([l0op::Cast])
 * C -->D([l0op::Mod])
 * E[(other)]-->F([l0op::Cast])
 * F --> D
 * D--> G([l0op::Cast])
 * G --> I([l0op::ViewCopy])
 * I --> J[(out)]
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor
 * 数据类型支持DOUBLE、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与other的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclScalar，
 * 数据类型支持DOUBLE、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与self的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnInplaceFmodScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFmodScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceFmodScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFmodScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FMOD_SCALAR_H_
// End content from: aclnn_fmod_scalar.h

// Begin content from: aclnn_lerp_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_LERP_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_LERP_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLerp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：根据给定的权重，在起始和结束Tensor之间进行线性插值，返回插值后的Tensor。

 *
 * 计算图：
 * ```mermaid
 * graph LR
 * A1[(self)] --> B1([l0op::Contiguous])
 * A2[(end)] --> B2([l0op::Contiguous])
 * A3[(weight)] --> B3([l0op::Contiguous])
 * C([l0op::Lerp])
 * B1 --> C
 * B2 --> C
 * B3 --> C
 * C --> D([l0op::Cast])
 * D --> E([l0op::ViewCopy]) --> F[(out)]
 * ```
 *
 * @param [in] self: 公式中的输入start，数据类型支持FLOAT16、FLOAT，shape需要与end和weight满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] end: 公式中的输入end，数据类型支持FLOAT16、FLOAT且与self的数据类型一致，shape需要与self和weight满足broadcast关系。
 * 支持非连续Tensor，数据格式支持ND。
 * @param [in] weight: 公式中的输入weight，数据类型支持FLOAT16、FLOAT且与`self`的数据类型一致，shape需要与`self`和`end`满足broadcast关系。
 * 支持非连续Tensor，数据格式支持ND。
 * @param [in] out: 公式中的out，数据类型支持FLOAT16、FLOAT且与self的数据类型一致、shape与self、end和weight
 * broadcast之后的shape一致。支持非连续Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLerpGetWorkspaceSize(const aclTensor* self, const aclTensor* end, const aclTensor* weight,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLerp的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLerpGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLerp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceLerp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：根据给定的权重，在起始和结束Tensor之间进行线性插值，返回插值后的Tensor。

 *
 * @param [in] selfRef: 公式中的输入start，数据类型支持FLOAT16、FLOAT，shape需要与end和weight满足broadcast关系，且broadcast后的shape与selfRef一致。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] end: 公式中的输入end，数据类型支持FLOAT16、FLOAT且与self的数据类型一致，shape需要与selfRef和weight满足broadcast关系，
 * 且broadcast后的shape与selfRef一致。 支持非连续Tensor，数据格式支持ND。
 * @param [in] weight: 公式中的输入weight，数据类型支持FLOAT16、FLOAT，shape需要与selfRef和end满足broadcast关系。支持非连续Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLerpGetWorkspaceSize(aclTensor* selfRef, const aclTensor* end,
                                                       const aclTensor* weight, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLerp的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLerpGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLerp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_LERP_TENSOR_H_// End content from: aclnn_lerp_tensor.h

// Begin content from: aclnn_l1_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_L1_LOSS_H_
#define OP_API_INC_L1_LOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnL1Loss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] self:
 * 公式中的输入`self`，与target满足数据类型推导规则，推导后数据类型支持INT64、FLOAT、FLOAT16，shape需要与target满足broadcast规则。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] target:
 * 公式中的输入`target`，与self满足数据类型推导规则，推导后数据类型支持INT64、FLOAT、FLOAT16，shape需要与self满足broadcast规则。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: 公式中的输入`reduction`，指定要应用到输出的缩减，
 * 支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] out:
 * 公式中的`out`，数据类型支持INT64、FLOAT、FLOAT16。reduction为0时，shape需要与self和target进行broadcast后的shape一致，
 * 其他场景为rank1de Tensor。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnL1LossGetWorkspaceSize(const aclTensor* self, const aclTensor* target, int64_t reduction,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnL1Loss的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnL1LossGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnL1Loss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_L1_LOSS_H_
// End content from: aclnn_l1_loss.h

// Begin content from: aclnn_logaddexp2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG_ADD_EXP_2_H_
#define OP_API_INC_LOG_ADD_EXP_2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogAddExp2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成幂和取对数计算（底为2）
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogAddExp2GetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogAddExp2的第二段接口，用于执行计算。
 *
 * 算子功能：完成幂和取对数计算（底为2）
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLogAddExp2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogAddExp2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG_ADD_EXP_2_H_
// End content from: aclnn_logaddexp2.h

// Begin content from: aclnn_moe_init_routing_v2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_INIT_ROUTING_V2_H_
#define ACLNN_MOE_INIT_ROUTING_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeInitRoutingV2GetWorkspaceSize
 * parameters :
 * x : required
 * expertIdx : required
 * activeNum : optional
 * expertCapacity : optional
 * expertNum : optional
 * dropPadMode : optional
 * expertTokensCountOrCumsumFlag : optional
 * expertTokensBeforeCapacityFlag : optional
 * expandedXOut : required
 * expandedRowIdxOut : required
 * expertTokensCountOrCumsumOutOptional : optional
 * expertTokensBeforeCapacityOutOptional : optional
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingV2GetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *expertIdx,
    int64_t activeNum,
    int64_t expertCapacity,
    int64_t expertNum,
    int64_t dropPadMode,
    int64_t expertTokensCountOrCumsumFlag,
    bool expertTokensBeforeCapacityFlag,
    const aclTensor *expandedXOut,
    const aclTensor *expandedRowIdxOut,
    const aclTensor *expertTokensCountOrCumsumOutOptional,
    const aclTensor *expertTokensBeforeCapacityOutOptional,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeInitRoutingV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_init_routing_v2.h

// Begin content from: aclnn_exp2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_EXP2_H_
#define OP_API_INC_LEVEL2_ACLNN_EXP2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：返回一个新的张量，该张量的每个元素都是输入张量对应元素的指数。
 * 计算公式：如下
 * $$
 *     out_{i} = 2^{self_{i}}
 * $$
 * @brief aclnnExp2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE，支持非连续的
 * Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型需要是self可转换的数据类型，shape需要与self一致，支持非连续的Tensor，数据
 * 格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExp2GetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnExp2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnExp2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExp2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceExp2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16（910B支持）、
 * DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExp2GetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceExp2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceExp2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExp2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_EXP2_H_
// End content from: aclnn_exp2.h

// Begin content from: aclnn_unique.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNIQUE_H_
#define OP_API_INC_UNIQUE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUnique的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回输入张量中的独特元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B ---> F([UniqueWithCountsAndSorting])
 *     C[(sorted)] --->F
 *     D[(returnInverse)] --->F
 *     F --> G([valueOut])
 *     F --> H([inverseOut])
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持BOOL, FLOAT, FLOAT16, DOUBLE, UINT8, INT8, UINT16, INT16,
 * INT32, UINT32, UINT64, INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] sorted: 可选参数，默认False，表示是否对 valueOut 按升序进行排序。
 * @param [in] returnInverse: 可选参数，默认False，表示是否返回输入数据中各个元素在 valueOut 中的下标。
 * @param [in] valueOut: npu device侧的aclTensor, 第一个输出张量，输入张量中的唯一元素，数据类型支持BOOL, FLOAT,
 * FLOAT16, DOUBLE, UINT8, INT8, UINT16, INT16, INT32, UINT32, UINT64, INT64，数据格式支持ND。
 * @param [in] inverseOut: npu
 * device侧的aclTensor，第二个输出张量，当returnInversie为True时有意义，返回self中各元素在valueOut中出现的位置下
 *                      标，数据类型支持INT64，shape与self保持一致
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUniqueGetWorkspaceSize(const aclTensor* self, bool sorted, bool returnInverse,
                                                  aclTensor* valueOut, aclTensor* inverseOut, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnUnique的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnUniqueGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUnique(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNIQUE_H_// End content from: aclnn_unique.h

// Begin content from: aclnn_grouped_matmul_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_MATMUL_V2_H
#define OP_API_INC_GROUPED_MATMUL_V2_H
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupedMatmulV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 表示公式中的x，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] weight:
 * 表示公式中的weight，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] biasOptional:
 * 表示公式中的bias，数据类型支持FLOAT16、FLOAT32、INT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] scaleOptional: 表示量化参数，数据类型支持UINT64数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] offsetOptional: 表示量化参数，数据类型支持FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantScaleOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantOffsetOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] groupListOptional: 可选参数，代表输入和输出分组轴上的索引情况，数据类型支持INT64，支持的最大长度为128个。
 * @param [in] splitItem:
 * 整数型参数，代表输出是否要做tensor切分，0/1代表输出为多tensor；2/3代表输出为单tensor，默认值为0。
 * @param [in] groupType:
 * 整数型参数，代表需要切分的轴，-1代表不需要切分；0代表需要切分M轴；1代表需要切分N轴；2代表需要切分K轴。
 * @param [out] y: 表示公式中的out，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV2GetWorkspaceSize(
    const aclTensorList* x, const aclTensorList* weight, const aclTensorList* biasOptional,
    const aclTensorList* scaleOptional, const aclTensorList* offsetOptional,
    const aclTensorList* antiquantScaleOptional, const aclTensorList* antiquantOffsetOptional,
    const aclIntArray* groupListOptional, int64_t splitItem, int64_t groupType, const aclTensorList* y,
    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGroupedMatmulV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_grouped_matmul_v2.h

// Begin content from: aclnn_convert_weight_to_int4_pack.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CONVERT_WEIGHT_TO_INT4_PACK_H_
#define OP_API_INC_LEVEL2_ACLNN_CONVERT_WEIGHT_TO_INT4_PACK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnConvertWeightToINT4Pack的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
*/
ACLNN_API aclnnStatus aclnnConvertWeightToINT4PackGetWorkspaceSize(const aclTensor *weight, aclTensor *weightInt4Pack,
                                                                   uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnConvertWeightToINT4Pack的第二段接口，用于执行计算。
*/
ACLNN_API aclnnStatus aclnnConvertWeightToINT4Pack(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_LEVEL2_ACLNN_CONVERT_WEIGHT_TO_INT4_PACK_H_// End content from: aclnn_convert_weight_to_int4_pack.h

// Begin content from: aclnn_cummax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CUMMAX_H_
#define OP_API_INC_LEVEL2_ACLNN_CUMMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：计算self中的累积最大值，并返回该值以及其对应的索引
 * 计算公式：
 * $self_{i}$是输入张量self中，从维度dim视角来看的某个元素（其它维度下标不变，只dim维度下标依次递增），
 * $$
 * valuesOut_{i} = max(self_{1}, self_{2}, self_{3}, ......, self_{i})
 * $$
 * $$
 * indicesOut_{i} = argmax(self_{1}, self_{2}, self_{3}, ......, self_{i})
 * $$
 */

/**
 * @brief aclnnCummax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、DOUBLE、UINT8、INT8、INT16、INT32、INT64、
 * FLOAT16、BFLOAT16、BOOL，数据类型需要能转换成out的数据类型，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] dim: host侧的整数，数据类型支持INT64。
 * @param [in] valuesOut：npu device侧的aclTensor，数据类型支持FLOAT、DOUBLE、UINT8、INT8、INT16、INT32、INT64、
 * FLOAT16、BFLOAT16、BOOL，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上, 且shape必须与self一致。
 * @param [in] indicesOut：npu device侧的aclTensor，数据类型支持INT32、INT64。支持非连续的Tensor，数据格式支持ND，
 * 数据维度不支持8维以上, 且shape必须与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCummaxGetWorkspaceSize(const aclTensor* self, int64_t dim, aclTensor* valuesOut,
                                                  aclTensor* indicesOut, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnCummax的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCummaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCummax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CUMMAX_H_
// End content from: aclnn_cummax.h

// Begin content from: aclnn_foreach_minimum_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_MINIMUM_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_MINIMUM_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachMinimumScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素和标量值scalar执行逐元素比较，返回最小值的输出。
 * 计算公式：
 * out_{i}=min(x_{i}, scalar)
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachMinimumScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachMinimumScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素和标量值scalar执行逐元素比较，返回最小值的输出。
 * 计算公式：
 * out_{i}=min(x_{i}, scalar)
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachMinimumScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachMinimumScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_minimum_scalar_v2.h

// Begin content from: aclnn_argmax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ARGMAX_H_
#define OP_API_INC_ARGMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnArgMax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：返回张量在指定维度上的最大值的索引。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMaxV2])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [in] dim: host侧int64类型，指定了要进行最大值计算的维度。
 * @param [in] keepdim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT32、INT64。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnArgMaxGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepdim,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnArgMax的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：返回张量在指定维度上的最大值的索引。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMaxV2])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnArgMaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnArgMax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ARGMAX_H_// End content from: aclnn_argmax.h

// Begin content from: aclnn_pdist_forward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_PDIST_H_
#define OP_API_INC_PDIST_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPdistForward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnPdistForwardGetWorkspaceSize(const aclTensor* self, const aclScalar* pScalar, aclTensor* out,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPdistForward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnPdistForward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_pdist_forward.h

// Begin content from: aclnn_rsub.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RSUB_H_
#define OP_API_INC_RSUB_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRsubs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ out_i = other - self_i * alpha $$
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型，复数，且数据类型需要与other构成互相推导关系，
 * shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，数据类型支持整型，浮点类型，复数，且数据类型需要与self构成互相推导关系。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型，复数，
 * 且数据类型需要是self与other推导之后可转换的数据类型，shape与self一致。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRsubsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, const aclScalar* alpha,
                                                 aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRsubs的第二段接口，用于执行计算。
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ out_i = other - self_i * alpha $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnRsubsGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRsubs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnRsub的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ out_i = other_i - self_i * alpha $$
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型，复数，且数据类型需要与other构成互相推导关系，
 * shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，数据类型支持整型，浮点类型，复数，且数据类型需要与self构成互相推导关系，
 * shape需要与self满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型，复数，
 * 且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other broadcast之后的shape。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRsubGetWorkspaceSize(const aclTensor* self, const aclTensor* other, const aclScalar* alpha,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRsub的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ out_i = other_i - self_i * alpha $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRsubGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRsub(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_RSUB_H_
// End content from: aclnn_rsub.h

// Begin content from: aclnn_reflection_pad1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REFLECTION_PAD1D_H_
#define OP_API_INC_REFLECTION_PAD1D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad1d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：使用输入边界的反射填充输入tensor。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持BFLOAT16,FLOAT16, FLOAT32, DOUBLE, INT8, INT16,
 * INT32, INT64, UINT8, BOOL，数据格式支持ND，维度支持二维或三维。
 * @param [in] padding: npu device侧的aclIntArray数组,
 * 数据类型为INT64，长度为2，两个数值依次代表左右两边需要填充的值，且均需 小于self最后一维度的数值。
 * @param [in] out: npu device侧的aclTensor,
 * 数据类型、数据格式、维度与self一致，最后一维度的数值等于self最后一维度的数值加padding前两个值。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad1dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad1d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界的反射填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReflectionPad1dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad1d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REFLECTION_PAD1D_H_// End content from: aclnn_reflection_pad1d.h

// Begin content from: aclnn_swin_transformer_ln_qkv_quant.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_SWIN_TRANSFORMER_LN_QKV_QUANT_H_
#define ACLNN_SWIN_TRANSFORMER_LN_QKV_QUANT_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnSwinTransformerLnQkvQuantGetWorkspaceSize
 * parameters :
 * x : required
 * gamma : required
 * beta : required
 * weight : required
 * bias : required
 * quantScale : required
 * quantOffset : required
 * dequantScale : required
 * headNum : required
 * seqLength : required
 * epsilon : required
 * oriHeight : required
 * oriWeight : required
 * hWinSize : required
 * wWinSize : required
 * weightTranspose : required
 * queryOutputOut : required
 * keyOutputOut : required
 * valueOutputOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwinTransformerLnQkvQuantGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *gamma,
    const aclTensor *beta,
    const aclTensor *weight,
    const aclTensor *bias,
    const aclTensor *quantScale,
    const aclTensor *quantOffset,
    const aclTensor *dequantScale,
    int64_t headNum,
    int64_t seqLength,
    double epsilon,
    int64_t oriHeight,
    int64_t oriWeight,
    int64_t hWinSize,
    int64_t wWinSize,
    bool weightTranspose,
    const aclTensor *queryOutputOut,
    const aclTensor *keyOutputOut,
    const aclTensor *valueOutputOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnSwinTransformerLnQkvQuant
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwinTransformerLnQkvQuant(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_swin_transformer_ln_qkv_quant.h

// Begin content from: aclnn_masked_scatter.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MASKED_SCATTER_H_
#define OP_API_INC_MASKED_SCATTER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaskedScatter的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：根据掩码mask张量中元素为True的位置，复制source中的元素到selfRef对应的位置上。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *   A[(selfRef)] -->B([l0op::Contiguous])
 *   B  -->D([l0op::MaskedScatter])
 *   D  -->J([l0op::ViewCopy])
 *   J   --> K[(selfRef)]
 *   A2[(mask)] -->B2([l0op::Contiguous])
 *   B2 --> C2([l0op::Cast])
 *   C2  -->D
 *   A1[(source)]-->B1([l0op::Contiguous])
 *   B1-->D
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] mask: npu device侧的aclTensor，数据类型支持BOOL、UINT8。shape不能大于selfRef，且需要和selfRef满足
 * broadcast关系。数据格式支持ND。
 * @param [in] source:npu device侧的aclTensor，数据类型需要与selfRef相同。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedScatterGetWorkspaceSize(aclTensor* selfRef, const aclTensor* mask,
                                                                const aclTensor* source, uint64_t* workspaceSize,
                                                                aclOpExecutor** executor);
/**
 * @brief aclnnMaskedScatter的第二段接口，用于执行计算。
 *
 * 算子功能：根据掩码mask张量中元素为True的位置，复制source中的元素到self对应的位置上。
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *   A[(selfRef)] -->B([l0op::Contiguous])
 *   B  -->D([l0op::MaskedScatter])
 *   D  -->J([l0op::ViewCopy])
 *   J   --> K[(selfRef)]
 *   A2[(mask)] -->B2([l0op::Contiguous])
 *   B2 --> C2([l0op::Cast])
 *   C2  -->D
 *   A1[(source)]-->B1([l0op::Contiguous])
 *   B1-->D
 * ```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMaskedScatterGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedScatter(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MASKED_SCATTER_H_
// End content from: aclnn_masked_scatter.h

// Begin content from: aclnn_ge_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GESCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_GESCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于等于other Scalar的值，返回一个Bool类型的Tensor，
 * 对应输入Tensor中每个位置的大于等于判断是否成立
 * 计算公式： $$ out_{i}= (self_i >= other) ? True : False $$
 *
 * @param [in] self: 待进行ge计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BFLOAT16，数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [in] other: 待进行ge计算的入参,aclScalar。
 * @param [in] out: ge计算的出参。npu device侧的aclTensor，
 * 输出一个数据类型为BOOL的Tensor，数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGeScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

/**
 * @brief aclnnInplaceGeScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于等于other Scalar的值
 * 计算公式： $$ selfRef_{i} = (selfRef_{i} >= other_{i}) ? True : False $$
 *
 * @param [in] selfRef: 待进行ge本地计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL、BFLOAT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: 待进行ge计算的入参,aclScalar。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGeScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceGeScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceGeScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GESCALAR_H_
// End content from: aclnn_ge_scalar.h

// Begin content from: aclnn_scatter_update.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SCATTER_UPDATE_H_
#define OP_API_INC_SCATTER_UPDATE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScatterUpdate的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 将tensor updates中的值按指定的轴axis和索引indices逐个更新tensor data中的值。该算子为自定义算子语义,
 * 无对应的tensorflow或pytorch接口。
 * @param [in] data: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT8, BFLOAT16,data只支持四维，
 * 且维度大小和维度数量需要与updates一致。支持非连续的Tensor，数据格式支持ND。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持INT32,
 * int64类型，目前仅支持一维。支持非连续的Tensor，数据格式支持ND。
 * @param [in] updates: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT32, INT8,
 * BFLOAT16,且维度大小和维度数量需要与data一致。 支持非连续的Tensor，数据格式支持ND,
 * @param [in] axis: host侧的axis, 数据类型支持INT64。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceScatterUpdateGetWorkspaceSize(aclTensor* data, const aclTensor* indices,
                                                                const aclTensor* updates, int64_t axis,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnScatterUpdate的第二段接口，用于执行计算
 *
 * 算子功能: 将tensor updates中的值按指定的轴axis和索引indices逐个更新tensor data中的值。该算子为自定义算子语义,
 * 无对应的tensorflow或pytorch接口。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceScatterUpdateGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceScatterUpdate(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SCATTER_UPDATE_H_// End content from: aclnn_scatter_update.h

// Begin content from: aclnn_median.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MEDIAN_H_
#define OP_API_INC_MEDIAN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMedian的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64，
 * 且数据类型与valuesOut相同。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND（[参考](#参考)）。
 * @param [in] valuesOut: device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64，
 * 且数据类型与self相同，valuesOut为一维Tensor，shape为(1,)。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND（[参考](#参考)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMedianGetWorkspaceSize(const aclTensor* self, aclTensor* valuesOut, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnMedian的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMedianGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMedian(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnMedianDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: self：npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] dim：指定的维度，数据类型支持INT64
 * @param [in] keepDim：reduce轴的维度是否保留，数据类型支持BOOL
 * @param [in] valuesOut: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] indicesOut：npu
 * device侧的aclTensor，数据类型支持INT64。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMedianDimGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepDim,
                                                     aclTensor* valuesOut, aclTensor* indicesOut,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMedianDim的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMedianDimGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMedianDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnNanMedian的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64，
 * 且数据类型与valuesOut相同。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND（[参考](#参考)）。
 * @param [in] out: device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64，
 * 且数据类型与self相同，out为一维Tensor，shape为(1,)。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND（[参考](#参考)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanMedianGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnNanMedian的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMedianGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanMedian(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnNanMedianDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: self：npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] dim：指定的维度，数据类型支持INT64
 * @param [in] keepDim：reduce轴的维度是否保留，数据类型支持BOOL
 * @param [in] valuesOut: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] indicesOut：npu
 * device侧的aclTensor，数据类型支持INT64。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanMedianDimGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepDim,
                                                        aclTensor* valuesOut, aclTensor* indicesOut,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNanMedianDim的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNanMedianDimGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanMedianDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MEDIAN_H_// End content from: aclnn_median.h

// Begin content from: aclnn_bitwise_xor_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http: *www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：计算输入张量self中每个元素与输入张量other中对应位置元素的按位异或，输入self和other必须是整数或布尔类型，对于布尔类型，
 * 计算逻辑异或。
 * 计算公式：如下
 * $$
 * \text{out}_i =
 * \text{self}_i \, \bigoplus\, \text{other}_i
 * $$
 *
 * 实现说明：如下
 * 计算图一：如下
 * 场景：经过类型推导后的数据类型为BOOL时，需要调用l0::NotEqual接口做计算
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::NotEqual])
 *   D[(other)] --> E([l0op::Contiguous])
 *   E --> C
 *   C --> F([l0op::Cast])
 *   F --> G([l0op::ViewCopy])
 *   G --> H[(out)]
 * ```
 *
 * 计算图二：如下
 * 场景：不满足计算图一的条件时，都会调用l0::BitwiseXor接口做计算
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::Cast])
 *   C --> D([l0op::BitwiseXor])
 *   E[(other)] --> F([l0op::Contiguous])
 *   F --> G([l0op::Cast])
 *   G --> D
 *   D --> H([l0op::Cast])
 *   H --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 */

/**
 * @brief aclnnBitwiseXorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与other的数据类型
 * 需满足数据类型推导规则，shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] other：npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与self的数据类型
 * 需满足数据类型推导规则，shape需要与self满足broadcast关系，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、FLOAT、FLOAT16、DOUBLE、
 * BFLOAT16、COMPLEX64、COMPLEX128，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后 的shape，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseXorTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseXorTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnBitwiseXorTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseXorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseXorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与other的
 * 数据类型需满足数据类型推导规则，且推导后的数据类型需要能转换成selfRef自身的数据类型，shape需要与other满足broadcast关系，且
 * broadcast之后的shape需要与selfRef自身的shape相同，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] other：npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与selfRef的
 * 数据类型需满足数据类型推导规则，shape需要与selfRef满足broadcast关系，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维
 * 以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseXorTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseXorTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceBitwiseXorTensorGetWorkspaceSize 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseXorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_TENSOR_H_// End content from: aclnn_bitwise_xor_tensor.h

// Begin content from: aclnn_silu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SILU_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_SILU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSiluBackwardGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 * 算子功能：求silu反向传播的梯度值
 */
ACLNN_API aclnnStatus aclnnSiluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                        aclTensor* gradInput, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/*
 * @brief aclnnSiluBackwardGetWorkspaceSize的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSiluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BINARY_CROSS_ENTROPY_BACKWARD_H_// End content from: aclnn_silu_backward.h

// Begin content from: aclnn_apply_fused_ema_adam.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_APPLY_FUSED_EMA_ADAM_H_
#define ACLNN_APPLY_FUSED_EMA_ADAM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnApplyFusedEmaAdamGetWorkspaceSize
 * parameters :
 * grad : required
 * varRef : required
 * mRef : required
 * vRef : required
 * sRef : required
 * step : required
 * lr : optional
 * emaDecay : optional
 * beta1 : optional
 * beta2 : optional
 * eps : optional
 * mode : optional
 * biasCorrection : optional
 * weightDecay : optional
 * varRef : required
 * mRef : required
 * vRef : required
 * sRef : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnApplyFusedEmaAdamGetWorkspaceSize(
    const aclTensor *grad,
    aclTensor *varRef,
    aclTensor *mRef,
    aclTensor *vRef,
    aclTensor *sRef,
    const aclTensor *step,
    double lr,
    double emaDecay,
    double beta1,
    double beta2,
    double eps,
    int64_t mode,
    bool biasCorrection,
    double weightDecay,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnApplyFusedEmaAdam
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnApplyFusedEmaAdam(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_apply_fused_ema_adam.h

// Begin content from: aclnn_bitwise_or_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBitwiseOrTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 *
 算子功能：计算张量self中每个元素与other张量中对应位置的元素的按位或。张量必须是整数或布尔类型，对于布尔类型，计算逻辑或。
 * 计算公式：如下
 * $$
 * \text{out}_i =
 * \text{self}_i \, | \, \text{other}_i
 * $$
 *
 * 实现说明：如下
 * 计算图一：如下
 * 场景：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalOr接口做计算

 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous])
 * B --> C([l0op::Cast])
 * C --> D([l0op::LogicalOr])
 * E[(other)] --> D
 * D --> F([l0op::Cast])
 * F --> G([l0op::ViewCopy])
 * G --> H[(out)]
 * ```
 *
 * 计算图二：如下
 * 场景：不满足计算图一的条件时，都会调用l0::BitwiseOr接口做计算
 *
 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous])
 * B --> C([l0op::Cast])
 * C --> D([l0op::BitwiseOr])
 * E[(other)] --> D
 * D --> F([l0op::Cast])
 * F --> G([l0op::ViewCopy])
 * G --> H[(out)]
 * ```
 *
 * @param [in] self: npu
 device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、UINT16、UINT32、UINT64，
 * 且数据类型需要与other构成互相推导关系，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] other：npu
 device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、UINT16、UINT32、UINT64，
 * 且数据类型需要与self构成互相推导关系。
 * @param [in] out: npu
 device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、UINT16、UINT32、UINT64、
 * FLOAT、FLOAT16、DOUBLE、BFLOAT16、COMPLEX64、COMPLEX128，且数据类型需要是self与other推导之后可转换的数据类型，
 * shape需要与self一致，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseOrTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseOrTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnBitwiseOrTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseOrTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseOrTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，
 * 且数据类型需要与other构成互相推导关系，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] other：npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，
 * 且数据类型需要与self构成互相推导关系。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseOrTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseOrTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口
 * aclnnInplaceBitwiseOrTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseOrTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_TENSOR_H_
// End content from: aclnn_bitwise_or_tensor.h

// Begin content from: aclnn_max_pool2d_with_indices_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxPool2dWithMaskBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 正向池化aclnnMaxPool2dWithMask的反向传播：
 *
 * @param [in] gradOutput: 梯度Tensor，与正向的输出shape一致，npu
 * device侧的aclTensor，数据类型支持float16、float32（仅昇腾910B AI处理器支持）, 数据格式支持NCHW(CHW)，
 * 支持非连续的Tensor。
 * @param [in] self: npu device侧的aclTensor，数据类型支持float16、float32（仅昇腾910B AI处理器支持）,
 * 数据格式支持NCHW(CHW)， 支持非连续的Tensor。
 * @param [in] indices: npu
 * device侧的aclTensor，最大值在求mask的kernel位置的bit值组成的Tensor，数据类型支持int8，数据格式支持NCHW(CHW)，
 * 支持非连续的Tensor。
 * @param [in] kernelSize: aclIntArray类型， 表示最大池化的窗口大小。
 * @param [in] stride: aclIntArray类型， 窗口移动的步长。
 * @param [in] padding: aclIntArray类型， 每一条边补充的层数，补充的位置填写负无穷。
 * @param [in] dilation: aclIntArray类型， 控制窗口中元素的步幅。
 * @param [in] ceilMode: aclIntArray类型， 为true时用上取整的方法计算输出形状，默认向下取整。
 * @param [in] gradInput: npu device侧的aclTensor，数据类型支持float16、float32（仅昇腾910B
 * AI处理器支持），数据格式支持NCHW(CHW)， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithMaskBackwardGetWorkspaceSize(
    const aclTensor* gradOutput, const aclTensor* self, const aclTensor* indices, const aclIntArray* kernelSize,
    const aclIntArray* stride, const aclIntArray* padding, const aclIntArray* dilation, bool ceilMode,
    aclTensor* gradInput, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxPool2dWithMaskBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMaxPool2dWithMaskBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithMaskBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     aclrtStream stream);

/**
 * @brief aclnnMaxPool2dWithIndicesBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 正向池化aclnnMaxPool2dWithIndices的反向传播：
 *
 * @param [in] gradOutput: 梯度Tensor，与正向的输出shape一致，npu device侧的aclTensor，数据类型支持float32（仅昇腾910B
 * AI处理器支持）, 数据格式支持NCHW(CHW)， 支持非连续的Tensor。
 * @param [in] self: npu device侧的aclTensor，数据类型支持float32（仅昇腾910B AI处理器支持）, 数据格式支持NCHW(CHW)，
 * 支持非连续的Tensor。
 * @param [in] indices: npu device侧的aclTensor，最大值索引位置组成的tensor，数据类型支持int32，数据格式支持NCHW(CHW)，
 * 支持非连续的Tensor。
 * @param [in] kernelSize: aclIntArray类型， 表示最大池化的窗口大小。
 * @param [in] stride: aclIntArray类型， 窗口移动的步长。
 * @param [in] padding: aclIntArray类型， 每一条边补充的层数，补充的位置填写负无穷。
 * @param [in] dilation: aclIntArray类型， 控制窗口中元素的步幅。
 * @param [in] ceilMode: aclIntArray类型， 为true时用上取整的方法计算输出形状，默认向下取整。
 * @param [in] gradInput: npu device侧的aclTensor，数据类型支持float32（仅昇腾910B
 * AI处理器支持），数据格式支持NCHW(CHW)， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithIndicesBackwardGetWorkspaceSize(
    const aclTensor* gradOutput, const aclTensor* self, const aclTensor* indices, const aclIntArray* kernelSize,
    const aclIntArray* stride, const aclIntArray* padding, const aclIntArray* dilation, bool ceilMode,
    aclTensor* gradInput, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxPool2dWithIndicesBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMaxPool2dWithIndicesBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxPool2dWithIndicesBackward(void* workspace, uint64_t workspaceSize,
                                                        aclOpExecutor* executor, aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MAX_POOL2D_WITH_INDICES_BACKWARD_H_// End content from: aclnn_max_pool2d_with_indices_backward.h

// Begin content from: aclnn_foreach_zero_inplace.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ZERO_INPLACE_H_
#define ACLNN_FOREACH_ZERO_INPLACE_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachZeroInplaceGetWorkspaceSize
 * parameters :
 * x : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachZeroInplaceGetWorkspaceSize(
    const aclTensorList *x,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachZeroInplace
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachZeroInplace(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_zero_inplace.h

// Begin content from: aclnn_foreach_mul_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MUL_SCALAR_H_
#define ACLNN_FOREACH_MUL_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMulScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMulScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_mul_scalar.h

// Begin content from: aclnn_apply_adam_w_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_APPLY_ADAM_W_V2_H_
#define OP_API_INC_APPLY_ADAM_W_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnApplyAdamWV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnApplyAdamWV2GetWorkspaceSize(aclTensor* varRef, aclTensor* mRef, aclTensor* vRef,
                                                        aclTensor* maxGradNormOptionalRef, const aclTensor* grad,
                                                        const aclTensor* step, float lr, float beta1, float beta2,
                                                        float weightDecay, float eps, bool amsgrad, bool maximize,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);
/* @brief aclnnApplyAdamWV2的第二段接口，用于执行计算。 */
ACLNN_API aclnnStatus aclnnApplyAdamWV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_apply_adam_w_v2.h

// Begin content from: aclnn_slice_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SLICEV2_H_
#define OP_API_INC_LEVEL2_ACLNN_SLICEV2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSliceV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnSliceV2GetWorkspaceSize(const aclTensor* self, const aclIntArray* starts,
                                                   const aclIntArray* ends, const aclIntArray* axes,
                                                   const aclIntArray* steps, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnSliceV2的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSliceV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SLICEV2_H_
// End content from: aclnn_slice_v2.h

// Begin content from: aclnn_roi_align.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_ROI_ALIGN_H_
#define OP_API_INC_LEVEL2_ACLNN_ROI_ALIGN_H_

#include <cstring>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRoiAlign的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：ROIAlign是一种池化层，用于非均匀输入尺寸的特征图，并输出固定尺寸的特征图。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32，支持非连续的Tensor，数据格式支持NCHW。
 * @param [in] rois: npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32，支持非连续的Tensor，数据格式支持ND。
 * @param [in] batchIndices: npu device侧的aclTensor，数据类型支持INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32，和self一致，支持非连续的Tensor，数据格式支持NCHW。
 * @param [in] mode: host侧的string类型，池化模式，支持"avg"或"max"。
 * @param [in] outputHeight: host侧的int类型，ROI输出特征图的H。
 * @param [in] outputWidth: host侧的int类型，ROI输出特征图的W。
 * @param [in] samplingRatio: host侧的int类型，用于计算每个输出元素的和W上的bin数。
 * @param [in] spatialScale: host侧的float类型，缩放因子，用于将ROI坐标转换为输入特征图。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRoiAlignGetWorkspaceSize(const aclTensor* self, const aclTensor* rois,
                                                    const aclTensor* batchIndices, const char* mode, int outputHeight,
                                                    int outputWidth, int samplingRatio, float spatialScale,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRoiAlign的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnRoiAlignGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRoiAlign(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ROI_ALIGN_H_// End content from: aclnn_roi_align.h

// Begin content from: aclnn_quant_matmul_all_reduce_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_quant_matmul_all_reduce_v3.h
 * \brief
 */
#ifndef OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V3_
#define OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V3_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulAllReduceV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] biasOptional: 偏置，数据类型支持：int32。
 * @param [in] x3Optional: add操作参数，数据类型支持：float16,bfloat16。
 * @param [in] dequantScale: 去量化系数，数据类型支持：int64,uint64,bfloat16,float32。
 * @param [in] pertokenScaleOptional: per-token去量化系数，数据类型支持：float32。
 * @param [in] commQuantScale1Optional: 通信量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] commQuantScale2Optional: 通信量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] output: 计算+通信的结果，数据类型：float16,bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceV3GetWorkspaceSize(const aclTensor *x1, const aclTensor *x2,
                                                        const aclTensor *biasOptional, const aclTensor *x3Optional,
                                                        const aclTensor *dequantScale,
                                                        const aclTensor *pertokenScaleOptional, const aclTensor* commQuantScale1Optional,
                                                        const aclTensor* commQuantScale2Optional, const char* group,
                                                        const char *reduceOp, int64_t commTurn,
                                                        int64_t streamMode, const aclTensor *output,
                                                        uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnQuantMatmulAllReduceV3的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnQuantMatmulAllReduceV3GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceV3(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V3_// End content from: aclnn_quant_matmul_all_reduce_v3.h

// Begin content from: aclnn_grid_sampler2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GRID_SAMPLER2D_BACKWARD_H_
#define OP_API_INC_GRID_SAMPLER2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGridSampler2DBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：aclnnGridSampler2D的反向计算。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous]) --> C([l0op::GridSampler2DGrad])
 *     D[(input)] --> E([l0op::Contiguous]) --> C
 *     F[(grid)] --> G([l0op::Contiguous]) --> C
 *     H((interpolationMode)) --> C
 *     I((paddingMode)) --> C
 *     J((alignCorners)) --> C
 *     C --> K([l0op::ViewCopy]) --> Out1[("inputGrad")]
 *     C --> L([l0op::ViewCopy]) --> Out2[("gridGrad")]
 * ```
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] input: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] grid: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] interpolationMode：int64_t类型，表示插值模式，0：bilinear（双线性插值），1：nearest（最邻近插值）。
 * @param [in] paddingMode：int64_t类型，表示填充模式，即当（x,y）取值超过输入特征图采样范围时，返回一个特定值，
 * 有0：zeros、1：border、2：reflection三种模式。
 * @param [in] alignCorners：bool类型，表示设定特征图坐标与特征值的对应方式，设定为true时，特征值位于像素中心。
 * @param [in] outputMask：aclBoolArray类型，输出的掩码。
 * @param [in] inputGrad: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gridGrad: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGridSampler2DBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* input,
                                                                 const aclTensor* grid, int64_t interpolationMode,
                                                                 int64_t paddingMode, bool alignCorners,
                                                                 const aclBoolArray* outputMask, aclTensor* inputGrad,
                                                                 aclTensor* gridGrad, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief aclnnGridSampler2DBackward的第二段接口，用于执行计算。
 *
 * 算子功能：aclnnGridSampler2D的反向计算。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous]) --> C([l0op::GridSampler2DGrad])
 *     D[(input)] --> E([l0op::Contiguous]) --> C
 *     F[(grid)] --> G([l0op::Contiguous]) --> C
 *     H((interpolationMode)) --> C
 *     I((paddingMode)) --> C
 *     J((alignCorners)) --> C
 *     C --> K([l0op::ViewCopy]) --> Out1[("inputGrad")]
 *     C --> L([l0op::ViewCopy]) --> Out2[("gridGrad")]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGridSampler2DBackwardGetWorkspaceSize
 * 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGridSampler2DBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GRID_SAMPLER2D_BACKWARD_H_// End content from: aclnn_grid_sampler2d_backward.h

// Begin content from: aclnn_all_to_all_all_gather_batch_matmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023-2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ALL_TO_ALL_ALL_GATHER_BATCH_MATMUL_
#define OP_API_INC_ALL_TO_ALL_ALL_GATHER_BATCH_MATMUL_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现alltoall + allGather + bmm 融合计算
 * @brief aclnnAlltoAllAllGatherBatchMatMul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] x: 计算输入，Tensor，数据类型支持float16，bfloat16。该输入进行AllToAll、AllGather集合通信，必须为3维，数据格式支持ND，通信后结果作为BatchMatMul计算的左矩阵
 * @param [in] weight: 计算输入，Tensor，数据类型支持float16, bfloat16，类型需与x保持一致，必须为3维，数据格式支持ND，BatchMatMul计算的右矩阵
 * @param [in] biasOptional: 计算输入，Tensor，数据类型支持float16, float32。x为float16时，bias需为float16；x为bfloat16时，bias需为float32，必须为两维或三维，数据格式支持ND，BatchMatMul计算的bias
 * @param [in] groupEp: 计算输入，str。ep通信域名称，专家并行的通信域
 * @param [in] groupTp: 计算输入，str。tp通信域名称，Tensor并行的通信域
 * @param [in] epWorldSize: 计算输入，int。ep通信域size，支持2/4/8/16
 * @param [in] tpWorldSize: 计算输入，int。tp通信域size，支持2/4/8/16
 * @param [in] xShardType: 计算输入，int，默认值为0，0表示在H维度按tp域进行allgather，1表示在C维度上按tp域进行allgather。当前仅支持shard_type等于1的场景
 * @param [in] actType: 计算输入，int，激活函数类型，默认值为0，表示无激活函数。支持传入0/1/2/3/4, 对应关系为[0: None, 1: GELU, 2: Silu, 3: Relu, 4: FastGELU]等
 * @param [out] y1Out: 计算输出，Tensor，数据类型支持float16, bfloat16，仅支持3维。最终计算结果，如果有激活函数则为激活函数的输出，否则为BatchMatMul的输出。数据类型与输入x保持一致
 * @param [out] y2OutOptional: 计算输出，Tensor，可选输出，数据类型支持float16, bfloat16，仅支持3维。AllGather的输出，数据类型与输入x保持一致。反向可能需要仅allgather通信操作的结果，数据类型：同输入。
 * @param [out] y3OutOptional: 计算输出，Tensor，可选输出，数据类型支持float16, bfloat16，仅支持3维。有激活函数时，BatchMatMul的输出，类型与输入x保持一致ctType非0时仅bmm计算的结果，数据类型：同输入。
 * @param [out] workspaceSize: 出参，返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 出参，返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回值，返回状态码
 *
 * 因为集合通信及BatchMatMul计算所需，输入输出shape需满足以下数学关系：（其中ep=epWorldSize，tp=tpWorldSize
 * x: (E, C/tp, H)；
 * weight：(E/ep, H, M/tp)；
 * bias：(E/ep, 1, M/tp)； 支持两维或三维，两维时shape为：(E/ep, M/tp)
 * y1：(E/ep, ep * tp * C/tp, M/tp)；
 * y2：(E/ep, ep * tp * C/tp, H)；
 * y3：(E/ep, ep * tp * C/tp, M/tp)
 * 数据关系说明：
 * 比如x.size(0)等于E，weight.size(0)等于E/ep，则表示，x.size(0) = ep * weight.size(0)，x.size(0)是ep的整数倍；其他关系类似
 * E的取值范围为[2, 512]，且E是ep的整数倍；
 * H的取值范围为：[1, 65535]；
 * M/tp的取值范围为：[1, 65535]；
 * E/ep的取值范围为：[1, 32]；
 * ep、tp均仅支持2、4、8、16；
 * groupEp和groupTp名称不能相同；
 * C大于0，上限为算子device内存上限；
 * 不支持跨超节点，只支持超节点内，ep域AlltoAll支持超节点内跨节点，tp域AllGather仅支持超节点内单一节点。
 */
ACLNN_API aclnnStatus aclnnAlltoAllAllGatherBatchMatMulGetWorkspaceSize(const aclTensor* x, const aclTensor* weight,
                                                                        const aclTensor* biasOptional, 
                                                                        const char* groupEp, const char* groupTp,
                                                                        int64_t epWorldSize, int64_t tpWorldSize,
                                                                        int64_t xShardType, int64_t actType,
                                                                        aclTensor* y1Out, aclTensor* y2OutOptional,
                                                                        aclTensor* y3OutOptional,
                                                                        uint64_t* workspaceSize,
                                                                        aclOpExecutor** executor);

/**
 * @brief aclnnAlltoAllAllGatherBatchMatMul的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAlltoAllAllGatherBatchMatMulGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAlltoAllAllGatherBatchMatMul(void* workspace, uint64_t workspaceSize,
                                                        aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ALL_TO_ALL_ALL_GATHER_BATCH_MATMUL_// End content from: aclnn_all_to_all_all_gather_batch_matmul.h

// Begin content from: aclnn_eq_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_EQTENSOR_H_
#define OP_API_INC_EQTENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEqTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 计算两个Tensor中的元素是否相等，返回一个Tensor，self=other的为True(1.)，否则为False(0.)：
 *
 * $$ out = (self == other)  ?  [True] : [False] $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *    A[(Self)] -->B([l0op::Contiguous])
 *    B -->C1([l0op::Cast])-->D([l0op::Equal])
 *    E[(other)] -->F([l0op::Contiguous])
 *    F --> C2([l0op::Cast])-->D
 *    D --> C3([l0op::Cast])-->F1([l0op::ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,BFLOAT16,
 * DOUBLE,INT16,COMPLEX64,COMPLEX128数据类型，shape需要与other满足broadcast关系，
 * 数据类型需要与other满足数据类型推导规则，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,BFLOAT16,
 * DOUBLE,INT16,COMPLEX64,COMPLEX128数据类型，shape需要与self满足broadcast关系，
 * 数据类型需要与self满足数据类型推导规则，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,BFLOAT16,
 * DOUBLE,INT16,COMPLEX64,COMPLEX128，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnEqTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEqTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceEqTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算selfRef中元素的值与other的值是否相等，将selfRef每个元素与other的值的比较结果原地返回到selfRef中；
 *
 * 计算公式：$$ selfRef = (selfRef == other)  ?  [True] : [False] $$
 *
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,BFLOAT16(仅1971支持),DOUBLE,INT16,COMPLEX64,COMPLEX128，
 * 数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,BFLOAT16(仅1971支持) ,DOUBLE,INT16,COMPLEX64,COMPLEX128，
 * 数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceEqTensorGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceEqTensor的第二段接口，用于执行计算。
 *
 * 算子功能：计算selfRef中元素的值与other的值是否相等，将selfRef每个元素与other的值的比较结果原地返回到selfRef中；
 *
 * 计算公式：$$ selfRef = (selfRef == other)  ?  [True] : [False] $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceEqTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EQTENSOR_H_// End content from: aclnn_eq_tensor.h

// Begin content from: aclnn_trunc.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRUNC_H_
#define OP_API_INC_TRUNC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTrunc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成trunc操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持FLOAT、BFLOAT16、FLOAT16，shape与out保持相同，
 *  支持非连续的Tensor，数据格式支持ND，维度小于8。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、BFLOAT16、FLOAT16, shape与self保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnTruncGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnTrunc的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成trunc操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTruncGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTrunc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceTrunc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成trunc操作
 * @param [in] selfRef: npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16，支持非连续的Tensor，数据格式支持ND，维度小于8。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceTruncGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceTrunc的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成trunc操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTruncGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTrunc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRUNC_H_// End content from: aclnn_trunc.h

// Begin content from: aclnn_le_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_LE_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_LE_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLeScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 计算self中的元素的值是否小于等于other的值，将self每个元素与other的值的比较结果写入out中
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE。数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: host侧的aclScalar，数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE
 * 数据类型需要能转换成self的数据类型
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE
 * shape与self的shape一致，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLeScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLeScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceLeScalarGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：计算selfRef Tensor中的元素是否小于等于other
 * Scalar中的元素，返回修改后的selfRef，selfRef<=other的为True，否则为False
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,INT32,UINT8,BOOL,UINT64,UINT32,DOUBLE,UINT16，BFLOAT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，shape需要与other满足broadcast关系。
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLeScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLeScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_LE_SCALAR_H_
// End content from: aclnn_le_scalar.h

// Begin content from: aclnn_deep_norm_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_DEEP_NORM_GRAD_H_
#define ACLNN_DEEP_NORM_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnDeepNormGradGetWorkspaceSize
 * parameters :
 * dy : required
 * x : required
 * gx : required
 * gamma : required
 * mean : required
 * rstd : required
 * alpha : optional
 * dxOut : required
 * dgxOut : required
 * dbetaOut : required
 * dgammaOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnDeepNormGradGetWorkspaceSize(
    const aclTensor *dy,
    const aclTensor *x,
    const aclTensor *gx,
    const aclTensor *gamma,
    const aclTensor *mean,
    const aclTensor *rstd,
    double alpha,
    const aclTensor *dxOut,
    const aclTensor *dgxOut,
    const aclTensor *dbetaOut,
    const aclTensor *dgammaOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnDeepNormGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnDeepNormGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_deep_norm_grad.h

// Begin content from: aclnn_stack.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_STACK_H_
#define OP_API_INC_STACK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnStack的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnStackGetWorkspaceSize(const aclTensorList* tensors, int64_t dim, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnStack的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnStack(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_STACK_H_
// End content from: aclnn_stack.h

// Begin content from: aclnn_bincount.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BINCOUNT_H_
#define OP_API_INC_BINCOUNT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBincount 的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算非负整数数组中每个数的频率。
 * 计算公式：
 * 如果n是self在位置i上的值，如果指定了weights，则
 * out[n] = out[n] + weights[i]
 * 否则
 * out[n] = out[n] + 1
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持INT8、INT16、INT32、INT64、UINT8，
 * 且必须是非负整数，数据格式支持1维ND。支持非连续的Tensor。
 * @param [in] weights:  npu device侧的aclTensor，self每个值的权重，可为空指针。
 * 数据类型支持FLOAT、FLOAT16、FLOAT64、INT8、INT16、INT32、INT64、UINT8、BOOL，
 * 数据格式支持1维ND，且shape必须与self一致。支持非连续的Tensor。
 * @param [in] minlength: host侧的int型，指定输出tensor最小长度。如果计算出来的size的最大值小于minlength，
 * 则输出长度为minlength，否则为size。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持INT32、INT64、FLOAT、DOUBLE。数据格式支持1维ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBincountGetWorkspaceSize(const aclTensor* self, const aclTensor* weights, int64_t minlength,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBincount的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：计算非负整数数组中每个数的频率。
 * 计算公式：
 * 如果n是self在位置i上的值，如果指定了weights，则
 * out[n] = out[n] + weights[i]
 * 否则
 * out[n] = out[n] + 1

 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnBincountGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBincount(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BINCOUNT_H_// End content from: aclnn_bincount.h

// Begin content from: aclnn_foreach_addcmul_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCMUL_SCALAR_LIST_H_
#define ACLNN_FOREACH_ADDCMUL_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcmulScalarListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulScalarListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcmulScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcmul_scalar_list.h

// Begin content from: aclnn_upsample_bicubic_2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023 All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_BICUBIC_2D_BACKWARD_H_
#define OP_API_INC_UNAMPLE_BICUBIC_2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBicubic2dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     aclrtStream stream);

/**
 * @brief aclnnUpsampleBicubic2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2dBackwardGetWorkspaceSize(
    const aclTensor* gradOut, const aclIntArray* outputSize, const aclIntArray* inputSize, const bool alignCorners,
    double scalesH, double scalesW, aclTensor* gradInput, uint64_t* workspaceSize, aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_Bicubic_2D_BACKWARD_H_// End content from: aclnn_upsample_bicubic_2d_backward.h

// Begin content from: aclnn_bitwise_and_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BITWISE_AND_SCALAR_H_
#define OP_API_INC_BITWISE_AND_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBitwiseAndScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，
 * 且数据类型需要与other构成互相推导关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other:
 * host侧的aclScalar，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与self构成互相推导关系。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，
 * 且数据类型需要是self与other推导之后可转换的数据类型，shape与self一致，数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseAndScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseAndScalar的第二段接口，用于执行计算。
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnBitwiseAndScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseAndScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseAndScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，
 * 且数据类型需要与other构成互相推导关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other:
 * host侧的aclScalar，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与self构成互相推导关系。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseAndScalarGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* other,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseAndScalar的第二段接口，用于执行计算。
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceBitwiseAndScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseAndScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BITWISE_AND_SCALAR_H_
// End content from: aclnn_bitwise_and_scalar.h

// Begin content from: aclnn_batch_norm_elemt.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_BATCH_NORM_ELEMT_H_
#define OP_API_INC_BATCH_NORM_ELEMT_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormElemt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormElemtGetWorkspaceSize(const aclTensor* input, const aclTensor* weight,
                                                          const aclTensor* bias, aclTensor* mean, aclTensor* invstd,
                                                          double eps, aclTensor* output, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormElemt的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormElemt(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_batch_norm_elemt.h

// Begin content from: aclnn_grouped_mat_mul_all_reduce.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_MATMUL_ALL_REDUCE_H_
#define OP_API_INC_GROUPED_MATMUL_ALL_REDUCE_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupedMatMulAllReduce的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnnop_ops_infer
 * 算子功能：实现 gmm + AllReduce 融合计算
 * @param [in] x: 表示公式中的x，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] weight:
 * 表示公式中的weight，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] bias: 表示公式中的bias，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] groupListOptional:
 * 可选参数，代表输入和输出M方向的matmul大小分布，数据类型支持INT64，支持的最大长度为128个。
 * @param [in] splitItemOptional:
 * 可选参数，代表输入和输出是否要做tensor切分，1代表输入需要切分，输出不需要切分；2代表输入不需要切分，
 * 输出需要切分；3代表输入和输出都需要切分。
 * @param [in] group: 可选参数，标识列组的字符串。
 * @param [in] reduceOp: 可选参数，reduce操作类型，默认值：sum。
 * @param [in] commTurn: 可选参数，通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: 可选参数，acl流模式的枚举，类型支持：0/1.
 * @param [out] out: 表示公式中的out，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatMulAllReduceGetWorkspaceSize(
    const aclTensorList* x, const aclTensorList* weight, const aclTensorList* bias,
    const aclIntArray* groupListOptional, int64_t splitItem, const char* group, const char* reduceOp, int64_t commTurn,
    int64_t streamMode, const aclTensorList* y, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGroupedMatMulAllReduce的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatMulAllReduce(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif
#endif  // OP_API_INC_GROUPED_MATMUL_ALL_REDUCE_H_// End content from: aclnn_grouped_mat_mul_all_reduce.h

// Begin content from: aclnn_linalg_qr.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LINALG_QR_H_
#define OP_API_INC_LINALG_QR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLinalgQr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成QR操作
 * @param [in] self: 计算输入, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128, 数据格式支持ND,
 * 支持非连续的Tensor
 * @param [in] mode: 计算输入, 指定输出的tensor shape是否为complete, 数据类型支持int64_t
 * @param [out] Q: 计算输出,
 * Qr分解后得到的正交矩阵，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，且与self相同 数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [out] R: 计算输出,
 * Qr分解后得到的上三角矩阵，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，且与self相同 数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLinalgQrGetWorkspaceSize(const aclTensor* self, int64_t mode, aclTensor* Q, aclTensor* R,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnLinalgQr的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成Qr 操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLinalgQrGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLinalgQr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LINALG_QR_H_
// End content from: aclnn_linalg_qr.h

// Begin content from: aclnn_chamfer_distance_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CHAMFER_DISTANCE_BACKWARD_H_
#define OP_API_INC_CHAMFER_DISTANCE_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnChamferDistanceBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnChamferDistanceBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

/**
 * @brief aclnnChamferDistanceBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnChamferDistanceBackwardGetWorkspaceSize(const aclTensor* xyz1, const aclTensor* xyz2,
                                                                   const aclTensor* idx1, const aclTensor* idx2,
                                                                   const aclTensor* gradDist1,
                                                                   const aclTensor* gradDist2, aclTensor* gradXyz1,
                                                                   aclTensor* gradXyz2, uint64_t* workspaceSize,
                                                                   aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_chamfer_distance_backward.h

// Begin content from: aclnn_cumsum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CUMSUM_H_
#define OP_API_INC_LEVEL2_ACLNN_CUMSUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：对输入张量self的元素，按照指定维度dim依次进行累加，并将结果保存到输出张量out中
 * 计算公式：
 * $x_{i}$是输入张量self中，从维度dim视角来看的某个元素（其它维度下标不变，只dim维度下标依次递增），
 * $y_{i}$是输出张量out中对应位置的元素，则
 *
 * $$
 * y_{i} = x_{1} + x_{2} + x_{3} + ...... + x_{i}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous])
 *     B --> C([l0op::Cast])
 *     C --> D([l0op::Cumsum])
 *     E((dim)) --> D
 *     F((dtype)) --> C
 *     D --> G([l0op::ViewCopy])
 *     G --> H[(out)]
 * ```
 */

/**
 * @brief aclnnCumsum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128、UINT8、INT8、INT16、INT32、
 * INT64、FLOAT16、BFLOAT16、BOOL，数据类型需要能转换成out的数据类型，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] dim: host侧的整数，数据类型支持INT64。
 * @param [in]
 * dtype：host侧的数据类型枚举，表示输出张量所需的数据类型，支持FLOAT、FLOAT16、INT32、DOUBLE、UINT8、INT8、INT16、
 * INT64、COMPLEX64、COMPLEX128、BFLOAT16（910B支持），且需与输出张量out的数据类型一致。
 * @param [in] out：npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT32、DOUBLE、UINT8、INT8、INT16、INT64、
 * COMPLEX64、COMPLEX128、BFLOAT16（910B支持），且数据类型需要与传入的dtype一致，shape需要与self一致，支持非连续的Tensor，数据
 * 格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCumsumGetWorkspaceSize(const aclTensor* self, int64_t dim, aclDataType dtype, aclTensor* out,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnCumsum的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCumsumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCumsum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnCumsumV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnCumsumV2GetWorkspaceSize(const aclTensor* self, int64_t dim, bool exclusive, bool reverse,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnCumsumV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CUMSUM_H_
// End content from: aclnn_cumsum.h

// Begin content from: aclnn_foreach_exp.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_EXP_H_
#define ACLNN_FOREACH_EXP_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachExpGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachExpGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachExp
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachExp(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_exp.h

// Begin content from: aclnn_foreach_copy.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_COPY_H_
#define ACLNN_FOREACH_COPY_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachCopyGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCopyGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachCopy
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCopy(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_copy.h

// Begin content from: aclnn_foreach_neg.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_NEG_H_
#define ACLNN_FOREACH_NEG_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachNegGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachNegGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachNeg
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachNeg(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_neg.h

// Begin content from: aclnn_embedding.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_EMBEDDING_H_
#define OP_API_INC_EMBEDDING_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEmbedding的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：把数据集合映射到向量空间，进而将数据进行量化
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(weight)] --> B([l0op::Contiguous])
 *     B --> C([l0op::GatherV2])
 *     D[(indices)] --> E([l0op::Contiguous]) --> C
 *     F((dim=0)) --> G[(dimTensor)] --> C
 *     C --> H([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] weight: npu
 * device侧的aclTensor，数据类型支持BFLOAT16,
 * FLOAT、FLOAT16、INT64、INT32、INT16、INT8、UINT8、BOOL、DOUBLE、COMPLEX64、
 * COMPLEX128，支持非连续Tensor，数据格式支持ND。
 * @param [in] indices: npu
 * device侧的aclTensor，数据类型支持INT64、INT32，支持非连续Tensor，数据格式支持ND。
 * @param [out] out: npu
 * device侧的aclTensor，数据类型同weight，数据格式支持ND，比indices的维度多1。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingGetWorkspaceSize(const aclTensor* weight, const aclTensor* indices,
                                                     const aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnEmbedding的第二段接口，用于执行计算。
 *
 * * 算子功能：把数据集合映射到向量空间，进而将数据进行量化
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(weight)] --> B([l0op::Contiguous])
 *     B --> C([l0op::GatherV2])
 *     D[(indices)] --> E([l0op::Contiguous]) --> C
 *     F((dim=0)) --> G[(dimTensor)] --> C
 *     C --> H([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnEmbeddingGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbedding(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EMBEDDING_H_// End content from: aclnn_embedding.h

// Begin content from: aclnn_foreach_cos.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_COS_H_
#define ACLNN_FOREACH_COS_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachCosGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCosGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachCos
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCos(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_cos.h

// Begin content from: aclnn_upsample_nearest_2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_2D_BACKWARD_H_
#define OP_API_INC_UNAMPLE_NEAREST_2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：upsample_nearest2d的反向计算。
 *
 * @param [in] gradOut: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持NCHW和NHWC（[参考](#参考)）。
 * @param [in] outputSize: 输入IntArray，size大小为2。表示输入gradOut在H和W维度上的空间大小。
 * @param [in] inputSize: 输入IntArray，size大小为4。表示输出out分别在N、C、H和W维度上的空间大小。
 * @param [in] scalesH: double常量，表示输出out的height维度乘数。
 * @param [in] scalesW: double常量，表示输出out的width维度乘数。
 * @param [out] gradInput: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，且数据类型与gradOut的数据类型一致。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持NCHW和NHWC（[参考](#参考)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest2dBackwardGetWorkspaceSize(const aclTensor* gradOut,
                                                                     const aclIntArray* outputSize,
                                                                     const aclIntArray* inputSize, double scalesH,
                                                                     double scalesW, aclTensor* gradInput,
                                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest2dBackward的第二段接口，用于执行计算。
 *
 * 算子功能：upsample_nearest2d的反向计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，
 * 由第一段接口aclnnUpsampleNearest2dBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_2D_BACKWARD_H_
// End content from: aclnn_upsample_nearest_2d_backward.h

// Begin content from: aclnn_add_layer_norm_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_ADD_LAYER_NORM_GRAD_H_
#define ACLNN_ADD_LAYER_NORM_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnAddLayerNormGradGetWorkspaceSize
 * parameters :
 * dy : required
 * x1 : required
 * x2 : required
 * rstd : required
 * mean : required
 * gamma : required
 * dsumOptional : optional
 * dxOut : required
 * dgammaOut : required
 * dbetaOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddLayerNormGradGetWorkspaceSize(
    const aclTensor *dy,
    const aclTensor *x1,
    const aclTensor *x2,
    const aclTensor *rstd,
    const aclTensor *mean,
    const aclTensor *gamma,
    const aclTensor *dsumOptional,
    const aclTensor *dxOut,
    const aclTensor *dgammaOut,
    const aclTensor *dbetaOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnAddLayerNormGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddLayerNormGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_add_layer_norm_grad.h

// Begin content from: aclnn_ne_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_NE_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_NE_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNeTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNeTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNeTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceNeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。selfRef也是输出结果。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNeTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceNeTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceNeTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_NE_TENSOR_H_// End content from: aclnn_ne_tensor.h

// Begin content from: aclnn_logical_or.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LogicalOr_H_
#define OP_API_INC_LogicalOr_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogicalOr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成逻辑或计算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalOr])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalOr])
 * D([LogicalOr])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL,
 * shape需要与self满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * shape为self与other两者broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalOrGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogicalOr的第二段接口，用于执行计算。
 *
 * 算子功能：完成逻辑或计算
 *
 * 实现说明：
 * api计算的基本路径:
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalOr])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalOr])
 * D([LogicalOr])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalOrGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalOr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnInplaceLogicalOr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128、BFLOAT16（910B支持），shape需要与other满足broadcast关系，且broadcast之后的shape需要与selfRef
 * 自身的shape相同。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX64、COMPLEX128、BFLOAT16（910B支持），shape需要与selfRef满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalOrGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLogicalOr的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLogicalOrGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalOr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LogicalOr_H_
// End content from: aclnn_logical_or.h

// Begin content from: aclnn_split_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SPLIT_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_SPLIT_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSplitTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、BFLOAT16、INT32、INT64、INT16、INT8、
 * UINT8、BOOL、COMPLEX128和COMPLEX64。支持非连续的Tensor，数据格式支持ND。
 * @param [in] splitSections: host侧的整型数值, 数据类型为UINT64。表示沿dim轴均匀切分后的块大小,
 * 最后一块可以小于splitSections。
 * @param [in] dim: host侧的整型数值，数据类型为INT64，表示输入tensor被split的维度。
 * @param [in] out: npu device侧的aclTensorList，表示被split后的输出tensor的列表，数据类型支持FLOAT、FLOAT16、DOUBLE、
 * BFLOAT16、INT32、INT64、INT16、INT8、UINT8、BOOL、COMPLEX128和COMPLEX64。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSplitTensorGetWorkspaceSize(const aclTensor* self, uint64_t splitSections, int64_t dim,
                                                       aclTensorList* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnSplitTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSplitTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSplitTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SPLIT_TENSOR_H_
// End content from: aclnn_split_tensor.h

// Begin content from: aclnn_log10.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG10_H_
#define OP_API_INC_LOG10_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLog10的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成输入以10为底对数的计算
 * 计算公式：
 * $$ output_i=log_{10}(self_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B -->C([l0op::Cast])
 *     C -->D([l0op::Log])
 *     D -->E([l0op::Cast])
 *     E -->I([l0op::ViewCopy])
 *     I -->J[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、FLOAT、FLOAT16、BFLOAT16。
 *                   支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。且数据类型是self可转化的数据类型。
 *                  数据shape与self一致。数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog10GetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnLog10的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLog10GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog10(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceLog10的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成输入以10为底对数的计算，在原Tensor上进行更新
 * 计算公式：
 * $$ output_i=log_{10}(self_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B -->C([l0op::Cast])
 *     C -->D([l0op::Log])
 *     D -->E([l0op::Cast])
 *     E -->I([l0op::ViewCopy])
 *     I -->J[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、FLOAT、FLOAT16、BFLOAT16。
 *                   支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog10GetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLog10的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLog10GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog10(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG10_H_
// End content from: aclnn_log10.h

// Begin content from: aclnn_global_max_pool.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GLOBAL_MAX_POOL_H_
#define OP_API_INC_GLOBAL_MAX_POOL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGlobalMaxPool的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：输入一个张量，并对同一通道中的值取最大值
 *
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、DOUBLE。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] out:
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、DOUBLE。支持连续和非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGlobalMaxPoolGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnGlobalMaxPool的第二段接口，用于执行计算。
 *
 * 算子功能：输入一个张量，并对同一通道中的值取最大值
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGlobalMaxPoolGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGlobalMaxPool(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GLOBAL_MAX_POOL_H_// End content from: aclnn_global_max_pool.h

// Begin content from: aclnn_max_pool.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_MAX_POOL_H_
#define OP_API_INC_LEVEL2_ACLNN_MAX_POOL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxPool的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMaxPoolGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelShape,
                                                   const aclIntArray* strides, const int64_t autoPad,
                                                   const aclIntArray* pads, const aclIntArray* dilations,
                                                   const int64_t ceilMode, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnMaxPool的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMaxPool(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MAX_POOL_H_// End content from: aclnn_max_pool.h

// Begin content from: aclnn_upsample_linear_1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_LINEAR_H_
#define OP_API_INC_UNAMPLE_LINEAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleLinear1d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleLinear1d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnUpsampleLinear1d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleLinear1dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                            const bool alignCorners, const double scales,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_LINEAR_H_// End content from: aclnn_upsample_linear_1d.h

// Begin content from: aclnn_sinh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SINH_H_
#define OP_API_INC_SINH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSinh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sinh操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、FLOAT16、
 *  BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128,
 * shape与self 保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSinhGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief: aclnnSinh的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sinh操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSinh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceSinh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sinh操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、
 *  FLOAT16、BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceSinhGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceSinh的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成sinh操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSinh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SINH_H_// End content from: aclnn_sinh.h

// Begin content from: aclnn_foreach_sub_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SUB_LIST_H_
#define ACLNN_FOREACH_SUB_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSubListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * alpha : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensor *alpha,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSubList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sub_list.h

// Begin content from: aclnn_foreach_sin.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SIN_H_
#define ACLNN_FOREACH_SIN_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSinGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSinGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSin
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSin(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sin.h

// Begin content from: aclnn_upsample_nearest_3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_3D_H_
#define OP_API_INC_UNAMPLE_NEAREST_3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             double scalesD, double scalesH, double scalesW,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest3d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_3D_H_
// End content from: aclnn_upsample_nearest_3d.h

// Begin content from: aclnn_hardsigmoid_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDSIGMOID_BACKWARD_H_
#define OP_API_INC_HARDSIGMOID_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardsigmoidBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：激活函数hardsigmoid的反向计算。
 *
 * @param [in] gradOutput(aclTensor*, 计算输入): 数据类型支持FLOAT、FLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] self(aclTensor*, 计算输入): 数据类型支持FLOAT、FLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [out] out(aclTensor*, 计算输出): 数据类型支持FLOAT、FLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size(uint64_t*, 出参): 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor(aclOpExecutor**, 出参): 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardsigmoidBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                               aclTensor* out, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);

/**
 * @brief aclnnHardsigmoidBackward的第二段接口，执行计算。
 *
 * 算子功能：激活函数hardsigmoid的反向计算。
 *
 * @param [in] workspace(void*, 入参): 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize(uint64_t, 入参): 在npu
 * device侧申请的workspace大小，由第一段接口aclnnHardsigmoidBackwardGetWorkspaceSize获取。
 * @param [in] executor(aclOpExecutor*, 入参): op执行器，包含了算子计算流程。
 * @param [in] stream(aclrtStream, 入参): acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardsigmoidBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSIGMOID_BACKWARD_H_
// End content from: aclnn_hardsigmoid_backward.h

// Begin content from: aclnn_moe_gating_top_k_softmax_v2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_GATING_TOP_KSOFTMAX_V2_H_
#define ACLNN_MOE_GATING_TOP_KSOFTMAX_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeGatingTopKSoftmaxV2GetWorkspaceSize
 * parameters :
 * x : required
 * finishedOptional : optional
 * k : required
 * renorm : optional
 * outputSoftmaxResultFlag : optional
 * yOut : required
 * expertIdxOut : required
 * softmaxResultOutOptional : optional
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeGatingTopKSoftmaxV2GetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *finishedOptional,
    int64_t k,
    int64_t renorm,
    bool outputSoftmaxResultFlag,
    const aclTensor *yOut,
    const aclTensor *expertIdxOut,
    const aclTensor *softmaxResultOutOptional,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeGatingTopKSoftmaxV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeGatingTopKSoftmaxV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_gating_top_k_softmax_v2.h

// Begin content from: aclnn_mse_loss_out.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MSE_LOSS_OUT_H_
#define OP_API_INC_MSE_LOSS_OUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMseLossOut的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与target满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] target: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出将被reduce 0轴求均值，'sum' 表示输出将被reduce 0轴求和。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLossOutGetWorkspaceSize(const aclTensor* self, const aclTensor* target, int64_t reduction,
                                                      aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnMseLossOut的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnMseLossOutGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLossOut(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MSE_LOSS_OUT_H_
// End content from: aclnn_mse_loss_out.h

// Begin content from: aclnn_moe_gating_top_k_softmax.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_GATING_TOP_KSOFTMAX_H_
#define ACLNN_MOE_GATING_TOP_KSOFTMAX_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeGatingTopKSoftmaxGetWorkspaceSize
 * parameters :
 * x : required
 * finishedOptional : optional
 * k : required
 * yOut : required
 * expertIdxOut : required
 * rowIdxOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeGatingTopKSoftmaxGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *finishedOptional,
    int64_t k,
    const aclTensor *yOut,
    const aclTensor *expertIdxOut,
    const aclTensor *rowIdxOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeGatingTopKSoftmax
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeGatingTopKSoftmax(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_gating_top_k_softmax.h

// Begin content from: aclnn_addcdiv.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADDCDIV_H_
#define OP_API_INC_ADDCDIV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddcdiv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：执行 tensor1 除以 tensor2 的元素除法，将结果乘以标量 value 并将其添加到 input
 * 计算公式：
 * $$ out_i = self_i + value \times {tensor1_i \over tensor2_i} $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 graph LR
 *  A[(self)] -->B([l0op::Contiguous])
 *  B --> K([l0op::Cast])
 *  K --> C([l0op::Addcdiv])
 *  D[(tensor1)] -->E([l0op::Contiguous])
 *  E --> L([l0op::Cast])
 *  L --> C
 *  F[(tensor2)] --> G([l0op::Contiguous])
 *  G --> M([l0op::Cast])
 *  M --> C
 *  H((value)) --> C
 *  C --> O([l0op::Cast])
 *  O --> I([l0op::ViewCopy])
 *  I --> J[(out)]
 * ```
 *
 * @param [in] self: npu
 * 需要累加的张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 *
 数据类型要和tensor1、tensor2支持可推导关系，shape要与tensor1、tensor2除过之后的tensor满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] tensor1: npu
 * 分子张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 * 数据类型要和self、tensor2支持可推导关系，shape要与tensor2满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] tensor2: npu
 * 分母张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 * 数据类型要和self、tensor1支持可推导关系，shape要与tensor1满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] value: host侧的aclScalar，数据类型与self、tensor1、tensor2数据类型保持一致，不一致时需要强转成一致。
 * @param [in] out: npu
 * 输出张量，npu device侧的aclTensor，数据类型支持DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 * 数据类型要满足和self、tensor1、tensor2推导关系，shape是self、tensor1、tensor2
 broadcast之后的shape，支持非连续Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddcdivGetWorkspaceSize(const aclTensor* self, const aclTensor* tensor1,
                                                   const aclTensor* tensor2, const aclScalar* value,
                                                   const aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnAddcdiv的第二段接口，用于执行计算。
 *
 * * 算子功能：执行 tensor1 除以 tensor2 的元素除法，将结果乘以标量 value 并将其添加到 input
 * 计算公式：
 * $$ out_i = self_i + value \times {tensor1_i \over tensor2_i} $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 graph LR
 *  A[(self)] -->B([l0op::Contiguous])
 *  B --> K([l0op::Cast])
 *  K --> C([l0op::Addcdiv])
 *  D[(tensor1)] -->E([l0op::Contiguous])
 *  E --> L([l0op::Cast])
 *  L --> C
 *  F[(tensor2)] --> G([l0op::Contiguous])
 *  G --> M([l0op::Cast])
 *  M --> C
 *  H((value)) --> C
 *  C --> O([l0op::Cast])
 *  O --> I([l0op::ViewCopy])
 *  I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddcdivGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddcdiv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   const aclrtStream stream);

/**
 * @brief aclnnInplaceAddcdiv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：执行 tensor1 除以 tensor2 的元素除法，将结果乘以标量 value 并将其添加到 input
 * 计算公式：
 * $$ out_i = self_i + value \times {tensor1_i \over tensor2_i} $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 graph LR
 *  A[(self)] -->B([l0op::Contiguous])
 *  B --> K([l0op::Cast])
 *  K --> C([l0op::Addcdiv])
 *  D[(tensor1)] -->E([l0op::Contiguous])
 *  E --> L([l0op::Cast])
 *  L --> C
 *  F[(tensor2)] --> G([l0op::Contiguous])
 *  G --> M([l0op::Cast])
 *  M --> C
 *  H((value)) --> C
 *  C --> O([l0op::Cast])
 *  O --> I([l0op::ViewCopy])
 *  I --> J[(out)]
 * ```
 *
 * @param [in] selfRef: npu
 * 需要累加的张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 *
 数据类型要和tensor1、tensor2支持可推导关系，shape要与tensor1、tensor2除过之后的tensor满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] tensor1: npu
 * 分子张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 * 数据类型要和self、tensor2支持可推导关系，shape要与tensor2满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] tensor2: npu
 * 分母张量，npu device侧的aclTensor，数据类型支持DT_BFLOAT16、DT_FLOAT16、DT_FLOAT、DT_DOUBLE、DT_INT64，
 * 数据类型要和self、tensor1支持可推导关系，shape要与tensor1满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] value: host侧的aclScalar，数据类型与self、tensor1、tensor2数据类型保持一致，不一致时需要强转成一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddcdivGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* tensor1,
                                                          const aclTensor* tensor2, const aclScalar* value,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAddcdiv的第二段接口，用于执行计算。
 *
 * * 算子功能：执行 tensor1 除以 tensor2 的元素除法，将结果乘以标量 value 并将其添加到 input
 * 计算公式：
 * $$ out_i = self_i + value \times {tensor1_i \over tensor2_i} $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 graph LR
 *  A[(self)] -->B([l0op::Contiguous])
 *  B --> K([l0op::Cast])
 *  K --> C([l0op::Addcdiv])
 *  D[(tensor1)] -->E([l0op::Contiguous])
 *  E --> L([l0op::Cast])
 *  L --> C
 *  F[(tensor2)] --> G([l0op::Contiguous])
 *  G --> M([l0op::Cast])
 *  M --> C
 *  H((value)) --> C
 *  C --> O([l0op::Cast])
 *  O --> I([l0op::ViewCopy])
 *  I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddcdivGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddcdiv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADDCDIV_H_
// End content from: aclnn_addcdiv.h

// Begin content from: aclnn_foreach_sign.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SIGN_H_
#define ACLNN_FOREACH_SIGN_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSignGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSignGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSign
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSign(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sign.h

// Begin content from: aclnn_all_gather_matmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ALL_GATHER_MATMUL_
#define OP_API_INC_ALL_GATHER_MATMUL_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现allGather + mm 融合计算
 * @brief aclnnAllGatherMatmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bf16。
 * @param [in] x2: matmul右矩阵，数据类型支持：float16, bf16。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] gatherIndex: 标识gather目标，0：左矩阵，1：右矩阵，默认值：0。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：0/1。
 * @param [out] output: 计算+通信的结果，数据类型：同输入。
 * @param [out] gatherOut: 仅gather通信操作的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAllGatherMatmulGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                           const aclTensor* bias, const char* group,
                                                           int64_t gatherIndex, int64_t commTurn, int64_t streamMode,
                                                           const aclTensor* output, const aclTensor* gatherOut,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAllGatherMatmul的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAbsGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAllGatherMatmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ALL_GATHER_MATMUL_// End content from: aclnn_all_gather_matmul.h

// Begin content from: aclnn_foreach_minimum_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MINIMUM_SCALAR_H_
#define ACLNN_FOREACH_MINIMUM_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMinimumScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMinimumScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_minimum_scalar.h

// Begin content from: aclnn_sort.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SORT_H_
#define OP_API_INC_SORT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSort的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成sort操作
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、INT8、INT16、INT32、INT64、UINT8。支持空Tensor，
 * 支持[非连续的Tensor](#)。
 * @param [in] stable: 是否稳定排序, True为稳定排序，False为非稳定排序, 数据类型为BOOLEAN。
 * @param [in] dim: 用来作为排序标准的维度, 数据类型为INT。范围为 [-N, N-1]。
 * @param [in] descending: 控制排序顺序，True为降序，False为升序, 数据类型为BOOLEAN。
 * @param [in] valuesOut: npu device侧的aclTensor, 数据类型支持FLOAT16、FLOAT32、INT8、INT16、INT32、INT64、UINT8。支持
 * 空Tensor，支持[非连续的Tensor](#)。数据格式支持ND([参考](#))。shape和数据格式需要与self一致。
 * @param [in] indicesOut: npu device侧的aclTensor,
 * 数据类型支持INT64。支持空Tensor，支持[非连续的Tensor](#)。数据格式支持 ND([参考](#))。shape和数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSortGetWorkspaceSize(const aclTensor* self, bool stable, int64_t dim, bool descending,
                                                aclTensor* valuesOut, aclTensor* indicesOut, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief: aclnnSort的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sort操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSortGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSort(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_sort.h

// Begin content from: aclnn_foreach_mul_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MUL_LIST_H_
#define ACLNN_FOREACH_MUL_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMulListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMulList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_mul_list.h

// Begin content from: aclnn_foreach_maximum_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MAXIMUM_SCALAR_H_
#define ACLNN_FOREACH_MAXIMUM_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMaximumScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMaximumScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_maximum_scalar.h

// Begin content from: aclnn_adaptive_avg_pool2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AdaptiveAvgPool2dBackward_H_
#define OP_API_INC_AdaptiveAvgPool2dBackward_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveAvgPool2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：aclnnAdaptiveAvgPool2d反向运算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * @param [in] gradOutput: 正向运算结果，npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32数据类型，
 * 且数据类型与self一致。支持非连续的Tensor，数据格式支持NCHW，且数据格式需要与self一致。
 * @param [in] self: 正向输入，npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32数据类型,
 * 支持非连续的Tensor，数据格式支持NCHW
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnAdaptiveAvgPool2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                     aclTensor* out, uint64_t* workspaceSize,
                                                                     aclOpExecutor** executor);

/**
 * @brief aclnnAdaptiveAvgPool2dBackward的第二段接口，用于执行计算。
 *
 * 算子功能：aclnnAdaptiveAvgPool2d反向运算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnAdaptiveAvgPool2dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AdaptiveAvgPool2dBackward_H_
// End content from: aclnn_adaptive_avg_pool2d_backward.h

// Begin content from: aclnn_foreach_addcmul_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCMUL_SCALAR_H_
#define ACLNN_FOREACH_ADDCMUL_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcmulScalarGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulScalarGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcmulScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcmul_scalar.h

// Begin content from: aclnn_masked_fill_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MASKEF_FILL_TENSOR_H_
#define OP_API_INC_MASKEF_FILL_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceMaskedFillTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：用value填充selfRef里面与mask矩阵中值为true的位置相对应的元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -->B([Contiguous])
 *  B  -->C([Unsqueeze])
 *  C  -->D([MaskedFill])
 *  D  -->I([Squeeze])
 *  I   --> J([ViewCopy])
 *  J   --> K[(out)]
 *
 *  A1[(mask)] -->B1([Contiguous])
 *  B1  -->C1([Cast])
 *  C1  -->D
 *
 *  A2[(value)]-->B2[(Cast)]
 *  B2-->D
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持BOOL、UINT8、INT8、INT16、INT32、INT64、FLOAT、
 *                      FLOAT16、BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128。
 *                      支持非连续的Tensor，数据格式支持ND。
 * @param [in] mask: npu device侧的aclTensor，数据类型支持BOOL。且shape与selfRef满足broadcast关系。数据格式支持ND。
 * @param [in] value: npu device侧的aclTensor。且shape与selfRef满足broadcast关系。数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedFillTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* mask,
                                                                   const aclTensor* value, uint64_t* workspaceSize,
                                                                   aclOpExecutor** executor);
/**
 * @brief aclnnInplaceMaskedFillTensor的第二段接口，用于执行计算。
 *
 * 算子功能：用value填充selfRef里面与mask矩阵中值为true的位置相对应的元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -->B([Contiguous])
 *  B  -->C([Unsqueeze])
 *  C  -->D([MaskedFill])
 *  D  -->I([Squeeze])
 *  I   --> J([ViewCopy])
 *  J   --> K[(out)]
 *
 *  A2[(mask)] -->B2([Contiguous])
 *  B2 --> C2([Cast])
 *  C2  -->D
 *
 *  A1[(value)]-->B1([Contiguous])
 *  B1-->C1([Cast])
 *  C1-->D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceMaskedFillTensorGetWorkspaceSize。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedFillTensor(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MASKEF_FILL_TENSOR_H_// End content from: aclnn_masked_fill_tensor.h

// Begin content from: aclnn_reduce_log_sum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REDUCE_LOG_SUM_H_
#define OP_API_INC_REDUCE_LOG_SUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnReduceLogSum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * * 算子功能：使用输入边界的反射填充输入tensor。
 * @param [in] data: 表示参与计算的目标张量，维度小于8维，Device侧的aclTensor，支持[非连续的Tensor](common/非连续的Tensor.md)，
 * 数据类型支持FLOAT16、FLOAT32，[数据格式](common/数据格式.md)支持ND。
 * @param [in] axes: 指定计算维度，Host侧的aclIntArray，数据类型支持INT64，取值范围为[-self.dim(), self.dim()-1]。
 * @param [in] keepDims: 指定是否在输出张量中保留输入张量的维度，Host侧的BOOL值。
 * @param [in] noopWithEmptyAxes: 指定axes为空时的行为：false即对所有轴进行计算；true即不进行计算，输出张量等于输入张量，Host侧的BOOL值。
 * @param [in] reduce: 表示计算后的结果，维度小于8维，Device侧的aclTensor，支持[非连续的Tensor](common/非连续的Tensor.md)，
 * 数据类型支持FLOAT16、FLOAT32，需与data一致，[数据格式](common/数据格式.md)支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReduceLogSumGetWorkspaceSize(const aclTensor* data, const aclIntArray* axes, bool keepDims,
                                                     bool noopWithEmptyAxes, aclTensor* reduce, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnReduceLogSum的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceRandom获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReduceLogSum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REDUCE_LOG_SUM_H_
// End content from: aclnn_reduce_log_sum.h

// Begin content from: aclnn_zero.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_Zero_H_
#define OP_API_INC_Zero_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceZero的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将self张量填充为全零。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(selfRef)]-->B([l0op::Contiguous])
 *     B-->C([l0op::Zeros_Like])
 *     C-->D([l0op::ViewCopy])
 *     D-->E[(selfRef)]
 * ```
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持INT8、INT16、INT32、INT64、QINT8、QINT16、QINT32、UINT8、UINT16、
 * UINT32、UINT64、QUINT8、QUINT16、FLOAT16、FLOAT32、DOUBLE、COMPLEX64、COMPLEX128。支持[非连续的Tensor](#)，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceZeroGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceZero的第二段接口，用于执行计算。
 *
 * 算子功能：将self张量填充为全零。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(selfRef)]-->B([l0op::Contiguous])
 *     B-->C([l0op::Zeros_Like])
 *     C-->D([l0op::ViewCopy])
 *     D-->E[(selfRef)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceZeroGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceZero(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_Zero_H_
// End content from: aclnn_zero.h

// Begin content from: aclnn_resize.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RESIZE_H_
#define OP_API_INC_RESIZE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnResize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：调整输入张量的大小
 * @param [in]   self 输入Tensor，数据类型支持FLOAT,  FLOAT16。支持非连续Tensor，数据格式支持NCHW.
 * @param [in]   scales 输入aclFloatArray，数据类型支持 INT32.
 * @param [in]   mode 输入属性，数据类型支持const char*
 * @param [out]  workspaceSize 返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnResizeGetWorkspaceSize(const aclTensor* self, const aclFloatArray* scales, const char* mode,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnResize的第二段接口，用于执行计算。
 * @domain aclnn_ops_infer
 *
 * 算子功能：调整输入张量的大小
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnResizeGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnResize(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_RESIZE_H_// End content from: aclnn_resize.h

// Begin content from: aclnn_sinkhorn.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SINKHORN_H_
#define OP_API_INC_SINKHORN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSinkhorn的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：计算Sinkhorn距离
 *
 * @param [in] cost: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND.
 * @param [in] tol: 误差，支持FLOAT类型，如果传入空指针，则tol取0.0001。
 * @param [in] p: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND.
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSinkhornGetWorkspaceSize(
    const aclTensor* cost, 
    const aclScalar* tol,
    aclTensor* p,
    uint64_t* workspaceSize, 
    aclOpExecutor** executor);
/**
 * @brief aclnnSinkhorn的第二段接口，用于执行计算。
 *
 * 算子功能：对输入的 Tensor cost， 计算Sinkhorn距离，形成一个新的 Tensor，并返回。 
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinkhornGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSinkhorn(
    void* workspace, 
    uint64_t workspaceSize, 
    aclOpExecutor* executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SINKHORN_H_
// End content from: aclnn_sinkhorn.h

// Begin content from: aclnn_prompt_flash_attention_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License. 
 */

#ifndef ACLNN_PROMPT_FLASH_ATTENTION_V2_H_
#define ACLNN_PROMPT_FLASH_ATTENTION_V2_H_
// #include "aclnn/acl_meta.h"
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief The first interface of aclnnPromptFlashAttentionV2 is used to calculate the workspace size based on the specific calculation process.
 * @domain aclnn_ops_infer
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttentionV2GetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *pseShift,
    const aclTensor *attenMask, // attenMask of V2
    const aclIntArray *actualSeqLengths,
    const aclIntArray *actualSeqLengthsKv,
    const aclTensor *deqScale1,
    const aclTensor *quantScale1,
    const aclTensor *deqScale2,
    const aclTensor *quantScale2,
    const aclTensor *quantOffset2,
    int64_t numHeads, // q_n of V2
    double scaleValue,
    int64_t preTokens,
    int64_t nextTokens,
    char *inputLayout,
    int64_t numKeyValueHeads,
    int64_t sparseMode, // sparse of V2
    const aclTensor *attentionOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief The second interface of aclnnPromptFlashAttentionV2 is used to perform calculations.
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttentionV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_prompt_flash_attention_v2.h

// Begin content from: aclnn_maxn.h
/*
 * Copyright (c) 2023 Huawei Technologies Co., Ltd.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAXN_H_
#define OP_API_INC_MAXN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxN的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入tensors列表中每个tensor对应元素求max。
 *
 * @param [in] tensors: npu device侧的aclTensorList，数据类型支持整型，浮点类型，shape需要与out满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxNGetWorkspaceSize(const aclTensorList* tensors, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnMaxN的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入tensors列表中每个tensor对应元素求max。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnSumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxN(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAXN_H_
// End content from: aclnn_maxn.h

// Begin content from: aclnn_mean.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MEAN_H_
#define OP_API_INC_MEAN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMean的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnMeanGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                aclDataType dtype, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnMean的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMean(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnMeanV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnMeanV2GetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                  bool noopWithEmptyAxes, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnMeanV2的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMeanV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_mean.h

// Begin content from: aclnn_moe_finalize_routing_v2_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_FINALIZE_ROUTING_V2GRAD_H_
#define ACLNN_MOE_FINALIZE_ROUTING_V2GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeFinalizeRoutingV2GradGetWorkspaceSize
 * parameters :
 * gradY : required
 * expandedRowIdx : required
 * expandedXOptional : optional
 * scalesOptional : optional
 * expertIdxOptional : optional
 * biasOptional : optional
 * dropPadMode : optional
 * activeNum : optional
 * expertNum : optional
 * expertCapacity : optional
 * gradExpandedXOut : required
 * gradScalesOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRoutingV2GradGetWorkspaceSize(
    const aclTensor *gradY,
    const aclTensor *expandedRowIdx,
    const aclTensor *expandedXOptional,
    const aclTensor *scalesOptional,
    const aclTensor *expertIdxOptional,
    const aclTensor *biasOptional,
    int64_t dropPadMode,
    int64_t activeNum,
    int64_t expertNum,
    int64_t expertCapacity,
    const aclTensor *gradExpandedXOut,
    const aclTensor *gradScalesOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeFinalizeRoutingV2Grad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRoutingV2Grad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_finalize_routing_v2_grad.h

// Begin content from: aclnn_moe_token_permute.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_TOKEN_PERMUTE_H_
#define ACLNN_MOE_TOKEN_PERMUTE_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeTokenPermuteGetWorkspaceSize
 * parameters :
 * tokens : required
 * indices : required
 * numOutTokens : optional
 * paddedMode : optional
 * permuteTokensOut : required
 * sortedIndicesOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenPermuteGetWorkspaceSize(
    const aclTensor *tokens,
    const aclTensor *indices,
    int64_t numOutTokens,
    bool paddedMode,
    const aclTensor *permuteTokensOut,
    const aclTensor *sortedIndicesOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeTokenPermute
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenPermute(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_token_permute.h

// Begin content from: aclnn_inplace_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_inplace_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_INPLACE_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_INPLACE_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bfloat16。
 * @param [in] x2: matmul右矩阵，数据类型支持：float16, bfloat16。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。输出MatmulAllReduce+Add(residual)的结果。
 * @param [in] residual: 残差，数据类型支持：float16, bfloat16。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* residual, const aclTensor* gamma,
    double epsilon, const char* group, const char* reduceOp, int64_t commTurn, int64_t streamMode,
    const aclTensor* normOut, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，
 *                             由第一段接口aclnnInplaceMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize,
                                                            aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INPLACE_MATMUL_ALL_REDUCE_ADD_RMS_NORM_// End content from: aclnn_inplace_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_foreach_maximum_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MAXIMUM_LIST_H_
#define ACLNN_FOREACH_MAXIMUM_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMaximumListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMaximumList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_maximum_list.h

// Begin content from: aclnn_var.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_VAR_H_
#define OP_API_INC_VAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnVar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnVarGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool unbiased,
                                               bool keepdim, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnVar的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnVar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnVarCorrection的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnVarCorrectionGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim,
                                                         int64_t correction, bool keepdim, aclTensor* out,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnVarCorrection的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnVarCorrection(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_var.h

// Begin content from: aclnn_lt_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LTTENSOR_H_
#define OP_API_INC_LTTENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLtTensorGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Tensor中的元素，返回一个Bool类型的Tensor，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ? [True]
 * : [False] $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持float,int32,int8,uint8,int64,int16,double,kHalf,kBool数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持float,int32,int8,uint8,int64,int16,double,kHalf,kBool数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLtTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLtTensor的第二段接口，用于执行计算。
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Tensor中的元素，返回一个Bool类型的Tensor，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ? [True]
 * : [False] $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLtTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceLtTensorGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Tensor中的元素，返回计算后的selfRef，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ?  [True] :
 * [False] $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持float,int32,int8,uint8,int64,int16,double,kHalf,kBool数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持float,int32,int8,uint8,int64,int16,double,kHalf,kBool数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLtTensorGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLtTensor的第二段接口，用于执行计算。
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Tensor中的元素，返回一个Bool类型的Tensor，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ? [True]
 * : [False] $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLtTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LTTENSOR_H_
// End content from: aclnn_lt_tensor.h

// Begin content from: aclnn_tril.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_TRIL_H_
#define OP_API_INC_LEVEL2_ACLNN_TRIL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTril的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入矩阵（2-D `Tensor`）或一批矩阵的下三角部分，结果`Tensor`out的其他元素设置为0。

 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0::Contiguous])
 *   B --> E([l0::Tril])
 *   C[(diagonal)] --> E
 *   E --> G([l0::ViewCopy])
 *   G --> H[(out)]
 * ```
 *
 * @param [in] self: 待进行tril计算的入参。npu device侧的aclTensor，
 * 数据类型支持DOUBLE,FLOAT,FLOAT16,INT16,INT32,INT64,INT8,UINT16,UINT32,UINT64,UINT8,BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] diagonal: 对角线的位置，数据类型支持INT。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTrilGetWorkspaceSize(const aclTensor* self, int64_t diagonal, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnTril的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAllGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTril(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceTril的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入矩阵（2-D `Tensor`）或一批矩阵的下三角部分，结果`Tensor`out的其他元素设置为0。
 *
 * @param [in] selfRef: 待进行tril计算的入参。npu device侧的aclTensor，
 * 数据类型支持DOUBLE,FLOAT,FLOAT16,INT16,INT32,INT64,INT8,UINT16,UINT32,UINT64,UINT8,BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] diagonal: 对角线的位置，数据类型支持INT。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTrilGetWorkspaceSize(const aclTensor* selfRef, int64_t diagonal,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceTril的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAllGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTril(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_TRIL_H_// End content from: aclnn_tril.h

// Begin content from: aclnn_ffn_v2.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file aclnn_ffn_v2.h
 * \brief
 */

#ifndef OP_API_INC_FFN_V2_H
#define OP_API_INC_FFN_V2_H
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFFNV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：该FFN算子提供MoeFFN和FFN的计算功能
 * 计算公式：y=activation(xW1+b1)W2+b2
 * @domain aclnn_ops_infer
 * @param [in]
 * x：必选参数，Device侧的aclTensor，公式中的输入x，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，支持输入的维度最少是2维[M,
 * K1]，最多是8维。
 * @param [in]
 * weight1：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W1，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K1, N1]/[K1, N1]。
 * @param [in]
 * weight2：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W2，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K2, N2]/[K2, N2]。
 * @param [in]
 * expertTokens：可选参数，Host侧的aclIntArray类型，代表各专家的token数，数据类型支持INT64，数据格式支持ND，若不为空时可支持的最大长度为256个。
 * @param [in]
 * bias1：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b1，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * bias2：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b2，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * scale：可选参数，Device侧的aclTensor，量化参数，量化缩放系数，数据类型支持FLOAT32，数据格式支持ND，per-tensor下输入在有/无专家时均为一维向量，输入元素个数在有/无专家时分别为[E]/[1]；per-channel下输入在有/无专家时为二维向量/一维向量，输入元素个数在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * offset：可选参数，Device侧的aclTensor，量化参数，量化偏移量，数据类型支持FLOAT32，数据格式支持ND，一维向量，输入元素个数在有/无专家时分别为[E]/[1]。
 * @param [in]
 * deqScale1：可选参数，Device侧的aclTensor，量化参数，第一个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * deqScale2：可选参数，Device侧的aclTensor，量化参数，第二个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * antiquantScale1：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantScale2：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * antiquantOffset1：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantOffset2：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * activation：必选参数，Host侧的属性值，代表使用的激活函数，公式中的activation，当前支持fastgelu/gelu/relu/silu以及geglu/swiglu/reglu。
 * @param [in]
 * innerPrecise：可选参数，Host侧的int，表示高精度或者高性能选择。数据类型支持：INT64。该参数仅对FLOAT16生效，BFLOAT16和INT8不区分高精度和高性能。
 * @param [in] tokensIndexFlag：可选参数，Host侧的bool，指示expertTokens是否为索引值。数据类型支持：bool。
 * @param [out] y：输出Tensor，公式中的输出y，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，输出维度与x一致。
 * @param [out] workspaceSize：返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor：返回op执行器，包含了算子计算流程。
 * @return      aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFFNV2GetWorkspaceSize(
    const aclTensor *x, const aclTensor *weight1, const aclTensor *weight2, const aclIntArray *expertTokens,
    const aclTensor *bias1, const aclTensor *bias2, const aclTensor *scale, const aclTensor *offset,
    const aclTensor *deqScale1, const aclTensor *deqScale2, const aclTensor *antiquantScale1,
    const aclTensor *antiquantScale2, const aclTensor *antiquantOffset1, const aclTensor *antiquantOffset2,
    const char *activation, int64_t innerPrecise, bool tokensIndexFlag, const aclTensor *y, uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnFFNV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小，由第一段接口aclnnFFNV2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL stream流。
 * @return     aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFFNV2(void *workspace, uint64_t workspaceSize,
                                                              aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_FFN_V2_H// End content from: aclnn_ffn_v2.h

// Begin content from: aclnn_atanh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ATANH_H_
#define OP_API_INC_ATANH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAtanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入Tensor中的每个元素进行反双曲正切操作后输出。
 * 计算公式：
 * out_{i}=ln((1 + input_{i})/(1 - input_{i}))
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnAtanhGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);
/**
 * @brief aclnnAtanh的第二段接口，用于执行计算。
 * 功能描述：对输入Tensor中的每个元素进行反双曲正切操作后输出。
 * 计算公式：
 * out_{i}=ln((1 + input_{i})/(1 - input_{i}))
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Atanh])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtanhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAtanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAtanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入Tensor中的每个元素进行反双曲正切操作后输出。
 * 计算公式：
 * out_{i}=ln((1 + input_{i})/(1 - input_{i}))
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAtanhGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAtanh的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成atanh操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtanhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAtanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_atanh.h

// Begin content from: aclnn_addcmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADD_CMUL_H_
#define OP_API_INC_ADD_CMUL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddcmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成乘加计算
 * 计算公式：
 * $$ output=self+ value \times tensor1 \times tensor2 $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]--> B([l0op::Contiguous]) --> C([l0op::Cast]) -->D([Addcmul])
 * E[(tensor1)]--> B1([l0op::Contiguous]) --> G([l0op::Cast]) --> D
 * E1[(tensor2)]--> B2([l0op::Contiguous]) --> G1([l0op::Cast])  --> D
 * D --> H([l0op::Cast]) --> I([l0op::ViewCopy]) --> J[(out)]
 * K((value)) --> L([l0op::Cast]) --> D
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] tensor1: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] tensor2: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] value: host侧的aclScalar，数据类型需要可转换成其他输入推导后的数据类型。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要是其他输入推导之后可转换的数据类型，shape需要是其他输入
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddcmulGetWorkspaceSize(const aclTensor* self, const aclTensor* tensor1,
                                                   const aclTensor* tensor2, const aclScalar* value, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAddcmul的第二段接口，用于执行计算。
 *
 * 算子功能：完成乘加计算
 * 计算公式：
 * $$ output=self+ value \times tensor1 \times tensor2 $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]--> B([l0op::Contiguous]) --> C([l0op::Cast]) -->D([Addcmul])
 * E[(tensor1)]--> B1([l0op::Contiguous]) --> G([l0op::Cast]) --> D
 * E1[(tensor2)]--> B2([l0op::Contiguous]) --> G1([l0op::Cast])  --> D
 * D --> H([l0op::Cast]) --> I([l0op::ViewCopy]) --> J[(out)]
 * K((value)) --> L([l0op::Cast]) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddcmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

/**
 * @brief aclnnInplaceAddcmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成乘加计算
 * 计算公式：
 * $$ output=self+ value \times tensor1 \times tensor2 $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]--> B([l0op::Contiguous]) --> C([l0op::Cast]) -->D([Addcmul])
 * E[(tensor1)]--> B1([l0op::Contiguous]) --> G([l0op::Cast]) --> D
 * E1[(tensor2)]--> B2([l0op::Contiguous]) --> G1([l0op::Cast])  --> D
 * D --> H([l0op::Cast]) --> I([l0op::ViewCopy]) --> J[(out)]
 * K((value)) --> L([l0op::Cast]) --> D
 * ```
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] tensor1: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] tensor2: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与其他输入构成互相推导关系，shape需要与其他输入满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与其他输入一致。
 * @param [in] value: host侧的aclScalar，数据类型需要可转换成其他输入推导后的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddcmulGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* tensor1,
                                                          const aclTensor* tensor2, const aclScalar* value,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAddcmul的第二段接口，用于执行计算。
 *
 * 算子功能：完成乘加计算
 * 计算公式：
 * $$ output=self+ value \times tensor1 \times tensor2 $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)]--> B([l0op::Contiguous]) --> C([l0op::Cast]) -->D([Addcmul])
 * E[(tensor1)]--> B1([l0op::Contiguous]) --> G([l0op::Cast]) --> D
 * E1[(tensor2)]--> B2([l0op::Contiguous]) --> G1([l0op::Cast])  --> D
 * D --> H([l0op::Cast]) --> I([l0op::ViewCopy]) --> J[(out)]
 * K((value)) --> L([l0op::Cast]) --> D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddcmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_CMUL_H_
// End content from: aclnn_addcmul.h

// Begin content from: aclnn_isin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_IS_IN_H_
#define OP_API_INC_IS_IN_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIsInScalarTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：检查element是否在张量testElements中。
 *
 * @param [in] element: npu device侧的aclScalar，数据类型支持整型，浮点类型。
 * @param [in] testElements: host侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND
 * @param [in] assumeUnique: host侧的bool，表示testElements中的值是否唯一。
 * @param [in] invert: host侧的bool，表示输出结果是否反转。
 * @param [in] out: npu device侧的aclTensor，数据类型支持布尔。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsInScalarTensorGetWorkspaceSize(const aclScalar* element, const aclTensor* testElements,
                                                            bool assumeUnique, bool invert, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIsInScalarTensor的第二段接口，用于执行计算。
 *
 * 算子功能：检查element是否在张量testElements中。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnIsInScalarTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsInScalarTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_IS_IN_H_
// End content from: aclnn_isin.h

// Begin content from: aclnn_adaptive_avg_pool2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ADAPTIVE_AVG_POOL2D_H_
#define OP_API_INC_ADAPTIVE_AVG_POOL2D_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#include <array>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveAvgPool2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnAdaptiveAvgPool2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_adaptive_avg_pool2d.h

// Begin content from: aclnn_acosh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACOSH_H_
#define OP_API_INC_ACOSH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAcosh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对输入Tensor中的每个元素进行反双曲余弦操作后输出。
 * 计算公式：
 * $$ out_{i}=cosh^{-1}(self_{i}) $$
 *
 * @param [in] self: 输入Tensor，数据类型支持整型，浮点，复数类型，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: 输出Tensor，数据类型支持浮点，复数类型，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAcoshGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnAcosh的第二段接口，用于执行计算。
 *
 * 算子功能：对输入Tensor中的每个元素进行反双曲余弦操作后输出。
 * 计算公式：
 * $$ out_{i}=cosh^{-1}(self_{i}) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAcoshGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAcosh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAcosh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对输入Tensor中的每个元素进行反双曲余弦操作后输出。
 * 计算公式：
 * $$ selfRef_{i}=cosh^{-1}(selfRef_{i}) $$
 *
 * @param [in] selfRef: 输入Tensor，数据类型支持浮点，复数类型，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAcoshGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAcosh的第二段接口，用于执行计算。
 *
 * 算子功能：对输入Tensor中的每个元素进行反双曲余弦操作后输出。
 * 计算公式：
 * $$ selfRef_{i}=cosh^{-1}(selfRef_{i}) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAcoshGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAcosh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ACOSH_H_
// End content from: aclnn_acosh.h

// Begin content from: aclnn_batch_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BATCH_NORM_H_
#define OP_API_INC_BATCH_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnBatchNormGetWorkspaceSize(const aclTensor* input, const aclTensor* weight,
                                                     const aclTensor* bias, aclTensor* runningMean,
                                                     aclTensor* runningVar, bool training, double momentum, double eps,
                                                     aclTensor* output, aclTensor* saveMean, aclTensor* saveInvstd,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBatchNorm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BATCH_NORM_H_
// End content from: aclnn_batch_norm.h

// Begin content from: aclnn_cast.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CAST_H_
#define OP_API_INC_CAST_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCast的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入tensor转换为指定的dtype类型。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、FlOAT64、INT8、UINT8、INT16、INT32、INT64、BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] dtype: 	host侧的aclDataType，输入tensor要转换的目标dtype。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、FlOAT64、INT8、UINT8、INT16、INT32、INT64、BOOL、
 * COMPLEX64、COMPLEX128。数据类型为dtype，shape与self相同，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCastGetWorkspaceSize(const aclTensor* self, const aclDataType dtype, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnCast的第二段接口，用于执行计算。
 *
 * 算子功能：将输入tensor转换为指定的dtype类型。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCastGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCast(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CAST_H_
// End content from: aclnn_cast.h

// Begin content from: aclnn_replication_pad1d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REPLICATION_PAD1D_BACKWARD_H_
#define OP_API_INC_REPLICATION_PAD1D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReplicationPad1dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：replication_pad1d的反向传播。
 * @param [in] gradOutput: 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))，
 * 维度支持二维或三维且与self和gradInput一致，shape需要与replication_pad1d正向传播的output一致。
 * @param [in] self:
 * 数据类型与gradOutput一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))，维度支持二维或三维且与gradOutput和
 * gradInput一致，shape与gradInput一致。
 * @param [in] padding: 数据类型为INT64，长度为2，数值依次代表左右需要填充的值。
 * @param [in] gradInput:
 * 数据类型与gradOutput一致，shape与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad1dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                    const aclIntArray* padding, aclTensor* gradInput,
                                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad1dBackward的第二段接口，用于执行计算
 *
 * 算子功能：replication_pad1d的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReplicationPad1dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad1dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REPLICATION_PAD1D_BACKWARD_H_// End content from: aclnn_replication_pad1d_backward.h

// Begin content from: aclnn_foreach_atan.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ATAN_H_
#define ACLNN_FOREACH_ATAN_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAtanGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAtanGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAtan
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAtan(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_atan.h

// Begin content from: aclnn_foreach_sub_list_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_SUB_LIST_V2_H_
#define OP_API_INC_ACLNN_FOREACH_SUB_LIST_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachSubListV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：两个Tensor列表中的元素逐个相减，并返回一个新的Tensor列表。可以通过设置alpha参数来调整相减的系数。
 * 计算公式：
 * out_{i}=x1_{i}-x2_{i}*alpha
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x1
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   x2
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   alpha
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachSubListV2GetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclScalar *alpha,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachSubListV2的第二段接口，用于执行计算。
 * 功能描述：两个Tensor列表中的元素逐个相减，并返回一个新的Tensor列表。可以通过设置alpha参数来调整相减的系数。
 * 计算公式：
 * out_{i}=x1_{i}-x2_{i}*alpha
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachSubListV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachSubListV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sub_list_v2.h

// Begin content from: aclnn_inplace_quant_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_inplace_quant_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_INPLACE_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_INPLACE_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceQuantMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] bias: 偏置，数据类型支持：int32。
 * @param [in] dequantScale: 去量化系数，数据类型支持：int64,uint64,bfloat16。
 * @param [in] residual: 残差，数据类型支持：float16, bfloat16。输出MatmulAllReduce+Add(residual)的结果。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */

ACLNN_API aclnnStatus aclnnInplaceQuantMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* dequantScale,
    const aclTensor* residual, const aclTensor* gamma, double epsilon, const char* group, const char* reduceOp,
    int64_t commTurn, int64_t streamMode, const aclTensor* normOut, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceQuantMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，
 *                             由第一段接口aclnnInplaceQuantMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceQuantMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize,
                                                                 aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INPLACE_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_// End content from: aclnn_inplace_quant_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_quant_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_quant_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] bias: 偏置，数据类型支持：int32。
 * @param [in] dequantScale: 去量化系数，数据类型支持：uint64,bfloat16。
 * @param [in] residual: 残差，数据类型支持：float16, bfloat16。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] y: MatmulAllReduce+Add(residual)的结果，数据类型：同输入。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */

ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* dequantScale,
    const aclTensor* residual, const aclTensor* gamma, double epsilon, const char* group, const char* reduceOp,
    int64_t commTurn, int64_t streamMode, const aclTensor* y, const aclTensor* normOut, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnQuantMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize,
                                                          aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_// End content from: aclnn_quant_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_floor_divide.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_FLOORDIVIDE_H_
#define OP_API_INC_FLOORDIVIDE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFloorDivide的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFloorDivideGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFloorDivides的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnFloorDividesGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFloorDivide的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceFloorDivideGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFloorDivides的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceFloorDividesGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFloorDivide的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnFloorDivideGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFloorDivide(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

ACLNN_API aclnnStatus aclnnFloorDivides(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceFloorDivide(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceFloorDivides(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_FLOORDIVIDE_H_
// End content from: aclnn_floor_divide.h

// Begin content from: aclnn_logsigmoid.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG_SIGMOID_H_
#define OP_API_INC_LOG_SIGMOID_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogSigmoid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：对输入Tensor逐元素实现LogSigmoid运算
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND，
 * 且shape需要与out一致，和out的数据类型满足数据类型推导规则。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND，
 * 且shape需要与self一致，和self的数据类型满足数据类型推导规则。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogSigmoidGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnLogSigmoidForward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnLogSigmoidForwardGetWorkspaceSize(const aclTensor* self, aclTensor* out, aclTensor* buffer,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogSigmoid的第二段接口，用于执行计算。
 *
 * 算子功能：对输入Tensor逐元素实现LogSigmoid运算
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLogSigmoidGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogSigmoid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnLogSigmoidForward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLogSigmoidForward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG_SIGMOID_H_
// End content from: aclnn_logsigmoid.h

// Begin content from: aclnn_all.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ALL_H_
#define OP_API_INC_LEVEL2_ACLNN_ALL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACLNN_MAX_SHAPE_RANK 8
#define DIM_BITS_LEN 64

/**
 * @brief aclnnAll的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：对Tensor中的所有元素进行与运算。

 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0::Contiguous])
 *   B --> E([l0::ReduceAll])
 *   C[(dim)] --> E
 *   D[(keepdim)] --> E
 *   E --> G([l0::ViewCopy])
 *   G --> H[(out)]
 * ```
 *
 * @param [in] self: 待进行all计算的入参。npu device侧的aclTensor，
 * 数据类型支持BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [in] dim: 需要压缩的维度，值需要在输入Tensor范围内，支持负数，数据类型支持INT
 * @param [in] keepdim: 输出张量`dim`是否与输入保持一致，数据类型支持BOOL
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAllGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepdim,
                                               aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAll的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAllGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAll(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ALL_H_// End content from: aclnn_all.h

// Begin content from: aclnn_softshrink_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFT_SHRINK_BACKWARD_H_
#define OP_API_INC_SOFT_SHRINK_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftshrinkBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * @param [in] gradOutput: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，
 * 且数据类型与output一致,shape与output相同。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] output: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] gradInput: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftshrinkBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                              const aclScalar* lambda, aclTensor* gradInput,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSoftshrinkBackward的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnSoftshrinkBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftshrinkBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFT_SHRINK_BACKWARD_H_// End content from: aclnn_softshrink_backward.h

// Begin content from: aclnn_upsample_nearest_1d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_NEAREST_H_
#define OP_API_INC_UNAMPLE_NEAREST_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest1dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest1dBackwardGetWorkspaceSize(const aclTensor* gradOut,
                                                                     const aclIntArray* outputSize,
                                                                     const aclIntArray* inputSize, double scales,
                                                                     aclTensor* out, uint64_t* workspaceSize,
                                                                     aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest1dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest1dBackward(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_H_
// End content from: aclnn_upsample_nearest_1d_backward.h

// Begin content from: aclnn_selu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SILU_GRAD_H_
#define OP_API_INC_LEVEL2_ACLNN_SILU_GRAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能： 对输入Tensor完成selubackward操作
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持INT8、INT32,
 * FLOAT、FLOAT16、shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] result: npu device侧的aclTensor, 数据类型支持INT8、INT32, FLOAT、FLOAT16, shape与gradOutput
 *  保持相同，数据格式支持ND。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型支持INT8、INT32, FLOAT、FLOAT16, shape与gradOutput
 *  保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSeluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* result,
                                                        aclTensor* gradInput, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnSelu的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成selu操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSeluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSeluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_selu_backward.h

// Begin content from: aclnn_adaptive_max_pool2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ADAPTIVE_MAX_POOL2D_H_
#define OP_API_INC_ADAPTIVE_MAX_POOL2D_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#include <array>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveMaxPool2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAdaptiveMaxPool2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             aclTensor* outputOut, aclTensor* indicesOut,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAdaptiveMaxPool2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAdaptiveMaxPool2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_adaptive_max_pool2d.h

// Begin content from: aclnn_addr.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_ADDR_H_
#define OP_API_INC_LEVEL2_ACLNN_ADDR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素绝对值的结果
 * 计算公式：
 * $$ out_{i} = addr(self, vec1, vec2, beta=1, alpha=1) $$
 * @param [in] self: addr入参，外积扩展矩阵。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [in] vec1: addr入参，外积入参第一向量，npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [in] vec2: addr入参，外积入参第二向量，npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [in] betaOptional: addr可选入参，外积扩展矩阵的比例因子，host侧的aclScalar
 * @param [in] alphaOptional: addr可选入参，外积的比例因子，host侧的aclScalar
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAddrGetWorkspaceSize(const aclTensor* self, const aclTensor* vec1, const aclTensor* vec2,
                                                const aclScalar* betaOptional, const aclScalar* alphaOptional,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAddr的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddrGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAddr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceAddr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnInplaceAddrGetWorkspaceSize(aclTensor* selfRef, const aclTensor* vec1, const aclTensor* vec2,
                                                       const aclScalar* betaOptional, const aclScalar* alphaOptional,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);
ACLNN_API aclnnStatus aclnnInplaceAddr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ADDR_H_// End content from: aclnn_addr.h

// Begin content from: aclnn_celu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CELU_H_
#define OP_API_INC_LEVEL2_ACLNN_CELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：对输入张量self中的每个元素x调用连续可微指数线性单元激活函数CELU，并将得到的结果存入输出张量out中。
 * 计算公式：如下
 * $$
 * CELU(x) = max(0, x) + min(0, \alpha \ast (exp(x / \alpha) - 1))
 * $$
 *
 * 实现说明：如下
 *
 * 计算图：如下
 *
 * ```mermaid
 * graph LR
 *     A[(Self)] --> B([l0op::Contiguous])
 *     B --> C([l0op::CeluV2])
 *     C --> D([l0op::Cast])
 *     D --> E([l0op::ViewCopy])
 *     E --> F[(out)]
 *     G((alpha)) --> C
 * ```
 */

/**
 * @brief aclnnCelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: 表示CELU激活函数的输入，npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的
 * Tensor, 数据格式支持ND，数据维度不支持8维以上。
 * @param [in] alpha: 表示CELU激活函数的激活系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] out: 表示CELU激活函数的输出，npu
 * device侧的aclTensor，数据类型需要是self可转换的数据类型，shape需要与self一致，
 * 支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCeluGetWorkspaceSize(const aclTensor* self, const aclScalar* alpha, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnCelu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCeluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceCelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] selfRef: 表示CELU激活函数的输入，npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，支持非连续的Tensor，数据 格式支持ND，数据维度不支持8维以上。
 * @param [in] alpha: 表示CELU激活函数的激活系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCeluGetWorkspaceSize(aclTensor* selfRef, const aclScalar* alpha,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceCelu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceCeluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CELU_H_
// End content from: aclnn_celu.h

// Begin content from: aclnn_weight_quant_batch_matmul_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V2_H_
#define OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnWeightQuantBatchMatmulV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmulV2GetWorkspaceSize(
    const aclTensor* x, const aclTensor* weight, const aclTensor* antiquantScale,
    const aclTensor* antiquantOffsetOptional, const aclTensor* quantScaleOptional, const aclTensor* quantOffsetOptional,
    const aclTensor* biasOptional, int antiquantGroupSize, const aclTensor* y, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnWeightQuantBatchMatmulV2的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmulV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V2_H_// End content from: aclnn_weight_quant_batch_matmul_v2.h

// Begin content from: aclnn_foreach_mul_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MUL_SCALAR_LIST_H_
#define ACLNN_FOREACH_MUL_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMulScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMulScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMulScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_mul_scalar_list.h

// Begin content from: aclnn_diag.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DIAG_H_
#define OP_API_INC_DIAG_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDiag的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] diagonal: 对应逻辑表达式中的对角线输入，默认值为0，数据类型支持：INT64。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDiagGetWorkspaceSize(const aclTensor* self, int64_t diagonal, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDiag的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnDiagGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDiag(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DIAG_H_
// End content from: aclnn_diag.h

// Begin content from: aclnn_aminmax_dim.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_AMINMAX_DIM_H_
#define OP_API_INC_AMINMAX_DIM_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAminmaxDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] dim: host侧的Int64_t，指定要缩减的维度。
 * @param [in] keepDim: host侧的bool，reduce轴的维度是否保留。
 * @param [in] minOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] maxOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminmaxDimGetWorkspaceSize(const aclTensor* self, const int64_t dim, bool keepDim,
                                                      aclTensor* minOut, aclTensor* maxOut, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnAminmaxDim的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAminmaxDimGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminmaxDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AMINMAX_DIM_H_
// End content from: aclnn_aminmax_dim.h

// Begin content from: aclnn_sigmoid_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SIGMOID_BACKWARD_H_
#define OP_API_INC_SIGMOID_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSigmoidgBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能： 对输入Tensor完成sigmoid backward操作
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128,
 * shape为非空， 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] output: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128,
 * shape与gradOutput保持相同，数据格式支持ND, 支持非连续的Tensor。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，
 * shape与gradOutput保持相同，数据格式支持ND, 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSigmoidBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* output,
                                                           aclTensor* gradInput, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnSigmoidBackward的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sigmoid backward操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSigmoidBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSigmoidBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SIGMOID_BACKWARD_H_// End content from: aclnn_sigmoid_backward.h

// Begin content from: aclnn_matmul_compress_dequant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MM_UNZIP_H_
#define OP_API_INC_MM_UNZIP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMatmulCompressDequant的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMatmulCompressDequantGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                                 const aclTensor* compressIndex, const aclTensor* bias,
                                                                 const aclTensor* deqScale, const aclTensor* offsetW,
                                                                 int offsetX, const aclIntArray* compressInfo,
                                                                 aclTensor* out, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief aclnnMatmulCompressDequant的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMatmulCompressDequant(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MM_UNZIP_H_
// End content from: aclnn_matmul_compress_dequant.h

// Begin content from: aclnn_reduce_nansum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REDUCE_NANSUM_H_
#define OP_API_INC_REDUCE_NANSUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReduceNansum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnReduceNansumGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                        aclDataType dtype, aclTensor* out, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnReduceNansum的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnReduceNansum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REDUCE_SUM_H_
// End content from: aclnn_reduce_nansum.h

// Begin content from: aclnn_add.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADD_H_
#define OP_API_INC_ADD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = self_i+alpha*other_i $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddGetWorkspaceSize(const aclTensor* self, const aclTensor* other, const aclScalar* alpha,
                                               aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAdd的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = self_i+alpha*other_i $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAdd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnAdds的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnAddsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, const aclScalar* alpha,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
ACLNN_API aclnnStatus aclnnAdds(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAdd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = self_i+alpha*other_i $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* other,
                                                      const aclScalar* alpha, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAdd的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = self_i+alpha*other_i $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAdd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceAdds的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceAddsGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* other,
                                                       const aclScalar* alpha, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnInplaceAdds(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_add.h

// Begin content from: aclnn_rand.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ACLNN_RAND_H_
#define OP_API_INC_ACLNN_RAND_H_

// #include "aclnn_normal.h"
// #include "aclnn_randperm.h"
// #include "aclnn_dropout_gen_mask.h"
// #include "aclnn_uniform.h"
// #include "aclnn_random.h"
// #include "aclnn_dropout_do_mask.h"
// #include "aclnn_multinomial.h"
// #include "aclnn_bernoulli.h"
// #include "aclnn_normal_out.h"
// #include "aclnn_dropout.h"
// #include "aclnn_dropout_backward.h"

#endif // OP_API_INC_ACLNN_RAND_H_// End content from: aclnn_rand.h

// Begin content from: aclnn_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NORM_H_
#define OP_API_INC_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief 根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor, 数据类型支持浮点数据类型
 * @param [in] p: host侧aclScalar, 数据类型支持浮点数据类型
 * @param [in] dim: npu device侧的aclIntArray, 代表对应的降维轴Axis的信息
 * @param [in] keepdim: host侧的bool, 代表是否保留对应dim维度的信息
 * @param [in] workspaceSize: 返回用户需要的npu device侧申请的workspace大小
 * @param [in] executor: 返回op执行器，包含算子计算流程
 * @return aclnnStatus: 返回状态码
 * */
ACLNN_API aclnnStatus aclnnNormGetWorkspaceSize(const aclTensor* self, const aclScalar* pScalar, const aclIntArray* dim,
                                                bool keepdim, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * aclnnNorm的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存地址
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNormGetWorkspaceSize获取
 * @param [in] executor: 返回op执行器，包含算子计算流程
 * @param [in] stream: acl stream流
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_norm.h

// Begin content from: aclnn_one_hot.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ONE_HOT_H_
#define OP_API_INC_ONE_HOT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnOneHot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnOneHotGetWorkspaceSize(const aclTensor* self, int numClasses, const aclTensor* onValue,
                                                  const aclTensor* offValue, int64_t axis, aclTensor* out,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnOneHot的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnOneHot(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ONE_HOT_H_
// End content from: aclnn_one_hot.h

// Begin content from: aclnn_upsample_nearest_exact2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_
#define OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearestExact2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对由三个输入通道组成的输入信号应用最近邻精确插值算法进行上采样插值
 * 计算公式：
 * out(N, C, l) = self(N, C, min(floor((l + 0.5) * scales),  L- 1))
 * @domain aclnn_ops_train aclnn_ops_infer
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND、NCHW。
 * @param [in]   outputSize
 * 输出的size大小，数据类型支持INT32、INT64。
 * @param [in]   scaleH
 * 输出的H方向缩放系数，数据类型支持DOUBLE。
 * @param [in]   scaleW
 * 输出的W方向缩放系数，数据类型支持DOUBLE。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND、NCHW。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact2dGetWorkspaceSize(const aclTensor *self, const aclIntArray *outputSize, 
                                                                  double scalesH, double scalesW, aclTensor *out, 
                                                                  uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleNearestExact2d的第二段接口，用于执行计算。
 * 
 * 功能描述：对由四个输入通道组成的输入信号应用最近邻精确插值算法进行上采样插值。
 * 
 * @domain aclnn_ops_train aclnn_ops_infer
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnUpsampleNearestExact2dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。 
*/
ACLNN_API aclnnStatus aclnnUpsampleNearestExact2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_
// End content from: aclnn_upsample_nearest_exact2d.h

// Begin content from: aclnn_foreach_erfc.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ERFC_H_
#define ACLNN_FOREACH_ERFC_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachErfcGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachErfcGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachErfc
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachErfc(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_erfc.h

// Begin content from: aclnn_ring_attention_update.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_RING_ATTENTION_UPDATE_H_
#define ACLNN_RING_ATTENTION_UPDATE_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnRingAttentionUpdateGetWorkspaceSize
 * parameters :
 * prevAttnOut : required
 * prevSoftmaxMax : required
 * prevSoftmaxSum : required
 * curAttnOut : required
 * curSoftmaxMax : required
 * curSoftmaxSum : required
 * actualSeqQlenOptional : optional
 * inputLayoutOptional : optional
 * attnOutOut : required
 * softmaxMaxOut : required
 * softmaxSumOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRingAttentionUpdateGetWorkspaceSize(
    const aclTensor *prevAttnOut,
    const aclTensor *prevSoftmaxMax,
    const aclTensor *prevSoftmaxSum,
    const aclTensor *curAttnOut,
    const aclTensor *curSoftmaxMax,
    const aclTensor *curSoftmaxSum,
    const aclTensor *actualSeqQlenOptional,
    char *inputLayoutOptional,
    const aclTensor *attnOutOut,
    const aclTensor *softmaxMaxOut,
    const aclTensor *softmaxSumOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnRingAttentionUpdate
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRingAttentionUpdate(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_ring_attention_update.h

// Begin content from: aclnn_precision_compare.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PRECISION_COMPARE_H_
#define OP_API_INC_PRECISION_COMPARE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnPrecisionCompare的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 比较计算两个Tensor是否相同，返回算子执行的状态码:
 *
 * @param [in] golden: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT数据类型，
 * golden与realdata数据类型一致
 * @param [in] realdata: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT数据类型，
 * realdata与golden数据类型一致
 * @param [in] out: 输出一个数据类型为UINT32类型的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnPrecisionCompareGetWorkspaceSize(const aclTensor *golden, const aclTensor *realdata,
                                                            aclTensor *out, uint64_t *workspaceSize,
                                                            aclOpExecutor **executor);
/**
 * @brief aclnnPrecisionCompare的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnPrecisionCompareGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnPrecisionCompare(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                            aclrtStream stream);
#ifdef __cplusplus
}
#endif
#endif  // OP_API_INC_PRECISION_COMPARE_H_// End content from: aclnn_precision_compare.h

// Begin content from: aclnn_foreach_lerp_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LERP_LIST_H_
#define ACLNN_FOREACH_LERP_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLerpListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * weight : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLerpListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *weight,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLerpList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLerpList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_lerp_list.h

// Begin content from: aclnn_moe_init_routing_quant.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_INIT_ROUTING_QUANT_H_
#define ACLNN_MOE_INIT_ROUTING_QUANT_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeInitRoutingQuantGetWorkspaceSize
 * parameters :
 * x : required
 * rowIdx : required
 * expertIdx : required
 * activeNum : required
 * scale : required
 * offset : required
 * expandedXOut : required
 * expandedRowIdxOut : required
 * expandedExpertIdxOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingQuantGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *rowIdx,
    const aclTensor *expertIdx,
    int64_t activeNum,
    double scale,
    double offset,
    const aclTensor *expandedXOut,
    const aclTensor *expandedRowIdxOut,
    const aclTensor *expandedExpertIdxOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeInitRoutingQuant
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingQuant(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_init_routing_quant.h

// Begin content from: aclnn_channel_shuffle.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CHANNEL_SHUFFLE_H_
#define OP_API_INC_LEVEL2_ACLNN_CHANNEL_SHUFFLE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnChannelShuffle的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将$(*, C, H, W)$张量的channels分成$g$个组，然后重新排列成$(*, C_{,}^{\underline{g}}g, H, W)$，
 * 同时保持最终输出张量的shape和输入张量保持一致。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT、DOUBLE、UINT8、INT8、INT16、INT32、
 * LONG、COMPLEX64、COMPLEX128、BOOL，支持非连续的Tensor，数据格式支持ND，数据维度大于2且不支持7维以上。
 * @param [in] groups：host侧的int64_t， 表示将输入self的channels分成多少组，值需要大于0且要能被self的channels整除。
 * @param [in] out: npu device侧的aclTensor，数据类型和数据维度需要与self一致，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnChannelShuffleGetWorkspaceSize(const aclTensor* self, int64_t groups, aclTensor* out,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnChannelShuffle的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnChannelShuffleGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnChannelShuffle(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CHANNEL_SHUFFLE_H_// End content from: aclnn_channel_shuffle.h

// Begin content from: aclnn_rrelu_with_noise.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RRELU_WITH_NOISE_H_
#define OP_API_INC_RRELU_WITH_NOISE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRReluWithNoise的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnRReluWithNoiseGetWorkspaceSize(const aclTensor* self, const aclTensor* noise,
                                                          const aclScalar* lower, const aclScalar* upper, bool training,
                                                          int64_t seed, int64_t offset, aclTensor* out,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceRReluWithNoise的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnInplaceRReluWithNoiseGetWorkspaceSize(const aclTensor* self, const aclTensor* noise,
                                                                 const aclScalar* lower, const aclScalar* upper,
                                                                 bool training, int64_t seed, int64_t offset,
                                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRReluWithNoise的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnRReluWithNoise(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);

/**
 * @brief aclnnInplaceRReluWithNoise的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceRReluWithNoise(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BINARY_CROSS_ENTROPY_H_
// End content from: aclnn_rrelu_with_noise.h

// Begin content from: aclnn_sub.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SUB_H_
#define OP_API_INC_SUB_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSub的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ output_i = self_i - alpha * other_i $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSubGetWorkspaceSize(const aclTensor* self, const aclTensor* other, const aclScalar* alpha,
                                               aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSub的第二段接口，用于执行计算。
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ output_i = self_i - alpha * other_i $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSubGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSub(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnSubs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnSubsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, const aclScalar* alpha,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
ACLNN_API aclnnStatus aclnnSubs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceSub的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ selfRef_{i} = selfRef_{i} - alpha \times other_{i} $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSubGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other, const aclScalar* alpha,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceSub的第二段接口，用于执行计算。
 *
 * 算子功能：完成减法计算
 * 计算公式：
 * $$ selfRef_{i} = selfRef_{i} - alpha \times other_{i} $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceSubGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSub(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceSubs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceSubsGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other, const aclScalar* alpha,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnInplaceSubs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SUB_H_
// End content from: aclnn_sub.h

// Begin content from: aclnn_binary_cross_entropy.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BINARY_CROSS_ENTROPY_H_
#define OP_API_INC_BINARY_CROSS_ENTROPY_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

enum Reduction { None = 0, Mean = 1, Sum = 2 };

/**
 * @brief aclnnBinaryCrossEntropy的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                              const aclTensor* weight, int64_t reduction,
                                                              aclTensor* out, uint64_t* workspaceSize,
                                                              aclOpExecutor** executor);

/*
 * @brief aclnnBinaryCrossEntropy的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropy(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BINARY_CROSS_ENTROPY_H_
// End content from: aclnn_binary_cross_entropy.h

// Begin content from: aclnn_x_log_y_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_X_LOG_Y_TENSOR_H_
#define OP_API_INC_X_LOG_Y_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnXLogYTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnXLogYTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnXLogYTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnXLogYTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

/**
 * @brief aclnnInplaceXLogYTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceXLogYTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceXLogYTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceXLogYTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_X_LOG_Y_TENSOR_H_// End content from: aclnn_x_log_y_tensor.h

// Begin content from: aclnn_convolution.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CONVOLUTION_H_
#define OP_API_INC_CONVOLUTION_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief convolution接口，计算并获取workspace大小
 * @domain aclnn_ops_infer
 *
 * @param [in] input: npu，feature map
 * device侧的aclTensor，数据类型浮点类型FLOAT16，FLOAT32，FLOAT64
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW
 * @param [in] weight: npu, kernels
 * device侧的aclTensor，数据类型与input一致
 * 支持非连续的Tensor，数据格式与input一致
 * @param [in] bias: npu，偏置
 * device侧的aclTensor，数据类型与input一致
 * 支持非连续的Tensor，数据格式与input一致
 * @param [in] stride: 步长
 * int64的数组，数组长度需等于input的维度-2（也等于kernel size -1），例：2D卷积的步长数组的有效长度是2位
 * @param [in] padding: 补边
 * int64的数组，数组长度需等于input的维度-2（也等于kernel size -1），例：2D卷积的padding数组的有效长度是2位
 * @param [in] dilation: kernel中元素的间隔，>1代表空洞卷积
 * int64的数组，数组长度需等于input的维度-2（也等于kernel size -1），例：2D卷积的dilation数组的有效长度是2位
 * @param [in] transposed: 是否转置
 * bool，True代表转置卷积
 * @param [in] outputPadding：转置卷积时生效，对输出的补边
 * int64的数组，数组长度需等于input的维度-2，值必须分别小于stride或者dilation的最大值，例：2D转置卷积的dilation数组的有效长度是2位
 * @param [in] groups：分组数，表示从输入通道到输出通道的块链接个数
 * int64，大于0且能整除input和output的通道数， input通道数 = weight通道数*groups
 * @param [out] output: npu
 * device侧的aclTensor，数据类型与input一致
 * broadcast之后的shape，数据格式与input一致
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvolutionGetWorkspaceSize(const aclTensor* input, const aclTensor* weight,
                                                       const aclTensor* bias, const aclIntArray* stride,
                                                       const aclIntArray* padding, const aclIntArray* dilation,
                                                       bool transposed, const aclIntArray* outputPadding,
                                                       const int64_t groups, aclTensor* output, int8_t cubeMathType,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnConvTbcGetWorkspaceSize(const aclTensor* self, const aclTensor* weight,
                                                   const aclTensor* bias, const int64_t pad, aclTensor* output,
                                                   int8_t cubeMathType, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnConvDepthwise2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnConvDepthwise2dGetWorkspaceSize(const aclTensor* self, const aclTensor* weight,
                                                           const aclIntArray* kernelSize, const aclTensor* bias,
                                                           const aclIntArray* stride, const aclIntArray* padding,
                                                           const aclIntArray* dilation, aclTensor* out,
                                                           int8_t cubeMathType, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief convolution接口，进行kernellaunch
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由aclnnConvolutionGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。调用该接口后，executor不再可用
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvolution(void* workspace, const uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

ACLNN_API aclnnStatus aclnnConvTbc(void* workspace, const uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

ACLNN_API aclnnStatus aclnnConvDepthwise2d(void* workspace, const uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CONVOLUTION_H_
// End content from: aclnn_convolution.h

// Begin content from: aclnn_split_with_size.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SPLIT_WITH_SIZE_H_
#define OP_API_INC_LEVEL2_ACLNN_SPLIT_WITH_SIZE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSplitWithSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、BFLOAT16、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX128和COMPLEX64。支持非连续的Tensor，数据格式支持ND。
 * @param [in] splitSize：host侧的aclIntArray,
 * 表示需要split的各块大小，数据类型支持INT64和INT32。所有块的大小总和需要等于self 在dim维度上的shape大小。
 * @param [in] dim: host侧的整型，数据类型为INT64，表示输入tensor被split的维度。
 * @param [in] out: npu device侧的aclTensorList，表示被split后的输出tensor的列表，数据类型支持FLOAT、FLOAT16、DOUBLE、
 * BFLOAT16、INT32、INT64、INT16、INT8、UINT8、BOOL、COMPLEX128和COMPLEX64。支持，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSplitWithSizeGetWorkspaceSize(const aclTensor* self, const aclIntArray* splitSize,
                                                         int64_t dim, aclTensorList* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnSplitWithSize的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSplitWithSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSplitWithSize(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SPLIT_WITH_SIZE_H_
// End content from: aclnn_split_with_size.h

// Begin content from: aclnn_foreach_sigmoid.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SIGMOID_H_
#define ACLNN_FOREACH_SIGMOID_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSigmoidGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSigmoidGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSigmoid
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSigmoid(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sigmoid.h

// Begin content from: aclnn_foreach_erf.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ERF_H_
#define ACLNN_FOREACH_ERF_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachErfGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachErfGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachErf
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachErf(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_erf.h

// Begin content from: aclnn_layer_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_LAYER_NORM_H_
#define OP_API_INC_LEVEL2_LAYER_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLayerNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnLayerNormGetWorkspaceSize(const aclTensor* input, const aclIntArray* normalizedShape,
                                                     const aclTensor* weightOptional, const aclTensor* biasOptional,
                                                     double eps, aclTensor* out, aclTensor* meanOutOptional,
                                                     aclTensor* rstdOutOptional, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnLayerNormWithImplMode的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnLayerNormWithImplModeGetWorkspaceSize(
    const aclTensor* input, const aclIntArray* normalizedShape, const aclTensor* weightOptional,
    const aclTensor* biasOptional, double eps, aclTensor* out, aclTensor* meanOutOptional, aclTensor* rstdOutOptional,
    int32_t implMode, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLayerNorm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLayerNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnLayerNormWithImplMode的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLayerNormWithImplMode(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_LAYER_NORM_H_
// End content from: aclnn_layer_norm.h

// Begin content from: aclnn_max_unpool2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_UNPOOL2d_BACKWARD_H_
#define OP_API_INC_MAX_UNPOOL2d_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxUnpool2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnMaxUnpool2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                               const aclTensor* indices, const aclIntArray* outputSize,
                                                               aclTensor* out, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);

/**
 * @brief aclnnMaxUnpool2dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMaxUnpool2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_UNPOOL2d_BACKWARD_H_
// End content from: aclnn_max_unpool2d_backward.h

// Begin content from: aclnn_bitwise_not.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BITWISE_NOT_H_
#define OP_API_INC_BITWISE_NOT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBitwiseNot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对输入的tensor进行逻辑“非”或者按位非操作

 * 计算公式：
 * $$ output_i = \lnot self_i $$
 *
 * 实现说明:
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Invert])
 *     C --> D([ViewCopy])
 *     D --> E([out])
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor, 数据类型支持INT16,INT32,INT64,INT8,UINT8,BOOL,支持非连续的Tensor,数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor, 数据类型支持INT16,INT32,INT64,INT8,UINT8,BOOL,且数据类型需要是self可转换的数据类型,
 * shape与self相同, 数据格式支持ND, 且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseNotGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);
/**
 * @brief aclnnBitwiseNot的第二段接口，用于执行计算。
 *
 * 算子功能：对输入的tensor进行逻辑“非”或者按位非操作
 * 计算公式：
 * $$ output_i = \lnot self_i $$
 *
 * 实现说明:
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Invert])
 *     C --> D([ViewCopy])
 *     D --> E([out])
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnBitwiseNotGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseNot(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BITWISE_NOT_H_// End content from: aclnn_bitwise_not.h

// Begin content from: aclnn_sigmoid.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SIGMOID_H_
#define OP_API_INC_SIGMOID_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSigmoid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成sigmoid操作
 * @param [in] self: npu device侧的aclTensor, 数据类型支持浮点类型，shape为非空，支持非连续的Tensor，数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持浮点类型, shape与self保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSigmoidGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief: aclnnSigmoid的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sigmoid操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSigmoidGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSigmoid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   const aclrtStream stream);

/**
 * @brief aclnnInplaceSigmoid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成sigmoid操作
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型支持浮点类型，shape为非空，支持非连续的Tensor，数据格式支持ND、NCHW、
 * NHWC、支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceSigmoidGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                          aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceSigmoid的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成sigmoid操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSigmoidGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSigmoid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SIGMOID_H_// End content from: aclnn_sigmoid.h

// Begin content from: aclnn_lgamma.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_LGAMMA_H_
#define OP_API_INC_LEVEL2_ACLNN_LGAMMA_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACLNN_MAX_SHAPE_RANK 8

/**
 * @brief aclnnLgamma的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素绝对值的结果
 * 计算公式：
 * $$ out_{i} = lgamma(self_{i}) $$ *
 * @param [in] self: 待进行lgamma计算的入参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE，数据格式支持ND，支持非连续的Tensor。
 * @param [in] out: lgamma计算的出参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE，数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLgammaGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnLgamma的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLgammaGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLgamma(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_LGAMMA_H_// End content from: aclnn_lgamma.h

// Begin content from: aclnn_foreach_log10.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LOG10_H_
#define ACLNN_FOREACH_LOG10_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLog10GetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog10GetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLog10
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog10(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_log10.h

// Begin content from: aclnn_addbmm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADDBMM_H_
#define OP_API_INC_ADDBMM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddbmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：首先进行batch1、batch2的矩阵乘计算，然后将该结果按照第一维（batch维度）批处理相加，将三维向量
 * 压缩为二维向量（shape大小为后两维的shape），然后该结果与α作乘积计算，再与β和self的乘积求和得到结果
 * 计算公式：
 * $$ out = βself+α(\sum_{i=0}^{b-1}batch1_{i}@batch2_{i}) $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且需要与batch1@batch2的数据类型需满足数据类型推导规则，
 * shape需要与batch1@batch2的后两维满足broadcast关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] batch1: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch2的数据类型需满足数据类型推导规则,
 * shape需要与batch2满足bmm输入约束关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] batch2: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch1的数据类型需满足数据类型推导规则
 * shape需要与batch1满足bmm输入约束关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] beta: host侧的aclScalar，数据类型需要可转换成self与batch1@batch2推导后的数据类型。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与batch1@batch2推导后的数据类型。
 * @param [in] cubeMathType:
 * INT8类型的枚举值，用于判断Cube单元应该使用那种计算逻辑进行运算，可通过此开关使能如HFLOAT32等功能
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与self保持一致，shape要求与batch1@batch2的后两维保持一致。
 * 支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddbmmGetWorkspaceSize(const aclTensor* self, const aclTensor* batch1,
                                                  const aclTensor* batch2, const aclScalar* beta,
                                                  const aclScalar* alpha, aclTensor* out, int8_t cubeMathType,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAddbmm的第二段接口，用于执行计算。
 *
 * 算子功能：首先进行batch1、batch2的矩阵乘计算，然后将该结果按照第一维（batch维度）批处理相加，将三维向量
 * 压缩为二维向量（shape大小为后两维的shape），然后该结果与α作乘积计算，再与β和self的乘积求和得到结果
 * 计算公式：
 * $$ out = βself+α(\sum_{i=0}^{b-1}batch1_{i}@batch2_{i}) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnBaddbmmGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAddbmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAddbmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：首先进行batch1、batch2的矩阵乘计算，然后将该结果按照第一维（batch维度）批处理相加，将三维向量
 * 压缩为二维向量（shape大小为后两维的shape），然后该结果与α作乘积计算，再与β和selfRef的乘积求和得到结果
 * 计算公式：
 * $$ selfRef = βselfRef+α(\sum_{i=0}^{b-1}batch1_{i}@batch2_{i}) $$
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且需要与batch1@batch2的数据类型需满足数据类型推导规则，
 * shape需要与batch1@batch2的后两维满足broadcast关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] batch1: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch2的数据类型需满足数据类型推导规则,
 * shape需要与batch2满足bmm输入约束关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] batch2: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch1的数据类型需满足数据类型推导规则
 * shape需要与batch1满足bmm输入约束关系。支持非连续的Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [in] beta: host侧的aclScalar，数据类型需要可转换成self与batch1@batch2推导后的数据类型。
 * @param [in] alpha: host侧的aclScalar，数据类型需要可转换成self与batch1@batch2推导后的数据类型。
 * @param [in] cubeMathType:
 * INT8类型的枚举值，用于判断Cube单元应该使用那种计算逻辑进行运算，可通过此开关使能如HFLOAT32等功能
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddbmmGetWorkspaceSize(aclTensor* selfRef, const aclTensor* batch1,
                                                         const aclTensor* batch2, const aclScalar* beta,
                                                         const aclScalar* alpha, int8_t cubeMathType,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAddbmm的第二段接口，用于执行计算。
 *
 * 算子功能：首先进行batch1、batch2的矩阵乘计算，然后将该结果按照第一维（batch维度）批处理相加，将三维向量
 * 压缩为二维向量（shape大小为后两维的shape），然后该结果与α作乘积计算，再与β和selfRef的乘积求和得到结果
 * 计算公式：
 * $$ selfRef = βselfRef+α(\sum_{i=0}^{b-1}batch1_{i}@batch2_{i}) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddbmmGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAddbmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BADDBMM_H_
// End content from: aclnn_addbmm.h

// Begin content from: aclnn_argsort.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ARGSORT_H_
#define OP_API_INC_ARGSORT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnArgsort的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnArgsortGetWorkspaceSize(const aclTensor* self, int64_t dim, bool descending, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnArgsort的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnArgsort(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ARGSORT_H_// End content from: aclnn_argsort.h

// Begin content from: aclnn_isposinf.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ISPOSINF_H_
#define OP_API_INC_ISPOSINF_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIsPosInf的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入张量的元素是否为正无穷。
 * 实现说明-计算图
 * 计算图：如下
 * 场景1：浮点数场景
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0op::Contiguous])
 *   B --> D([l0op::IsPosInf])
 *   D --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 * 场景2：非浮点数，有界，返回False
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0op::Fill])
 *   B --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 *
 * @param [in] self：输入`self`,数据类型支持FLOAT、FLOAT16、BFLOAT16(Ascend910B),
 * INT32、INT64、INT16、INT8、UINT8、BOOL。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] out：输入`out`,数据类型支持BOOL，shape需要与self一致。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsPosInfGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                    aclOpExecutor** executor);

/**
 * @brief aclnnIsPosInf的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnIsPosInfGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsPosInf(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                    const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ISPOSINF_H_
// End content from: aclnn_isposinf.h

// Begin content from: aclnn_max_dim.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_DIM_H_
#define OP_API_INC_MAX_DIM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回张量在指定维度上的最大值的索引。
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMaxWithValue])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [in] dim: host侧int64类型，指定了要进行最大值计算的维度。
 * @param [in] keepdim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持INT32、INT64。数据格式支持ND。支持非连续的Tensor。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxDimGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepdim,
                                                  aclTensor* out, aclTensor* indices, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnArgMax的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：返回张量在指定维度上的最大值的索引。
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMaxWithValue])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnArgMaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_DIM_H_// End content from: aclnn_max_dim.h

// Begin content from: aclnn_grouped_bias_add_grad.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_BIAS_ADD_GRAD_H_
#define OP_API_INC_GROUPED_BIAS_ADD_GRAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupedBiasAddGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：实现groupBiasAdd的反向计算。
 * @param [in] gradY: 反向传播梯度，公式中的gradY，Device侧的aclTensor，数据类型支持FLOAT,FLOAT16,BFLOAT16。
 *                    有可选输入groupIdxOptional时，shape仅支持2维，无可选输入groupIdxOptional时，shape仅支持3维。
 * @param [in] groupIdxOptional: 每个分组结束位置，公式中的groupIdxOptional，Device侧的aclTensor，数据类型支持INT32，INT64，shape仅支持1维。
 * @param [out] out: bias的梯度，公式中的out，Device侧的aclTensor，数据类型支持FLOAT,FLOAT16,BFLOAT16，数据类型必须与gradY的数据类型一致，shape仅支持2维。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedBiasAddGradGetWorkspaceSize(const aclTensor *gradY,
                                                    const aclTensor *groupIdxOptional, aclTensor *out,
                                                    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnGroupedBiasAddGrad的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGroupedBiasAddGradGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedBiasAddGrad(void *workspace, uint64_t workspaceSize,
                                              aclOpExecutor *executor, aclrtStream stream);

/**
 * @brief aclnnGroupedBiasAddGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：实现groupBiasAdd的反向计算。
 * @param [in] gradY: 反向传播梯度，公式中的gradY，Device侧的aclTensor，数据类型支持FLOAT,FLOAT16,BFLOAT16。
 *                    有可选输入groupIdxOptional时，shape仅支持2维，无可选输入groupIdxOptional时，shape仅支持3维。
 * @param [in] groupIdxOptional: 每个分组结束位置，公式中的groupIdxOptional，Device侧的aclTensor，数据类型支持INT32，INT64，shape仅支持1维。
 * @param [in] groupIdxType: 表示groupIdx的类型。支持的值为0和1，
 * 0：表示groupIdxOptional中的值为每个group的结束索引，1：表示groupIdxOptional中的值为每个group的大小。数据类型支持Int64。
 * @param [out] out: bias的梯度，公式中的out，Device侧的aclTensor，数据类型支持FLOAT,FLOAT16,BFLOAT16，数据类型必须与gradY的数据类型一致，shape仅支持2维。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedBiasAddGradV2GetWorkspaceSize(const aclTensor *gradY, const aclTensor *groupIdxOptional,
                                                    int64_t groupIdxType, aclTensor *out,
                                                    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnGroupedBiasAddGrad的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGroupedBiasAddGradV2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedBiasAddGradV2(void *workspace, uint64_t workspaceSize,
                                              aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_grouped_bias_add_grad.h

// Begin content from: aclnn_index_copy.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/*!
 * \\file aclnn_index_copy.h
 * \\brief
 */
#ifndef OP_API_INC_INDEX_COPY_H_
#define OP_API_INC_INDEX_COPY_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnIndexCopy第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将index张量中元素值作为索引，针对指定轴dim，把source中元素复制到outRef的对应位置上。outRef中的其他元素从selfRef对应位置复制
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，输入数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT64、INT16、INT8、UINT8、
 * DOUBLE、BOOL、COMPLEX128、COMPLEX64，支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] dim: host侧的int64类型。
 * @param [in] index: npu device侧的aclTensor，数据类型支持INT32、INT64,维度不能大于1，元素个数要求和张量source中
 * dim轴的大小相同。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] source: npu
 * device侧的aclTensor，数据类型和selfRef一致,。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] outRef: npu device侧的aclTensor, 数据类型、数据格式与selfRef相同。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexCopyGetWorkspaceSize(aclTensor* selfRef, int64_t dim, const aclTensor* index,
                                                     const aclTensor* source, aclTensor* outRef,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIndexCopy的第二段接口，用于执行计算。
 *
 * 算子功能：将index张量中元素值作为索引，针对指定轴dim，把source中元素复制到outRef的对应位置上。并把其余元素从selfRef复制到outRef
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceIndexCopyGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexCopy(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnInplaceIndexCopy第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将index张量中元素值作为索引，针对指定轴dim，把source中元素复制到selfRef的对应位置上。
 *
 * @param [in] selfRef: npu
 device侧的aclTensor，输入数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT64、INT16、INT8、UINT8、
 * DOUBLE、BOOL、COMPLEX128、COMPLEX64，支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] dim: host侧的int64类型。
 * @param [in] index: npu device侧的aclTensor，数据类型支持INT32、INT64,维度不能大于1，元素个数要求和张量source中
 * dim轴的大小相同。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] source: npu
 device侧的aclTensor，数据类型和selfRef一致,。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。

 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceIndexCopyGetWorkspaceSize(aclTensor* selfRef, int64_t dim, const aclTensor* index,
                                                            const aclTensor* source, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnInplacePut的第二段接口，用于执行计算。
 *
 * 算子功能：将index张量中元素值作为索引，针对指定轴dim，把source中元素复制到selfRef的对应位置上。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceIndexCopyGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceIndexCopy(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INDEX_COPY_H_// End content from: aclnn_index_copy.h

// Begin content from: aclnn_foreach_addcmul_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_ADDCMUL_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_ADDCMUL_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachAddcmulScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入对张量列表x2和张量列表x3执行逐元素乘法，将结果乘以标量值scalar后将结果与张量列表x1执行逐元素加法。
 * 计算公式：
 * {\rm out}_i = x1_i + {\rm scalar} × x2_i × x3_i
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、INT32。数据格式支持ND。
  * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、INT32。数据格式支持ND。
  * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、INT32。数据格式支持ND。
  * @param [in]  input
 * 输入Scalar，数据类型支持FLOAT、FLOAT16、INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachAddcmulScalarV2GetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachAddcmulScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入对张量列表x2和张量列表x3执行逐元素乘法，将结果乘以标量值scalar后将结果与张量列表x1执行逐元素加法。
 * 计算公式：
 * {\rm out}_i = x1_i + {\rm scalar} × x2_i × x3_i
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachAddcmulScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachAddcmulScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcmul_scalar_v2.h

// Begin content from: aclnn_moe_token_unpermute_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_TOKEN_UNPERMUTE_GRAD_H_
#define ACLNN_MOE_TOKEN_UNPERMUTE_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeTokenUnpermuteGradGetWorkspaceSize
 * parameters :
 * permutedTokens : required
 * unpermutedTokensGrad : required
 * sortedIndices : required
 * probsOptional : optional
 * paddedMode : optional
 * restoreShapeOptional : optional
 * permutedTokensGradOut : required
 * probsGradOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenUnpermuteGradGetWorkspaceSize(
    const aclTensor *permutedTokens,
    const aclTensor *unpermutedTokensGrad,
    const aclTensor *sortedIndices,
    const aclTensor *probsOptional,
    bool paddedMode,
    const aclIntArray *restoreShapeOptional,
    const aclTensor *permutedTokensGradOut,
    const aclTensor *probsGradOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeTokenUnpermuteGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenUnpermuteGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_token_unpermute_grad.h

// Begin content from: aclnn_repeat_interleave.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_REPEAT_INTERLEAVE_H_
#define OP_API_INC_LEVEL2_ACLNN_REPEAT_INTERLEAVE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

// 无dim, tensor repeats
/**
 * @brief aclnnRepeatInterleave的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] self: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、BFLOAT16类型。
 * 支持空tensor, 支持非连续的tensor。数据格式支持ND。
 * @param [in] repeats: npu device侧的aclTensor。数据类型支持INT64。repeats只能为0D / 1D Tensor。如果为1D Tensor，那么
 * repeats的size必须为1或self的元素个数。支持空tensor，支持非连续的tensor。数据格式支持ND。
 * @param [in] outputSize: 进行重复后的tensor最终大小。数据类型为INT64。当repeats中只有一个元素时，outputSize =
 * self的元素 个数 * repeats的值。当repeats中有多个值时，outputSize = repeats的值之和。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32类型。数据
 * 类型需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveGetWorkspaceSize(const aclTensor* self, const aclTensor* repeats,
                                                            int64_t outputSize, aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief: aclnnRepeatInterleave的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRepeatInterleaveGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRepeatInterleave(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

// 有dim, tensor repeats
/**
 * @brief aclnnRepeatInterleaveWithDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] self: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、BFLOAT16类型。
 * 支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] repeats: npu device侧的aclTensor。数据类型支持INT64。repeats只能为0D / 1D tensor。如果为1D tensor，那么
 * repeats的size必须为1或self的dim维度的size。支持空tensor，支持非连续的tensor。数据格式支持ND。
 * @param [in] dim: 进行重复的维度，数据类型为INT64。范围为[-self的维度数量, self的维度数量-1]。
 * @param [in] outputSize:
 * dim维度在进行重复后的最终大小。数据类型为INT64。如果repeats中有多个值，则outputSize值必须为repeats
 * 的求和。如果repeats只有一个元素时，则outputSize值必须为repeats * self的dim维度size。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32类型。数据
 * 类型需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveWithDimGetWorkspaceSize(const aclTensor* self, const aclTensor* repeats,
                                                                   int64_t dim, int64_t outputSize, aclTensor* out,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnRepeatInterleaveWithDim的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnRepeatInterleaveWithDimGetWorkspaceSize 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveWithDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

// 无dim, int repeats
/**
 * @brief aclnnRepeatInterleaveInt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] self: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、BFLOAT16类型。
 * 支持空tensor, 支持非连续的tensor。数据格式支持ND。
 * @param [in] repeats: 重复的次数。数据类型为INT64。repeats的值必须为自然数。
 * @param [in] outputSize: 进行重复后的tensor最终大小。数据类型为INT64。outputSize必须等于self的元素个数 * repeats的值。
 * @param [in] out: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32类型。
 * 数据类型需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveIntGetWorkspaceSize(const aclTensor* self, int64_t repeats,
                                                               int64_t outputSize, aclTensor* out,
                                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnRepeatInterleaveInt的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRepeatInterleaveIntGetWorkspaceSize
 * 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveInt(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

// 有dim, int repeats
/**
 * @brief aclnnRepeatInterleaveIntWithDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] self: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、BFLOAT16类型。
 * 支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] repeats: 重复的次数。数据类型为INT64。repeats的值必须为自然数。
 * @param [in] dim: 进行重复的维度，数据类型为INT64。范围为[-self的维度数量, self的维度数量-1]。
 * @param [in] outputSize: dim维度在进行重复后的最终大小。数据类型为INT64。outputSize值必须为repeats *
 * self的dim维度size。
 * @param [in] out: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32类型。
 * 数据类型需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveIntWithDimGetWorkspaceSize(const aclTensor* self, int64_t repeats,
                                                                      int64_t dim, int64_t outputSize, aclTensor* out,
                                                                      uint64_t* workspaceSize,
                                                                      aclOpExecutor** executor);

/**
 * @brief: aclnnRepeatInterleaveIntWithDim的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口
 * aclnnRepeatInterleaveIntWithDimGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveIntWithDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                      aclrtStream stream);

// repeat_interleave.Tensor
/**
 * @brief aclnnRepeatInterleaveTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] repeats: npu device侧的aclTensor。数据类型支持INT32、INT64。repeats只能为1D Tensor
 * (包括shape=[0,]的场景)。 支持非连续的tensor。数据格式支持ND。
 * @param [in] outputSize: 进行重复后的tensor最终大小。数据类型为INT64。outputSize必须等于repeats的值之和。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT64类型。数据类型需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveTensorGetWorkspaceSize(const aclTensor* repeats, int64_t outputSize,
                                                                  aclTensor* out, uint64_t* workspaceSize,
                                                                  aclOpExecutor** executor);

/**
 * @brief: aclnnRepeatInterleaveTensor的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成repeatinterleave操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRepeatInterleaveTensorGetWorkspaceSize
 * 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRepeatInterleaveTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_REPEAT_INTERLEAVE_H_// End content from: aclnn_repeat_interleave.h

// Begin content from: aclnn_logical_xor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LogicalXor_H_
#define OP_API_INC_LogicalXor_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogicalXor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成给定输入张量元素的逻辑异或运算。当self和other为非bool类型时，0被视为False，非0被视为True。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalXor])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalXor])
 * D([LogicalXor])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且shape需要是self与other broadcast之后的shape，数据格式支持ND,
 * 且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalXorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogicalXor的第二段接口，用于执行计算。
 *
 * 算子功能：完成给定输入张量元素的逻辑异或运算。当self和other为非bool类型时，0被视为False，非0被视为True。
 *
 * 实现说明：
 * api计算的基本路径:
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalXor])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalXor])
 * D([LogicalXor])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalXorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalXor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LogicalXor_H_// End content from: aclnn_logical_xor.h

// Begin content from: aclnn_abs.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_ABS_H_
#define OP_API_INC_LEVEL2_ACLNN_ABS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACLNN_MAX_SHAPE_RANK 8

/**
 * @brief aclnnAbs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素绝对值的结果
 * 计算公式：
 * $$ out_{i} = abs(self_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)]--->B([l0op::Contiguous])
 *   B--->C([l0op::Abs])
 *   C--->D([l0op::ViewCopy])
 *   D--->E[(out)]
 * ```
 *
 * @param [in] self: 待进行abs计算的入参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [in] out: abs计算的出参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAbsGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnAbs的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAbsGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnAbs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ABS_H_// End content from: aclnn_abs.h

// Begin content from: aclnn_upsample_bilinear_2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_BILINEAR_H_
#define OP_API_INC_UNAMPLE_BILINEAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBilinear2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

/**
 * @brief aclnnUpsampleBilinear2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                              const bool alignCorners, const double scalesH,
                                                              const double scalesW, aclTensor* out,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_BILINEAR_H_// End content from: aclnn_upsample_bilinear_2d.h

// Begin content from: aclnn_upsample_nearest_exact3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_EXACT3D_H_
#define OP_API_INC_UNAMPLE_NEAREST_EXACT3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearestExact3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             double scalesD, double scalesH, double scalesW,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearestExact3d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_EXACT3D_H_
// End content from: aclnn_upsample_nearest_exact3d.h

// Begin content from: aclnn_foreach_div_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_DIV_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_DIV_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachDivScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素x除以标量值scalar后输出。
 * 计算公式：
 * out_{i}=x_{i}/scalar
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachDivScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachDivScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素x除以标量值scalar后输出。
 * 计算公式：
 * out_{i}=x_{i}/scalar
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachDivScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachDivScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_div_scalar_v2.h

// Begin content from: aclnn_triangular_solve.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRIANGULAR_SOLVE_H_
#define OP_API_INC_TRIANGULAR_SOLVE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTriangularSolve的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor求解AX=b
 * @param [in] self: 公式中的b, 数据类型支持FLOAT、DOUBLE、COMPLEX64，COMPLEX128，数据维度至少为2，且不大于8，
 * 支持非连续的Tensor，数据格式支持ND，且shape(*,m,n)需要与A(*,m,m)除最后两维外满足broadcast关系。
 * @param [in] A: 公式中的A, 数据类型支持FLOAT、DOUBLE、COMPLEX64，COMPLEX128，数据维度至少为2，且不大于8，
 * 支持非连续的Tensor，数据格式支持ND，且shape(*,m,n)需要与self(*,m,n)除最后两维外满足broadcast关系。
 * @param [in] upper: 计算属性，默认为true，A为上三角方阵，当upper为false时，A为下三角方阵
 * @param [in] transpose: 计算属性，默认为false，当transpose为true时，计算ATX=B,AT为A的转置
 * @param [in] unitriangular: 计算属性，默认为false，当unitriangular为true时，A的对角线元素视为1，而不是从A引用
 * @param [out] xOut: 公式中的X，数据类型支持FLOAT、DOUBLE、COMPLEX64，COMPLEX128，数据维度至少为2，且不大于8，
 * 支持非连续的Tensor，数据格式支持ND，且shape需要与broadcast后的A,b满足AX=b约束
 * @param [out] mOut: broadcast后A的上三角（下三角）拷贝，数据类型支持FLOAT、DOUBLE、COMPLEX64，COMPLEX128，
 * 数据维度至少为2，且不大于8，支持非连续的Tensor，数据格式支持ND，且shape需要与broadcast后的A一致
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnTriangularSolveGetWorkspaceSize(const aclTensor* self, const aclTensor* A, bool upper,
                                                           bool transpose, bool unitriangular, aclTensor* xOut,
                                                           aclTensor* mOut, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnTriangularSolve的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成求解操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTriangularSolveGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTriangularSolve(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRIANGULAR_SOLVE_H_// End content from: aclnn_triangular_solve.h

// Begin content from: aclnn_softplus_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFTPLUS_BACKWARD_H_
#define OP_API_INC_SOFTPLUS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftplusBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnSoftplusBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                            const aclScalar* beta, const aclScalar* threshold,
                                                            aclTensor* gradInput, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnSoftplusBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSoftplusBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFTPLUS_BACKWARD_H_
// End content from: aclnn_softplus_backward.h

// Begin content from: aclnn_circular_pad3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CIRCULAR_PAD3D_BACKWARD_H_
#define OP_API_INC_CIRCULAR_PAD3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnCircularPad3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：循环填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, BFLOAT16, 数据格式支持ND，
 * 维度支持三维或四维且与self和gradInput一致，shape需要与circular_pad3d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持四维或五维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为6，数值依次代表左右上下前后需要填充的值。
 * 前两个数值需小于self最后一维度的数值，中间两个数值需小于self倒数第二维度的数值，后两个数值需小于self倒数第三维度的数值。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持四维或五维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad3dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                   const aclIntArray* padding, aclTensor* gradInput,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnCircularPad3dBackward的第二段接口，用于执行计算
 *
 * 算子功能：循环填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnCircularPad3dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CIRCULAR_PAD3D_BACKWARD_H_// End content from: aclnn_circular_pad3d_backward.h

// Begin content from: aclnn_leaky_relu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEAKY_RELU_H_
#define OP_API_INC_LEAKY_RELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLeakyRelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND。
 * @param [in] negativeSlope: host侧的aclScalar，表示self<0时的斜率。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE，且数据类型与self一致，shape与self相同。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeakyReluGetWorkspaceSize(const aclTensor* self, const aclScalar* negativeSlope,
                                                     aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLeakyRelu的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLeakyReluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeakyRelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnInplaceLeakyRelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数。
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND。
 * @param [in] negativeSlope: host侧的aclScalar，表示self<0时的斜率。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeakyReluGetWorkspaceSize(aclTensor* selfRef, const aclScalar* negativeSlope,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLeakyRelu的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLeakyReluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeakyRelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEAKY_RELU_H_// End content from: aclnn_leaky_relu.h

// Begin content from: aclnn_foreach_pow_scalar_and_tensor.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_POW_SCALAR_AND_TENSOR_H_
#define ACLNN_FOREACH_POW_SCALAR_AND_TENSOR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachPowScalarAndTensorGetWorkspaceSize
 * parameters :
 * scalar : required
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalarAndTensorGetWorkspaceSize(
    const aclScalar *scalar,
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachPowScalarAndTensor
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalarAndTensor(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_pow_scalar_and_tensor.h

// Begin content from: aclnn_scatter_nd.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_SCATTER_ND_H_
#define OP_API_INC_SCATTER_ND_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScatterNd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnScatterNdGetWorkspaceSize(const aclTensor* data, const aclTensor* indices,
                                                     const aclTensor* updates, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnScatterNd的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnScatterNd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SCATTER_ND_H_// End content from: aclnn_scatter_nd.h

// Begin content from: aclnn_reduce_sum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REDUCE_SUM_H_
#define OP_API_INC_REDUCE_SUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnReduceSum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnReduceSumGetWorkspaceSize(const aclTensor* self, const aclIntArray* dims, bool keepDims,
                                                     aclDataType dtype, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnReduceSum的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnReduceSum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REDUCE_SUM_H_
// End content from: aclnn_reduce_sum.h

// Begin content from: aclnn_moe_init_routing_v2_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_INIT_ROUTING_V2GRAD_H_
#define ACLNN_MOE_INIT_ROUTING_V2GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeInitRoutingV2GradGetWorkspaceSize
 * parameters :
 * gradExpandedX : required
 * expandedRowIdx : required
 * topK : required
 * dropPadMode : optional
 * activeNum : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingV2GradGetWorkspaceSize(
    const aclTensor *gradExpandedX,
    const aclTensor *expandedRowIdx,
    int64_t topK,
    int64_t dropPadMode,
    int64_t activeNum,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeInitRoutingV2Grad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingV2Grad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_init_routing_v2_grad.h

// Begin content from: aclnn_index_select.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_INDEX_SELECT_H_
#define OP_API_INC_LEVEL2_ACLNN_INDEX_SELECT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIndexSelect的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素向下取整的结果
 * 计算公式：
 * 以输入为三维张量为例：
 *   x=$\begin{bmatrix}[[1,&2],&[3,&4]], \\ [[5,&6],&[7,&8]], \\ [[9,&10],&[11,&12]]\end{bmatrix}$
 *   idx=[1, 0],
 * dim为0, index_select(0, idx)：   I=index[i];  &nbsp;&nbsp;   y$[i][m][n]$ = x$[I][m][n]$
 * dim为1, index_select(1, idx)：   J=index[j];  &nbsp;&nbsp;&nbsp;    y$[l][j][n]$ = x$[l][J][n]$
 * dim为2, index_select(2, idx)：   K=index[k]; &nbsp;  y$[l][m][k]$ = x$[l][m][K]$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(Self)] -->B([l0op::Contiguous])
 *   B --> In_0([l0op::cast]) --> Op([GatherV2])
 *   In_1[(index)] --> con([l0op::Contiguous])--> Op
 *   In_2(dim) --> a(dimVec) --> Op
 *   Op --> C([l0op::cast]) --> D([l0op::ViewCopy])  --> Out[(out)]
 * ```

 *
 * @param [in] self: 原始张量。npu device侧的aclTensor，
 *
 数据类型支持FLOAT、FLOAT16、BFLOAT16、INT64、INT32、INT16、INT8、UINT8、BOOL、DOUBLE、COMPLEX64、COMPLEX128，数据格式支持ND，支持非连续的Tensor。
 * @param [in] dim: host侧的int64。
 * @param [in] index: indecies，切片用，npu device侧的aclTensor。
 * 数据类型支持INT64、INT32，数据格式支持ND，维度是0D或1D。 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexSelectGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* index,
                                                       aclTensor* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnIndexSelect的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnIndexSelectGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexSelect(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_INDEX_SELECT_H_
// End content from: aclnn_index_select.h

// Begin content from: aclnn_searchsorted.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SEARCHSORTED_H_
#define OP_API_INC_LEVEL2_ACLNN_SEARCHSORTED_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSearchSorted的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 在一个已排序的张量（sortedSequence）中查找给定值（self）应该插入的位置。
 * @param [in] sortedSequence: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64,
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64,
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] outInt32: host侧的BOOL类型，指定输出Tensor是否维INT32类型。
 * @param [in] right: host侧的BOOL类型，如果找到的是相同值，是否返回右侧位置。
 * @param [in] sorter: npu device侧的aclTensor, 数据类型支持INT64, 指定sortedSequence中元素的顺序。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64,
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSearchSortedGetWorkspaceSize(const aclTensor* sortedSequence, const aclTensor* self,
                                                        const bool outInt32, const bool right, const aclTensor* sorter,
                                                        aclTensor* out, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnSearchSorted的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能： 在一个已排序的张量（sortedSequence）中查找给定值（self）应该插入的位置。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSearchSortedGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSearchSorted(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

/**
 * @brief aclnnSearchSorteds的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 在一个已排序的张量（sortedSequence）中查找给定值（self）应该插入的位置。
 * @param [in] sortedSequence: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64,
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] self: npu device侧的aclScalar, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64。
 * @param [in] outInt32: host侧的BOOL类型，指定输出Tensor是否维INT32类型。
 * @param [in] right: host侧的BOOL类型，如果找到的是相同值，是否返回右侧位置。
 * @param [in] sorter: npu device侧的aclTensor, 数据类型支持INT64, 指定sortedSequence中元素的顺序。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8, INT16, INT64,
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSearchSortedsGetWorkspaceSize(const aclTensor* sortedSequence, const aclScalar* self,
                                                         const bool outInt32, const bool right, const aclTensor* sorter,
                                                         aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnSearchSorteds的第二段接口，用于执行计算。
 *
 * 算子功能： 在一个已排序的张量（sortedSequence）中查找给定值（self）应该插入的位置。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSearchSortedsGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSearchSorteds(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SEARCHSORTED_H_
// End content from: aclnn_searchsorted.h

// Begin content from: aclnn_foreach_norm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_NORM_H_
#define ACLNN_FOREACH_NORM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachNormGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachNormGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachNorm
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachNorm(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_norm.h

// Begin content from: aclnn_leaky_relu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEAKY_RELU_BACKWARD_H_
#define OP_API_INC_LEAKY_RELU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnLeakyReluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：激活函数反向。
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE，且数据类型与self一致，shape与self相同。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND[（参考）](#参考)。
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND[（参考）](#参考)。
 * @param [in] negativeSlope: host侧的aclScalar，表示self<0时的斜率。
 * @param [in] selfIsResult: host侧的bool，表示self是否做为输出。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND[（参考）](#参考)。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeakyReluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                             const aclScalar* negativeSlope, bool selfIsResult,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);
/**
 * @brief aclnnLeakyReluBackward的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数反向。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLeakyReluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeakyReluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEAKY_RELU_BACKWARD_H_// End content from: aclnn_leaky_relu_backward.h

// Begin content from: aclnn_mish.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MISH_H_
#define OP_API_INC_MISH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMish的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：一个自正则化的非单调神经网络激活函数。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT16、BFLOAT16、FLOAT。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、BFLOAT16、FLOAT。它的shape与self相同，且数据类型需要与self一致，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMishGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnMish的第二段接口，用于执行计算。
 *
 * 算子功能：一个自正则化的非单调神经网络激活函数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMishGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMish(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceMish的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：一个自正则化的非单调神经网络激活函数。
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT16、BFLOAT16、FLOAT。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMishGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceMish的第二段接口，用于执行计算。
 *
 * 算子功能：一个自正则化的非单调神经网络激活函数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceMishGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMish(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MISH_H_
// End content from: aclnn_mish.h

// Begin content from: aclnn_minimum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MINIMUM_H_
#define OP_API_INC_MINIMUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMinimum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算两个张量中每个元素的最小值，并返回一个新的张量。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinimumGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMinimum的第二段接口，用于执行计算。
 *
 * 算子功能：计算两个张量中每个元素的最小值，并返回一个新的张量。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMinimumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinimum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MINIMUM_H_
// End content from: aclnn_minimum.h

// Begin content from: aclnn_arange.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ARANGE_H_
#define OP_API_INC_ARANGE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnArange的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 功能描述：从start到end按照step的间隔获取每个值，保存到输出1维张量。
 */
ACLNN_API aclnnStatus aclnnArangeGetWorkspaceSize(const aclScalar* start, const aclScalar* end, const aclScalar* step,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
/* @brief aclnnArange的第二段接口，用于执行计算。 */
ACLNN_API aclnnStatus aclnnArange(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_arange.h

// Begin content from: aclnn_moe_compute_expert_tokens.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_COMPUTE_EXPERT_TOKENS_H_
#define ACLNN_MOE_COMPUTE_EXPERT_TOKENS_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeComputeExpertTokensGetWorkspaceSize
 * parameters :
 * sortedExperts : required
 * numExperts : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeComputeExpertTokensGetWorkspaceSize(
    const aclTensor *sortedExperts,
    int64_t numExperts,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeComputeExpertTokens
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeComputeExpertTokens(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_compute_expert_tokens.h

// Begin content from: aclnn_floor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_FLOOR_H_
#define OP_API_INC_LEVEL2_ACLNN_FLOOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFloor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素向下取整的结果
 * 计算公式：
 * $$ out_{i} =floor(self_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::Floor])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: 待进行floor计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [in] out: floor计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与self一致， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFloorGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnFloor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnFloorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFloor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceFloor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND， 数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFloorGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFloor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceFloorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFloor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FLOOR_H_
// End content from: aclnn_floor.h

// Begin content from: aclnn_tan.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TAN_H_
#define OP_API_INC_TAN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTan的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入张量self中每个元素的正切值，并将结果存储在张量out中
 * 计算公式：$$ out[i] = tan(self[i]) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B --> C([l0op::Tan])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持 FLOAT、BFLOAT16、FLOAT16、INT32、DOUBLE、COMPLEX64、COMPLEX128, 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，
 * 数据类型支持 FLOAT、BFLOAT16、FLOAT16、INT32、DOUBLE、COMPLEX64、COMPLEX128, 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTanGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnInplaceTan的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入张量selfRef中每个元素的正切值，并将结果存储在张量selfRef中
 * 计算公式：$$ out[i] = tan(self[i]) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(selfRef)] -->B([l0op::Contiguous])
 *     B --> C([l0op::Tan])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(selfRef)]
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持 FLOAT、FLOAT16、INT32、DOUBLE、COMPLEX64、COMPLEX128, 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTanGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnTan的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量self中每个元素的正切值，并将结果存储在张量out中
 * 计算公式：$$ out[i] = tan(self[i]) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B --> C([l0op::Tan])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTanhGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTan(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

/**
 * @brief aclnnInplaceTan的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量selfRef中每个元素的正切值，并将结果存储在张量selfRef中
 * 计算公式：$$ out[i] = tan(self[i]) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(selfRef)] -->B([l0op::Contiguous])
 *     B --> C([l0op::Tan])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(selfRef)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTanhGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTan(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TANH_H// End content from: aclnn_tan.h

// Begin content from: aclnn_group_quant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GROUP_QUANT_H_
#define OP_API_INC_LEVEL2_ACLNN_GROUP_QUANT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupQuant的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 待进行GroupQuant计算的入参。npu device侧的aclTensor，
 * 数据类型支持float16, bfloat16, float32, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] scale: npu device侧的aclTensor, 数据类型支持float, bf16, float16
 * @param [in] groupIndex: npu device侧的aclTensor，数据类型支持float, bf16, float16
 * @param [in] offsetOptional:  npu device侧的aclTensor，数据类型int, int64
 * @param [in] dstType:  host侧的aclScalar, 数据类型int
 * @param [in] y: GroupQuant计算的出参。npu device侧的aclTensor，
 * 数据类型支持int8, int4, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupQuantGetWorkspaceSize(const aclTensor* x, const aclTensor* scale,
                                                      const aclTensor* groupIndex, const aclTensor* offsetOptional,
                                                      int32_t dstType, aclTensor* y,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGroupQuant的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGroupQuantGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupQuant(void* workspace, uint64_t workspaceSize,
                                      aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GROUP_QUANT_H_// End content from: aclnn_group_quant.h

// Begin content from: aclnn_hardswish_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDSWISH_BACKWARD_H_
#define OP_API_INC_HARDSWISH_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardswishBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：激活函数hardswish的反向
 * 计算公式：
 * $$ res_{i} = grad\_output_{i} \times grad\_self_{i} $$
 * $$
 * grad\_self_{i} = \begin{cases}
 * 0, & self_{i} \lt -3, \\
 * self_{i} / 3 + 0.5, &   -3 \le self_{i} \le 3, \\
 * 1, & self_{i} \gt 3
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous])
 *     B --> C([l0op::HardSwishGrad])
 *     D[(self)] --> E([l0op::Contiguous])
 *     E --> C([l0op::HardSwishGrad])
 *     C --> F([l0op::ViewCopy])
 *     F --> g[(out)]
```
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，数据类型支持浮点类型，且数据类型需要与self一致，shape需要与self一致。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC，且数据格式需要与other一致。
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持浮点类型，且数据类型需要与gradOutput一致，shape需要与gradOutput一致。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC，且数据格式需要与gradOutput一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持浮点类型，且数据类型需要与gradOutput一致，shape需要与gradOutput一致。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC，且数据格式需要与gradOutput一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardswishBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnHardswishBackward的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数hardswish的反向
 * 计算公式：
 * $$ res_{i} = grad\_output_{i} \times grad\_self_{i} $$
 * $$
 * grad\_self_{i} = \begin{cases}
 * 0, & self_{i} \lt -3, \\
 * self_{i} / 3 + 0.5, &   -3 \le self_{i} \le 3, \\
 * 1, & self_{i} \gt 3
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous])
 *     B --> C([l0op::HardSwishGrad])
 *     D[(self)] --> E([l0op::Contiguous])
 *     E --> C([l0op::HardSwishGrad])
 *     C --> F([l0op::ViewCopy])
 *     F --> g[(out)]
```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSubGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardswishBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSWISH_BACKWARD_H_// End content from: aclnn_hardswish_backward.h

// Begin content from: aclnn_isfinite.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ISFINITE_H_
#define OP_API_INC_LEVEL2_ISFINITE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：判断输入张量的元素是否有界。
 * 计算图：如下
 * 场景一：非浮点数，肯定有界，直接返回True
 *
 * ```mermaid
 * graph LR
 *     A[(self)] --> C([l0op::Fill]) --> H([l0op::ViewCopy]) --> K[(out)]
 * ```
 *
 * 场景二：浮点数的情况，直接调IsFinite
 *
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous]) --> C([l0op::IsFinite]) --> H([l0op::ViewCopy]) --> K[(out)]
 * ```
 */

/**
 * @brief aclnnIsFinite的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: 原始张量。npu device侧的aclTensor，
 * 数据类型支持FLOAT、FLOAT16、BFLOAT16(仅910B支持)、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，支持非连续的Tensor。数据格式支持ND。
 * @param [out] out: npu device侧的aclTensor，数据类型只能是BOOL，shape与self一致，支持非连续的Tensor。数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsFiniteGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                    aclOpExecutor** executor);

/**
 * @brief aclnnIsFinite的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnIsFiniteGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsFinite(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ISFINITE_H_
// End content from: aclnn_isfinite.h

// Begin content from: aclnn_frac.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_FRAC_H_
#define OP_API_INC_FRAC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFrac的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：计算输入Tensor中每个元素的小数部分。
 * 计算公式：
 * out_{i}=input_{i} - \lfloor \vert input_{i} \vert \rfloor * sgn(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT16、FLOAT、UINT8、INT8、INT16、INT32、INT64。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT16、FLOAT、UINT8、INT8、INT16、INT32、INT64。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnFracGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnFrac的第二段接口，用于执行计算。
 * 功能描述：计算输入Tensor中每个元素的小数部分。
 * 计算公式：
 * out_{i}=input_{i} - \lfloor \vert input_{i} \vert \rfloor * sgn(input_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Input)] -->B([l0op::Contiguous])
    B -->C([l0op::Sub])

    B -->F1([l0op::Trunc])
    F1 --> C
    C -->D([l0op::ViewCopy])
    D -->E[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnFracGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFrac(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceFrac的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：计算输入Tensor中每个元素的小数部分。
 * 参数描述：
 * @param [in]   inputRef
 * 输入Tensor，数据类型支持FLOAT16、FLOAT、UINT8、INT8、INT16、INT32、INT64。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceFracGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceFrac的第二段接口，用于执行计算
 * 功能描述：计算输入Tensor中每个元素的小数部分。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceFracGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFrac(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_frac.h

// Begin content from: aclnn_foreach_add_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_ADD_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_ADD_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachAddScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素与scalar相加后输出。
 * 计算公式：
 * out_{i}=x_{i}+scalar
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachAddScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachAddScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素与scalar相加后输出。
 * 计算公式：
 * out_{i}=x_{i}+scalar
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachAddScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachAddScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_add_scalar_v2.h

// Begin content from: aclnn_erfc.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ERFC_H_
#define OP_API_INC_LEVEL2_ACLNN_ERFC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnErfc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素对应的误差互补函数的值。
 * 计算公式：
 * $$ erfc(x)=1-\frac{2}{\sqrt{\pi } } \int_{0}^{x} e^{-t^{2} } \mathrm{d}t $$
 *
 * 场景：当输入类型在Erfc算子支持的范围之内（FLOAT32、BFLOAT16、FLOAT16、FLOAT64）时，使用Erfc算子完成计算。
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B([l0op::Contiguous])
 *     B --> C([l0op::Erfc])
 *     C --> D([l0op::Cast])
 *     D --> E([l0op::ViewCopy])
 *     E --> F[(out)]
 * ```
 *
 * 场景：self的数据类型为BOOL或INT64，将self的数据类型CAST为FLOAT32，再使用Erfc算子完成计算。
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B([l0op::Contiguous])
 *     B -->C([l0op::Cast])
 *     C -->D([l0op::Erfc])
 *     D --> E([l0op::Cast])
 *     E --> F([l0op::ViewCopy])
 *     F --> G[(out)]
 * ```
 *
 * @param [in] self: 待进行erfc计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、BFLOAT16、FLOAT16、BOOL、INT64，数据格式支持ND， 支持非连续的Tensor。
 * @param [in] out: erfc计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、BFLOAT16、FLOAT16，默认和self保持一致，若self数据类型为BOOL或INT64时，out的数据类型默认为FLOAT32，
 * 数据格式支持ND，和self的shape保持一致，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErfcGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnErfc的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnErfcGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErfc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceErfc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素对应的误差互补函数的值
 * 计算公式：
 * $$ erfc(x)=1-\frac{2}{\sqrt{\pi } } \int_{0}^{x} e^{-t^{2} } \mathrm{d}t $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::Erfc])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(out)]
 * ```
 *
 * @param [in] selfRef: 待进行erfc计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、BFLOAT16、FLOAT16，数据格式支持ND， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErfcGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceErfc的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceErfcGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErfc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ERFC_H_// End content from: aclnn_erfc.h

// Begin content from: aclnn_quant_scatter.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_QUANT_SCATTER_H_
#define OP_API_INC_LEVEL2_ACLNN_QUANT_SCATTER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceQuantScatter的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：
 * 先将updates进行量化，然后将updates中的值按指定的轴axis和索引indices逐个更新selfRef中的值。该算子为自定义算子语义,
 * 无对应的tensorflow或pytorch接口。
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持INT8。维度数量需要与updates一致。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持INT32。支持非连续的Tensor，数据格式支持ND。
 * @param [in] updates: npu device侧的aclTensor，数据类型支持BFLOAT16（仅昇腾910B AI处理器支持）。
 * 维度数量需要与selfRef一致。支持非连续的Tensor，数据格式支持ND。
 * @param [in] quantScales: npu device侧的aclTensor，数据类型支持BFLOAT16（仅昇腾910B AI处理器支持）类型。
 * 支持非连续的tensor。数据格式支持ND。
 * @param [in] quantZeroPoints: npu device侧的aclTensor，数据类型支持BFLOAT16（仅昇腾910B AI处理器支持）类型。
 * 支持非连续的tensor。数据格式支持ND。
 * @param [in] axis: 用来scatter的维度，数据类型为INT64。
 * @param [in] quantAxis: 用来量化的维度，数据类型为INT64。
 * @param [in] reduction: 指定要应用到输出的缩减，数据类型为INT64。当前仅支持1('update')。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceQuantScatterGetWorkspaceSize(aclTensor* selfRef, const aclTensor* indices,
                                                               const aclTensor* updates, const aclTensor* quantScales,
                                                               const aclTensor* quantZeroPoints, int64_t axis,
                                                               int64_t quantAxis, int64_t reduction,
                                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceQuantScatter的第二段接口，用于执行计算
 * @domain aclnn_ops_infer
 * 算子功能：
 * 先将updates进行量化，然后将updates中的值按指定的轴axis和索引indices逐个更新selfRef中的值。该算子为自定义算子语义,
 * 无对应的tensorflow或pytorch接口。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceQuantScatterGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceQuantScatter(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_QUANT_SCATTER_H_// End content from: aclnn_quant_scatter.h

// Begin content from: aclnn_fill_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_FILL_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_FILL_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：对Tensor各个位置赋予value值
 * 计算公式：
 *   不涉及
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *    A[(selfRef)] --> B(生成AclTensor: dims) -->C([l0op::Fill])
 *    D[(value)] --> C
 *    C --> E([l0op::ViewCopy])--> F[(out)]
 * ```
 */

/**
 * @brief aclnnInplaceFillTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64、DOUBLE、
 * COMPLEX64、COMPLEX128、BOOL、BFLOAT16，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] value: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、INT16、INT32、INT64、DOUBLE、
 * COMPLEX64、COMPLEX128、BOOL、BFLOAT16，且数据类型需要能转换成selfRef的数据类型，数据格式支持ND，数据维度只能是0D或者
 * size=1的1D。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* value,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFillTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspacSize: 在npu device侧申请的workspace大小，由第一段接口aclnnFillTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FILL_TENSOR_H_// End content from: aclnn_fill_tensor.h

// Begin content from: aclnn_atan.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ATAN_H_
#define OP_API_INC_ATAN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAtan的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反正切操作后输出。
 * 计算公式：
 * out_{i}=tan^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnAtanGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);
/**
 * @brief aclnnAtan的第二段接口，用于执行计算。
 * 功能描述：从输入矩阵的每一个元素进行反正切操作后输出。。
 * 计算公式：
 * out_{i}=tan^{-1}(input_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Atan])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAtan(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceAtan的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=tan^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAtanGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAtan的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成atan操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtanGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAtan(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_atan.h

// Begin content from: aclnn_reflection_pad1d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REFLECTION_PAD1D_BACKWARD_H_
#define OP_API_INC_REFLECTION_PAD1D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad1dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：reflection_pad1d的反向传播。
 * @param [in] gradOutput: 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))，
 * 维度支持二维或三维且与self和gradInput一致，shape需要与reflection_pad1d正向传播的output一致。
 * @param [in] self:
 * 数据类型与gradOutput一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))，维度支持二维或三维且与gradOutput和
 * gradInput一致，shape与gradInput一致。
 * @param [in] padding: 数据类型为INT64，长度为2，数值依次代表左右需要填充的值，需小于self最后一维度的数值。
 * @param [in] gradInput:
 * 数据类型与gradOutput一致，shape与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad1dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                   const aclIntArray* padding, aclTensor* gradInput,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad1dBackward的第二段接口，用于执行计算
 *
 * 算子功能：reflection_pad1d的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReflectionPad1dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad1dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REFLECTION_PAD1D_BACKWARD_H_// End content from: aclnn_reflection_pad1d_backward.h

// Begin content from: aclnn_polar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_POLAR_H_
#define OP_API_INC_POLAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPolar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 
 * 算子功能：构造一个张量，其元素是笛卡尔坐标，对应于具有绝对值abs和角度angle的极坐标。
 *
 * @param [in] input: npu device侧的aclTensor，数据类型支持FLOAT，
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] angle: npu device侧的aclTensor，数据类型需与input一致，
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] out： 数据类型支持complex64。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 */
ACLNN_API aclnnStatus aclnnPolarGetWorkspaceSize(const aclTensor* input, const aclTensor* angle, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPolar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnPolarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnPolar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_POLAR_H_// End content from: aclnn_polar.h

// Begin content from: aclnn_kl_div_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_KL_DIV_BACKWARD_H_
#define OP_API_INC_KL_DIV_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnKlDivBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnKlDivBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                         const aclTensor* target, int64_t reduction, bool logTarget,
                                                         aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnKlDivBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnKlDivBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_KL_DIV_DBACKWARD_H_
// End content from: aclnn_kl_div_backward.h

// Begin content from: aclnn_foreach_log.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LOG_H_
#define ACLNN_FOREACH_LOG_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLogGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLogGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLog
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_log.h

// Begin content from: aclnn_foreach_div_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_DIV_SCALAR_LIST_H_
#define ACLNN_FOREACH_DIV_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachDivScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachDivScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_div_scalar_list.h

// Begin content from: aclnn_nll_loss2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_NLL_LOSS2D_BACKWARD_H_
#define OP_API_INC_NLL_LOSS2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNLLLoss2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：负对数似然损失的反向。
 *
 * @param [in] gradOutput：npu
 * device侧的aclTensor，shape为三维（第一维是N）或者一维（且元素个数为1），数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] self：npu device侧的aclTensor，shape为四维，第一维是N表示batch
 * size，第二维是C表示类别，数据类型支持FLOAT、FLOAT16。 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] target：npu device侧的aclTensor，表示真实标签，shape为三维，第一维是N，其中每个元素的取值范围是[0, C -
 * 1]，数据类型支持INT64、UINT8。 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] weight：npu device侧的aclTensor，表示各个类别的权重，shape为(C, )，数据类型支持FLOAT、FLOAT16。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] reduction：host侧的int64_t，指定要应用到输出的缩减。支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] ignoreIndex：host侧的int64_t，指定一个被忽略且不影响输入梯度的目标值。
 * @param [in] totalWeight：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，且数据类型为与weight相同，shape为(1，)，数据格式支持ND（[参考](#)）。
 * @param [out] out：npu device侧的aclTensor，shape与self相同。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLoss2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                             const aclTensor* target, const aclTensor* weight,
                                                             int64_t reduction, int64_t ignoreIndex,
                                                             aclTensor* totalWeight, aclTensor* out,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNLLLoss2dBackward的第二段接口，用于执行计算。
 *
 * 算子功能：负对数似然损失的反向。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnNLLLoss2dBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLoss2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NLL_LOSS2_DBACKWARD_H_
// End content from: aclnn_nll_loss2d_backward.h

// Begin content from: aclnn_adaptive_avg_pool3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ADAPTIVE_AVG_POOL3D_H_
#define OP_API_INC_ADAPTIVE_AVG_POOL3D_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveAvgPool3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnAdaptiveAvgPool3d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_adaptive_avg_pool3d.h

// Begin content from: aclnn_ger.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GER_H_
#define OP_API_INC_LEVEL2_ACLNN_GER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGer的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnGerGetWorkspaceSize(const aclTensor* self, const aclTensor* vec2, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGer的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGer(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GER_H_// End content from: aclnn_ger.h

// Begin content from: aclnn_embedding_dense_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_EMBEDDING_DENSE_BACKWARD_H_
#define OP_API_INC_EMBEDDING_DENSE_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEmbeddingDenseBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成Embedding的反向计算
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(grad)] -->B([l0op::Contiguous])
 *     B --> L([l0op::Cast])
 *     L --> C([l0op::EmbeddingDenseGrad])
 *     D[(indices)] -->E([l0op::Contiguous])
 *     E --> F([l0op::Cast])
 *     F --> C
 *     C --> M([l0op::Cast])
 *     M --> G([l0op::ViewCopy])
 *     G --> K[(out)]
 *     H((numWeights)) --> C
 *     I((paddingIdx)) --> C
 *     J((scaleGradByFreq)) --> C
 * ```
 *
 * @param [in] grad: npu
 * device侧的aclTensor，梯度Tensor，和正向的输出shape一致，npu
 * device侧的aclTensor，数据类型支持float、float16、bfloat16类型， 数据格式支持ND，比indices的维度多一。
 * @param [in] indices: npu
 * device侧的aclTensor，正向中需要映射到向量空间的索引张量，数据类型支持int32，支持非连续的Tensor，数据格式支持ND。
 * @param [in] numWeights: 向量空间的大小。
 * @param [in] paddingIdx:
 * 填充ID，默认为None，如果指定的话，将指定位置处的向量元素全部置为0，且paddingIdx对应的参数不会对梯度产生影响。
 * @param [in] scaleGradByFreq: 根据单词出现的频率，对梯度进行放缩，默认为False。
 * @param [out] out: npu
 * 反向输出Tensor，数据类型支持float32类型，数据格式仅支持2D。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingDenseBackwardGetWorkspaceSize(const aclTensor* grad, const aclTensor* indices,
                                                                  uint64_t numWeights, uint64_t paddingIdx,
                                                                  bool scaleGradByFreq, const aclTensor* out,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnEmbeddingDenseBackward的第二段接口，用于执行计算。
 *
 * 算子功能：完成Embedding的反向计算
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(grad)] -->B([l0op::Contiguous])
 *     B --> L([l0op::Cast])
 *     L --> C([l0op::EmbeddingDenseGrad])
 *     D[(indices)] -->E([l0op::Contiguous])
 *     E --> F([l0op::Cast])
 *     F --> C
 *     C --> M([l0op::Cast])
 *     M --> G([l0op::ViewCopy])
 *     G --> K[(out)]
 *     H((numWeights)) --> C
 *     I((paddingIdx)) --> C
 *     J((scaleGradByFreq)) --> C
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnEmbeddingDenseBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingDenseBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EMBEDDING_DENSE_BACKWARD_H_
// End content from: aclnn_embedding_dense_backward.h

// Begin content from: aclnn_smooth_l1_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_SMOOTH_L1_LOSS_BACKWARD_H_
#define OP_API_INC_SMOOTH_L1_LOSS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSmoothL1LossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能： 对输入Tensor完成SmoothL1Loss backward操作
 * 对于smoothL1Loss的第一种情况, 即$|x-y|<1$,其导数为：$$\frac{\partial SmoothL1Loss(x,y)}{\partial x} = x - y $$
 * 对于smoothL1Loss的第一种情况, 即$|x-y|\geq 1$,其导数为：$$\frac{\partial SmoothL1Loss(x,y)}{\partial x} = sign(x-y)$$
 * @param [in] gradOut: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] target: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] reduction: host侧的参数，数据类型为int64_t，指定要应用到输出的缩减，支持0("none"),1("mean"),2"sum")。
 * @param [in] beta:指定在L1和L2损失之间更改的阈值，数据类型为double，该值必须是非负的。
 * @param [in] gradInput:npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * 当reduction为"none"时，shape与self相同，否则shape为[1]
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码， 成功返回ACLNN_SUCCESS, 失败返回对应错误码。
 */
ACLNN_API aclnnStatus aclnnSmoothL1LossBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* self,
                                                                const aclTensor* target, int64_t reduction, float beta,
                                                                aclTensor* gradInput, uint64_t* workspaceSize,
                                                                aclOpExecutor** executor);

/**
 * @brief: aclnnSmoothL1LossBackward的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成SmoothL1Loss backward操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSigmoidBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码,成功返回ACLNN_SUCCESS, 失败返回对应错误码。
 */
ACLNN_API aclnnStatus aclnnSmoothL1LossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SMOOTH_L1_LOSS_BACKWARD_H_// End content from: aclnn_smooth_l1_loss_backward.h

// Begin content from: aclnn_pad2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PAD2D_BACKWARD_H_
#define OP_API_INC_PAD2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：反射填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，数据格式支持ND，
 * 维度支持三维或四维且与self和gradInput一致，shape需要与reflection_pad2d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持三维或四维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * 前两个数值需小于self最后一维度的数值，后两个数值需小于self倒数第二维度的数值。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持三维或四维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                   const aclIntArray* padding, aclTensor* gradInput,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad2dBackward的第二段接口，用于执行计算
 *
 * 算子功能：反射填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReflectionPad2dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   const aclrtStream stream);

/**
 * @brief aclnnReplicationPad2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：边界填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，数据格式支持ND，
 * 维度支持三维或四维且与self和gradInput一致，shape需要与replication_pad2d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持三维或四维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持三维或四维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                    const aclIntArray* padding, aclTensor* gradInput,
                                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad2dBackward的第二段接口，用于执行计算
 *
 * 算子功能：边界填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReplicationPad2dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PAD2D_BACKWARD_H_// End content from: aclnn_pad2d_backward.h

// Begin content from: aclnn_index_fill_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_INDEX_FILL_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_INDEX_FILL_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIndexFillTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnIndexFillTensorGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclIntArray* index,
                                                           const aclScalar* value, aclTensor* out,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIndexFillTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnIndexFillTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

/**
 * @brief aclnnInplaceIndexFillTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnInplaceIndexFillTensorGetWorkspaceSize(aclTensor* selfRef, int64_t dim,
                                                                  const aclIntArray* index, const aclScalar* value,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceIndexFillTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceIndexFillTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_INDEX_FILL_TENSOR_H_// End content from: aclnn_index_fill_tensor.h

// Begin content from: aclnn_histc.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HISTC_H_
#define OP_API_INC_HISTC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHistc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE支持非连续的Tensor，数据格式支持ND
 * @param [in] bins: 直方图bins的数量，数据类型INT32
 * @param [in] min: 直方图统计下限（包括），host侧的aclScalar，数据类型需要可转换成FLOAT的数据类型。
 * @param [in] max: 直方图统计上限（包括），host侧的aclScalar，数据类型需要可转换成FLOAT的数据类型。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、INT32。且数据类型是self可转化的数据类型。数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHistcGetWorkspaceSize(const aclTensor* self, int64_t bins, const aclScalar* min,
                                                 const aclScalar* max, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnHistc的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnHistcGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHistc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HISTC_H_
// End content from: aclnn_histc.h

// Begin content from: aclnn_quant_matmul.h

/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_QUANT_MM_H_
#define OP_API_INC_QUANT_MM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACL_DEPRECATED_MESSAGE("aclnnQuantMatmulGetWorkspaceSize will be deprecated, use aclnnQuantMatmulV4GetWorkspaceSize instead")
ACLNN_API aclnnStatus aclnnQuantMatmulGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2, const aclTensor* bias,
                                                       float deqScale, aclTensor* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmul的第二段接口，用于执行计算。
 */
ACL_DEPRECATED_MESSAGE("aclnnQuantMatmul will be deprecated, use aclnnQuantMatmulV4 instead")
ACLNN_API aclnnStatus aclnnQuantMatmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

/**
 * @brief aclnnQuantMatmulV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACL_DEPRECATED_MESSAGE("aclnnQuantMatmulV2GetWorkspaceSize will be deprecated, use aclnnQuantMatmulV4GetWorkspaceSize instead")
ACLNN_API aclnnStatus aclnnQuantMatmulV2GetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                         const aclTensor* bias, const aclTensor* deqScale, bool adjX1,
                                                         bool adjX2, aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmulV2的第二段接口，用于执行计算。
 */
ACL_DEPRECATED_MESSAGE("aclnnQuantMatmulV2 will be deprecated, use aclnnQuantMatmulV4 instead")
ACLNN_API aclnnStatus aclnnQuantMatmulV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MM_H_
// End content from: aclnn_quant_matmul.h

// Begin content from: aclnn_atan2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ATAN2_H_
#define OP_API_INC_ATAN2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAtan2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入张量self和other进行逐元素的反正切运算，注（self表示y坐标，other表示x坐标）。
 * 计算公式：
 * out_{i}=tan^{-1}(self_{i}/other_{i})
 * 参数描述：
 * @param [in] self:npu device侧的aclTensor，数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、BFLOAT16、
 * FLOAT、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND，维度不大于8，且shape需要与other满足broadcast关系。
 * @param [in] other:npu device侧的aclTensor，数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、BFLOAT16、
 * FLOAT、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND，维度不大于8，且shape需要与self满足broadcast关系。
 * @param [out] out:npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE。支持非连续Tensor，
 * 数据格式支持ND，维度不大于8，且shape是self与other broadcast之后的shape。
 * @param [out] workspaceSize:返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor:返回op执行器，包含了算子计算流程。
 * @return aclnnStatus:返回状态码
 */
ACLNN_API aclnnStatus aclnnAtan2GetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnAtan2的第二段接口，用于执行计算。
 * 功能描述：对输入张量self和other进行逐元素的反正切运算，注（self表示y坐标，other表示x坐标）。
 * 计算公式：
 * out_{i}=tan^{-1}(self_{i}/other_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Atan])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtan2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAtan2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAtan2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入张量self和other进行逐元素的反正切运算，注（self表示y坐标，other表示x坐标）。
 * 计算公式：
 * out_{i}=tan^{-1}(self_{i}/other_{i})
 * 参数描述：
 * @param [in] self:npu device侧的aclTensor，数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、BFLOAT16、
 * FLOAT、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND，维度不大于8，且shape需要与other满足broadcast关系。
 * @param [in] other:npu device侧的aclTensor，数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、BFLOAT16、
 * FLOAT、FLOAT16、DOUBLE。支持非连续的Tensor，数据格式支持ND，维度不大于8，且shape需要与self满足broadcast关系。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAtan2GetWorkspaceSize(aclTensor* selfRef, aclTensor* other, uint64_t* workspace_size,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAtan2的第二段接口，用于执行计算
 *
 * 算子功能： 对输入张量self和other进行逐元素的反正切运算，注（self表示y坐标，other表示x坐标）。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtan2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAtan2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_atan2.h

// Begin content from: aclnn_scatter.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SCATTER_H_
#define OP_API_INC_SCATTER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScatter的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。self的维度数量需要与index、src相同。self的数据类型需要与src一致。支持空tensor, 支持
 * 非连续的tensor。数据格式支持ND。
 * @param [in] dim: 用来scatter的维度，数据类型为INT64。范围为[-self的维度数量, self的维度数量-1]。
 * @param [in] index: npu device侧的aclTensor。数据类型支持INT32,
 * INT64。index的维度数量需要与self、src相同。支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] src: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。src的维度数量需要与self、index相同。src的数据类型需要与self一致。支持空tensor，
 * 支持非连续的 tensor。数据格式支持ND。
 * @param [in] reduce: 选择应用的reduction操作。可选的操作选项以及对应的int值为 (add, 1), (mul, 2)，(none, 0)。
 * 具体操作含义如下：
 * 0：表示替换操作，将src中的对应位置的值按照index替换到out中的对应位置
 * 1：表示累加操作，将src中的对应位置的值按照index累加到out中的对应位置
 * 2：表示累乘操作，将src中的对应位置的值按照index累乘到out中的对应位置
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。数据格式、数据类型、shape需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnScatterGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* index,
                                                   const aclTensor* src, int64_t reduce, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnScatter的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnScatterGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScatter(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   const aclrtStream stream);

/**
 * @brief aclnnScatterValue的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。self的维度数量需要与index、src相同。self的数据类型需要与src一致。支持空tensor, 支持
 * 非连续的tensor。数据格式支持ND。
 * @param [in] dim: 用来scatter的维度，数据类型为INT64。范围为[-self的维度数量, self的维度数量-1]。
 * @param [in] index: npu device侧的aclTensor。数据类型支持INT32,
 * INT64。index的维度数量需要与self、src相同。支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] value: host侧的aclScalar,
 * 数据类型支持UINT8、INT8、INT16、INT32、INT64、FLOAT16、FLOAT32、DOUBLE、COMPLEX64、
 * COMPLEX128。当value为COMPLEX时，self也必须为COMPLEX tensor。
 * @param [in] reduce: 选择应用的reduction操作。可选的操作选项以及对应的int值为 (add, 1), (mul, 2)，(none, 0)。
 * 具体操作含义如下：
 * 0：表示替换操作，将src中的对应位置的值按照index替换到out中的对应位置
 * 1：表示累加操作，将src中的对应位置的值按照index累加到out中的对应位置
 * 2：表示累乘操作，将src中的对应位置的值按照index累乘到out中的对应位置
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。数据格式、数据类型、shape需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnScatterValueGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* index,
                                                        const aclScalar* value, int64_t reduce, aclTensor* out,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnScatter的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnScatterValueGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScatterValue(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

/**
 * @brief aclnnInplaceScatter的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] selfRef:
 * 数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、COMPLEX64、COMPLEX128类型。
 * selfRef的维度数量需要与index、src相同。selfRef的数据类型需要与src一致。支持空tensor，
 * 支持非连续的tensor。数据格式支持ND。
 * @param [in] dim: 用来scatter的维度，数据类型为INT64。范围为[-selfRef的维度数量, selfRef的维度数量-1]。
 * @param [in] index: npu device侧的aclTensor。数据类型支持INT32,
 * INT64。index的维度数量需要与selfRef、src相同。支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] src: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。src的维度数量需要与selfRef、index相同。src的数据类型需要与selfRef一致。支持空tensor，
 * 支持非连续的 tensor。数据格式支持ND。
 * @param [in] reduce: 选择应用的reduction操作。可选的操作选项以及对应的int值为 (add, 1), (mul, 2)，(none, 0)。
 * 具体操作含义如下：
 * 0：表示替换操作，将src中的对应位置的值按照index替换到out中的对应位置
 * 1：表示累加操作，将src中的对应位置的值按照index累加到out中的对应位置
 * 2：表示累乘操作，将src中的对应位置的值按照index累乘到out中的对应位置
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceScatterGetWorkspaceSize(aclTensor* selfRef, int64_t dim, const aclTensor* index,
                                                          const aclTensor* src, int64_t reduce, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceScatter的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceScatterGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceScatter(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

/**
 * @brief aclnnInplaceScatterValue的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、
 * DOUBLE、COMPLEX64、COMPLEX128类型。selfRef的维度数量需要与index、src相同。selfRef的数据类型需要与src一致。支持空tensor,
 * 支持 非连续的tensor。数据格式支持ND。
 * @param [in] dim: 用来scatter的维度，数据类型为INT64。范围为[-selfRef的维度数量, selfRef的维度数量-1]。
 * @param [in] index: npu device侧的aclTensor。数据类型支持INT32,
 * INT64。index的维度数量需要与selfRef、src相同。支持空tensor， 支持非连续的tensor。数据格式支持ND。
 * @param [in] value: host侧的aclScalar,
 * 数据类型支持UINT8、INT8、INT16、INT32、INT64、FLOAT16、FLOAT32、DOUBLE、COMPLEX64、
 * COMPLEX128。当value为COMPLEX时，selfRef也必须为COMPLEX tensor。
 * @param [in] reduce: 选择应用的reduction操作。可选的操作选项以及对应的int值为 (add, 1), (mul, 2)，(none, 0)。
 * 具体操作含义如下：
 * 0：表示替换操作，将src中的对应位置的值按照index替换到out中的对应位置
 * 1：表示累加操作，将src中的对应位置的值按照index累加到out中的对应位置
 * 2：表示累乘操作，将src中的对应位置的值按照index累乘到out中的对应位置
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、BOOL、FLOAT16、FLOAT32、DOUBLE、
 * COMPLEX64、COMPLEX128类型。数据格式、数据类型、shape需要与selfRef一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceScatterValueGetWorkspaceSize(aclTensor* selfRef, int64_t dim, const aclTensor* index,
                                                               const aclScalar* value, int64_t reduce,
                                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceScatterValue的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成scatter操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceScatterValueGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceScatterValue(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_scatter.h

// Begin content from: aclnn_mish_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MISH_BACKWARD_H_
#define OP_API_INC_MISH_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMishBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnMishBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                        aclTensor* gradInput, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnMishBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMishBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MISH_DBACKWARD_H_
// End content from: aclnn_mish_backward.h

// Begin content from: aclnn_global_average_pool.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GLOBAL_AVERAGE_POOL_H_
#define OP_API_INC_GLOBAL_AVERAGE_POOL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGlobalAveragePool的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：GlobalAveragePool consumes an input tensor X and applies average pooling across the values in the same
 * channel.
 *
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、DOUBLE。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] out:
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、DOUBLE。支持连续和非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGlobalAveragePoolGetWorkspaceSize(const aclTensor* self, aclTensor* out,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGlobalAveragePool的第二段接口，用于执行计算。
 *
 * 算子功能：GlobalAveragePool consumes an input tensor X and applies average pooling across the values in the same
 * channel.
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnGlobalAveragePoolGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGlobalAveragePool(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GLOBAL_AVERAGE_POOL_H_// End content from: aclnn_global_average_pool.h

// Begin content from: aclnn_put.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PUT_H_
#define OP_API_INC_PUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplacePut的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将selfRef视为一维张量，把index张量中元素值作为索引，如果accumulate为true，把source中元素和selfRef对应的位置上元素做累加操作;
 * 如果accumulate为false，把source中元素替换掉selfRef对应位置上的元素。
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL，
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] index: npu
 * device侧的aclTensor，数据类型支持INT32、INT64,元素个数要求和source保持一致。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] source: npu
 * device侧的aclTensor，数据类型和selfRef一致,元素个数为和index一致。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplacePutGetWorkspaceSize(aclTensor* selfRef, const aclTensor* index,
                                                      const aclTensor* source, bool accumulate, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnInplacePut的第二段接口，用于执行计算。
 *
 * 算子功能：将selfRef视为一维张量，把index张量中元素值作为索引，如果accumulate为true，把source中元素和selfRef对应的位置上元素做累加操作;
 * 如果accumulate为false，把source中元素替换掉selfRef对应位置上的元素。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplacePutGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplacePut(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PUT_H_
// End content from: aclnn_put.h

// Begin content from: aclnn_threshold_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_THRESHOLD_BACKWARG_H_
#define OP_API_INC_THRESHOLD_BACKWARG_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnThresholdBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成threshold_forward对应反向
 * 计算公式：
 * res(i) = gradOutput(i) if self(i) > threshold else 0
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous])
 * B --> C([l0op::ThresholdGradV2D or l0op::ReluGrad])
 * D[(grad_output)] -->  E([l0op::Contiguous])
 * E  --> C
 * F[(threshold)] --> C
 * C--> G([l0op::ViewCopy])
 * G --> H[(out)]
 * ```
 *
 * @param [in] gradOutput: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT8、UINT8，shape需要与self一致。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT8、UINT8。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] threshold: host侧的aclScalar，数据类型需要可转换成self与other推导后的数据类型。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT8、UINT8，shape需要与self一致。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnThresholdBackwardGetWorkspaceSize(const aclTensor *gradOutput, const aclTensor *self,
                                                             const aclScalar *threshold, aclTensor *out,
                                                             uint64_t *workspaceSize, aclOpExecutor **executor);
/**
 * @brief aclnnAdd的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnThresholdBackward(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_THRESHOLD_BACKWARG_H_
// End content from: aclnn_threshold_backward.h

// Begin content from: aclnn_foreach_minimum_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MINIMUM_SCALAR_LIST_H_
#define ACLNN_FOREACH_MINIMUM_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMinimumScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMinimumScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_minimum_scalar_list.h

// Begin content from: aclnn_hardswish.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDSWISH_H_
#define OP_API_INC_HARDSWISH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardswish的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数
 * $$
 * Hardswish(x)=\begin{cases}
 * x, & x\ gt 3 \\
 * 0, & x\ le -3 \\
 * \frac{x · (x + 3)}{6}, & otherwise
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardswish])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardswishGetWorkspaceSize(const aclTensor* self, const aclTensor* out,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnHardswish的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数
 * $$
 * Hardswish(x)=\begin{cases}
 * x, & x\ gt 3 \\
 * 0, & x\ le -3 \\
 * \frac{x · (x + 3)}{6}, & otherwise
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardswish])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnHardswishGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardswish(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     const aclrtStream stream);

/**
 * @brief aclnnInplaceHardswish的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数
 * $$
 * Hardswish(x)=\begin{cases}
 * x, & x\ gt 3 \\
 * 0, & x\ le -3 \\
 * \frac{x · (x + 3)}{6}, & otherwise
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardswish])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardswishGetWorkspaceSize(const aclTensor* self, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnInplaceHardswish的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数
 * $$
 * Hardswish(x)=\begin{cases}
 * x, & x\ gt 3 \\
 * 0, & x\ le -3 \\
 * \frac{x · (x + 3)}{6}, & otherwise
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Hardswish])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceHardswishGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardswish(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSWISH_H
// End content from: aclnn_hardswish.h

// Begin content from: aclnn_gt_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GTTENSOR_H_
#define OP_API_INC_GTTENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGtTensor的第一段接口,根据具体的计算流程,计算workspace大小。
 * @domain aclnn_math
 * 计算self Tensor中的元素是否大于(>)other Tensor中的元素,返回一个Tensor,self>other的为True(1.),否则为False(0.):
 *
 * $$ out = (self_i > other_i)  ?  [True] : [False] $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([l0op::Contiguous])
 *     B -->C1([l0op::Cast])-->D([l0op::Greater])--> C3([l0op::Cast])
 *     E[(other)] -->F([l0op::Contiguous])
 *     F --> C2([l0op::Cast])-->D
 *     C3 -->F1([l0op::ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor,数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,UINT64,
 * UINT32,INT16,DOUBLE,UINT16,BFLOAT16数据类型,shape需要与other满足broadcast关系,
 * 数据类型需要与other满足数据类型推导规则,支持非连续的Tensor,数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor,数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,UINT64,
 * UINT32,INT16,DOUBLE,UINT16,BFLOAT16数据类型,shape需要与other满足broadcast关系,
 * 数据类型需要与self满足数据类型推导规则,支持非连续的Tensor,数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor,数据类型支持FLOAT16,FLOAT,INT64,INT32,INT8,UINT8,BOOL,UINT64,
 * UINT32,INT16,DOUBLE,UINT16,BFLOAT16数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器,包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGtTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGtTensor的第二段接口,用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小,由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器,包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGtTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceGtTensor的第一段接口,根据具体的计算流程,计算workspace大小。
 * @domain aclnn_math
 * 计算selfRef Tensor中的元素是否大于(>)other Tensor中的元素,返回结果复写到selfRef：
 *
 * $$ selfRef = (selfRef_i > other_i)  ?  [True] : [False] $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *    A[(SelfRef)] -->B([l0op::Contiguous])
 *    B -->C1([l0op::Cast])-->D([l0op::Greater])--> C3([l0op::Cast])
 *    E[(other)] -->F([l0op::Contiguous])
 *    F --> C2([l0op::Cast])-->D
 *    C3 -->F1([l0op::ViewCopy])--> J[(SelfRef)]
 * ```
 *
 * @param [in] selfRef(aclTensor*): 数据类型支持UINT8,INT8,DOUBLE,FLOAT,INT32,INT64,INT16,FLOAT16,BOOL,BFLOAT16数据类型,
 * shape需要与other满足broadcast关系,数据类型需要与other满足数据类型推导规则,支持非连续的Tensor,
 * 数据格式支持ND；selfRef也是输出Tensor。
 * @param [in] other(aclTensor*): 数据类型支持UINT8,INT8,DOUBLE,FLOAT,INT32,INT64,INT16,FLOAT16,BOOL,BFLOAT16数据类型,
 * shape需要与other满足broadcast关系,数据类型需要与selfRef满足数据类型推导规则,支持非连续的Tensor,数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器,包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGtTensorGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGtTensor的第二段接口,用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小,由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器,包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGtTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GTTENSOR_H_
// End content from: aclnn_gt_tensor.h

// Begin content from: aclnn_foreach_maximum_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_MAXIMUM_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_MAXIMUM_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachMaximumScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素和标量值scalar执行逐元素比较，返回最大值的输出。
 * 计算公式：
 * out_{i}=max(x_{i}, scalar)
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT32和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachMaximumScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachMaximumScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素和标量值scalar执行逐元素比较，返回最大值的输出。
 * 计算公式：
 * out_{i}=max(x_{i}, scalar)
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachMaximumScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachMaximumScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_maximum_scalar_v2.h

// Begin content from: aclnn_upsample_bilinear_2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_BILINEAR_2D_BACKWARD_H_
#define OP_API_INC_UNAMPLE_BILINEAR_2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBilinear2dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2dBackward(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                                      aclrtStream stream);

/**
 * @brief aclnnUpsampleBilinear2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2dBackwardGetWorkspaceSize(
    const aclTensor* gradOut, const aclIntArray* outputSize, const aclIntArray* inputSize, bool alignCorners,
    double scalesH, double scalesW, aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_BILINEAR_2D_BACKWARD_H_// End content from: aclnn_upsample_bilinear_2d_backward.h

// Begin content from: aclnn_embedding_bag.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/*!
 * \file aclnn_embedding_bag.h
 * \brief
 */
#ifndef OP_API_INC_EMBEDDING_BAG_H_
#define OP_API_INC_EMBEDDING_BAG_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnEmbeddingBag第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：根据indices从weight中获取一组被聚合的数，然后根据offsets的便宜和mode指定的聚合模式进行max、sum、mean等聚合。
 *
 * @param [in] weight: npu device侧的aclTensor，数据类型支持float32 float16 bfloat16，必须是2D。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64。必须是1D或2D tensor。
 * @param [in] offsets: npu
 * device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64，但和indices必须有一个是INT32或INT64,
 *                      在indices是1D时，offsets必须是1D tensor.
 * @param [in] scaleGradByFreq: bool数据类型，用于控制缩放梯度。
 * @param [in] mode: int64类型，用于控制聚合模式。
 * @param [in] sparse: bool数据类型，用于控制稀疏模式。
 * @param [in] perSampleWeights: npu device侧的aclTensor，指定样本权重。必须是1D
 * tensor，数据类型与weight一致，仅在sum可以不是null。
 * @param [in] includeLastOffset: bool数据类型，控制是否包含最后的偏移。
 * @param [in] paddingIdx: int64类型，取值范围是[-n,n-1]，其中n是weigit第一维元素个数。
 * @param [out] workspaceSize: 返回用户需要在npu侧申请的workspace大小。
 * @param [out] executor: 返回op执行器。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingBagGetWorkspaceSize(const aclTensor* weight, const aclTensor* indices,
                                                        const aclTensor* offsets, bool scaleGradByFreq, int64_t mode,
                                                        bool sparse, const aclTensor* perSampleWeights,
                                                        bool includeLastOffset, int64_t paddingIdx, aclTensor* output,
                                                        aclTensor* offset2bag, aclTensor* bagSize,
                                                        aclTensor* maxIndices, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnEmbeddingBag的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceIndexCopyGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingBag(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INDEX_COPY_H_
// End content from: aclnn_embedding_bag.h

// Begin content from: aclnn_bitwise_or_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 *
 算子功能：计算输入张量self中每个元素和输入标量other的按位或。输入self和other必须是整数或布尔类型，对于布尔类型，计算逻辑或。
 * 计算公式：如下
 * $$
 * \text{out}_i =
 * \text{self}_i \, | \, \text{other}
 * $$
 *
 * 实现说明：如下
 * 计算图一：如下
 * 场景：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalOr接口做计算

 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous])
 * B --> C([l0op::Cast])
 * C --> D([l0op::LogicalOr])
 * E((other)) --> D
 * D --> F([l0op::Cast])
 * F --> G([l0op::ViewCopy])
 * G --> H[(out)]
 * ```
 *
 * 计算图二：如下
 * 场景：不满足计算图一的条件时，都会调用l0::BitwiseOr接口做计算
 *
 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous])
 * B --> C([l0op::Cast])
 * C --> D([l0op::BitwiseOr])
 * E((other)) --> D
 * D --> F([l0op::Cast])
 * F --> G([l0op::ViewCopy])
 * G --> H[(out)]
 * ```
 */

/**
 * @brief aclnnBitwiseOrScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、UINT16、INT32、INT64、UINT8，且数据类型与other的数据类型
 * 需满足数据类型推导规则，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in]
 * other：host侧的aclScalar，数据类型支持BOOL、INT8、INT16、UINT16、INT32、INT64、UINT8，且数据类型与self的数据类型需满足
 * 数据类型推导规则。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、UINT16、INT32、INT64、UINT8、FLOAT、FLOAT16、DOUBLE、
 * BFLOAT16、COMPLEX64、COMPLEX128，且数据类型需要是self与other推导之后可转换的数据类型，shape需要与self一致，支持非连续的
 * Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseOrScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseOrScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnBitwiseOrScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseOrScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseOrScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、UINT16、INT32、INT64、UINT8，且数据类型与other
 * 的数据类型需满足数据类型推导规则，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in]
 * other：host侧的aclScalar，数据类型支持BOOL、INT8、INT16、UINT16、INT32、INT64、UINT8，且数据类型与self的数据类型需满足
 * 数据类型推导规则。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseOrScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseOrScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceBitwiseOrScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseOrScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BITWISE_OR_SCALAR_H_
// End content from: aclnn_bitwise_or_scalar.h

// Begin content from: aclnn_bitwise_xor_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：计算输入张量self中每个元素和输入标量other的按位异或，输入self和other必须是整数或布尔类型，对于布尔类型，计算逻辑异或。
 * 计算公式：如下
 * $$
 * \text{out}_i =
 * \text{self}_i \, \bigoplus\, \text{other}
 * $$
 *
 * 实现说明：如下
 * 计算图一：如下
 * 场景：经过类型推导后的数据类型为BOOL时，需要调用l0::NotEqual接口做计算
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::NotEqual])
 *   D((other)) --> C
 *   C --> E([l0op::Cast])
 *   E --> F([l0op::ViewCopy])
 *   F --> G[(out)]
 * ```
 *
 * 计算图二：如下
 * 场景：不满足计算图一的条件时，都会调用l0::BitwiseXor接口做计算
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::Cast])
 *   C --> D([l0op::BitwiseXor])
 *   E((other)) --> D
 *   D --> F([l0op::Cast])
 *   F --> G([l0op::ViewCopy])
 *   G --> H[(out)]
 * ```
 */

/**
 * @brief aclnnBitwiseXorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与other的数据类型
 * 需满足数据类型推导规则，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in]
 * other：host侧的aclScalar，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与self的数据类型需满足
 * 数据类型推导规则。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8、FLOAT、FLOAT16、DOUBLE、
 * BFLOAT16、COMPLEX64、COMPLEX128，且数据类型需要是self与other推导之后可转换的数据类型，shape需要与self一致，支持非连续的
 * Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseXorScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseXorScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnBitwiseXorScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseXorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseXorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与other的
 * 数据类型需满足数据类型推导规则，且推导后的数据类型需要能转换成selfRef自身的数据类型，支持非连续的Tensor，数据格式支持ND，数据
 * 维度不支持8维以上。
 * @param [in]
 * other：host侧的aclScalar，数据类型支持BOOL、INT8、INT16、INT32、INT64、UINT8，且数据类型与selfRef的数据类型需
 * 满足数据类型推导规则。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseXorScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseXorScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceBitwiseXorScalarGetWorkspaceSize 获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseXorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BITWISE_XOR_SCALAR_H_// End content from: aclnn_bitwise_xor_scalar.h

// Begin content from: aclnn_mul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_MUL_H_
#define OP_API_INC_LEVEL2_ACLNN_MUL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMuls的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。shape与输入self的shape相等。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMulsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: nnpu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。支持非连续的
 * Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。支持非连续的Tensor，数据格式
 * 支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。shape需要与输入broadcast之后的shape相等。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMulGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceMuls的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMulsGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceMul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64，数据类型需与other构成互相推导关系，且推导后的数据类型需支持转换到selfRef的数据类型。
 * shape需要与other满足broadcast关系，且broadcast后的shape需与selfRef的shape相等。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL、
 * COMPLEX128、COMPLEX64，且数据类型需要与selfRef构成互相推导关系，shape需要与selfRef满足broadcast关系。支持非连续的Tensor，
 * 数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMulGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMuls的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMulsGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMuls(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnMul的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMulGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceMuls的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceMulsGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMuls(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

/**
 * @brief aclnnInplaceMul的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceMulGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MUL_H_
// End content from: aclnn_mul.h

// Begin content from: aclnn_nan_to_num.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_NANTONUM_H_
#define OP_API_INC_LEVEL2_ACLNN_NANTONUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNand的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：将输入中的NaN、正无穷大和负无穷大分别替换为nan、posinf和neginf指定的值。
 *
 * @param [in] self: 待进行NanToNum计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT32、FLOAT16、BFLOAT16、INT8、INT16、INT32、INT64、UINT8、BOOL，
 * 数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [in] nan: 输入参数。数据类型支持FLOAT，替换tensor元素中NaN的值。
 * @param [in] posinf: 输入参数。数据类型支持FLOAT，替换tensor元素中正无穷大的值。
 * @param [in] neginf: 输入参数。数据类型支持FLOAT，替换tensor元素中负无穷大的值。
 * @param [out] out: NanToNum计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT32、FLOAT16、BFLOAT16、INT8、INT16、INT32、INT64、UINT8、BOOL，
 * 数据格式支持ND，且数据格式需要与self一致， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanToNumGetWorkspaceSize(const aclTensor* self, float nan, float posinf, float neginf,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNanToNum的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnNanToNumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNanToNum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceNanToNum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: 待进行NanToNum计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNanToNumGetWorkspaceSize(aclTensor* selfRef, float nan, float posinf, float neginf,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceNanToNum的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnNanToNumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNanToNum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_NANTONUM_H_// End content from: aclnn_nan_to_num.h

// Begin content from: aclnn_gcd.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_GCD_H_
#define OP_API_INC_GCD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGcd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对给定的self和other计算element-wise维度的最大公约数。
 *
 * @param [in] self:
 * 输入`self`，与other满足数据类型推导规则，推导后数据类型支持INT32、INT16(Ascend910B)，shape需要与other满足broadcast规则。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] other:
 * 输入`other`，与self满足数据类型推导规则，推导后数据类型支持INT32、INT16(Ascend910B)，shape需要与self满足broadcast规则。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: 输出`out`，数据类型支持INT32、INT16(Ascend910B)。shape需要与self和other进行broadcast后的shape一致，
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGcdGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGcd的第二段接口，用于执行计算。
 *
 * 算子功能：对给定的self和other计算element-wise维度的最大公约数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGcdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGcd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GCD_H_
// End content from: aclnn_gcd.h

// Begin content from: aclnn_real.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_REAL_H_
#define OP_API_INC_LEVEL2_ACLNN_REAL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACLNN_MAX_SHAPE_RANK 8

/**
 * @brief aclnnReal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素绝对值的结果
 * 计算公式：
 * $$ out_{i} = real(self_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)]--->B([l0op::Contiguous])
 *   B--->C([l0op::Real])
 *   C--->D([l0op::ViewCopy])
 *   D--->E[(out)]
 * ```
 *
 * @param [in] self: 待进行real计算的入参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、COMPLEX32、COMPLEX64、COMPLEX128，数据格式支持ND，支持非连续的Tensor。
 * @param [in] out: real计算的出参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE，数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRealGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnReal的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnRealGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnReal(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_REAL_H_
// End content from: aclnn_real.h

// Begin content from: aclnn_ge_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GETENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_GETENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于等于other Tensor的值，返回一个Bool类型的Tensor，
 * 对应输入Tensor中每个位置的大于等于判断是否成立
 * 计算公式： $$ out_{i}= (self_i >= other_i) ? True : False $$
 *
 * @param [in] self: 待进行ge计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BFLOAT16，数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [in] other: 待进行ge计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BFLOAT16，数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [in] out: ge计算的出参。npu device侧的aclTensor，
 * 输出一个数据类型为BOOL的Tensor，数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGeTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

/**
 * @brief aclnnInplaceGeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于等于other Tensor的值
 * 计算公式： $$ selfRef_{i} = (selfRef_{i} >= other_{i}) ? True : False $$
 *
 * @param [in] selfRef: 待进行ge本地计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL、BFLOAT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: 待进行ge计算的入参,aclTensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGeTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceGeTensor的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceGeTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GETENSOR_H_
// End content from: aclnn_ge_tensor.h

// Begin content from: aclnn_ffn_v3.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file aclnn_ffn_v3.h
 * \brief
 */

#ifndef OP_API_INC_FFN_V3_H
#define OP_API_INC_FFN_V3_H
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFFNV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：该FFN算子提供MoeFFN和FFN的计算功能
 * 计算公式：y=activation(xW1+b1)W2+b2
 * @domain aclnn_ops_infer
 * @param [in]
 * x：必选参数，Device侧的aclTensor，公式中的输入x，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，支持输入的维度最少是2维[M,
 * K1]，最多是8维。
 * @param [in]
 * weight1：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W1，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K1, N1]/[K1, N1]。
 * @param [in]
 * weight2：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W2，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K2, N2]/[K2, N2]。
 * @param [in]
 * expertTokensOptional：可选参数，Device侧的aclTensor类型，代表各专家的token数，数据类型支持INT64，数据格式支持ND，若不为空时可支持的最大长度为256个。
 * @param [in]
 * bias1Optional：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b1，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * bias2Optional：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b2，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * scaleOptional：可选参数，Device侧的aclTensor，量化参数，量化缩放系数，数据类型支持FLOAT32，数据格式支持ND，per-tensor下输入在有/无专家时均为一维向量，输入元素个数在有/无专家时分别为[E]/[1]；per-channel下输入在有/无专家时为二维向量/一维向量，输入元素个数在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * offsetOptional：可选参数，Device侧的aclTensor，量化参数，量化偏移量，数据类型支持FLOAT32，数据格式支持ND，一维向量，输入元素个数在有/无专家时分别为[E]/[1]。
 * @param [in]
 * deqScale1Optional：可选参数，Device侧的aclTensor，量化参数，第一个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * deqScale2Optional：可选参数，Device侧的aclTensor，量化参数，第二个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * antiquantScale1Optional：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantScale2Optional：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * antiquantOffset1Optional：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantOffset2Optional：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * activation：必选参数，Host侧的属性值，代表使用的激活函数，公式中的activation，当前支持fastgelu/gelu/relu/silu以及geglu/swiglu/reglu。
 * @param [in]
 * innerPrecise：可选参数，Host侧的int，表示高精度或者高性能选择。数据类型支持：INT64。该参数仅对FLOAT16生效，BFLOAT16和INT8不区分高精度和高性能。
 * @param [in] tokensIndexFlag：可选参数，Host侧的bool，指示expertTokens是否为索引值。数据类型支持：bool。
 * @param [out] y：输出Tensor，公式中的输出y，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，输出维度与x一致。
 * @param [out] workspaceSize：返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor：返回op执行器，包含了算子计算流程。
 * @return      aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFFNV3GetWorkspaceSize(
    const aclTensor *x, const aclTensor *weight1, const aclTensor *weight2, const aclTensor *expertTokensOptional,
    const aclTensor *bias1Optional, const aclTensor *bias2Optional, const aclTensor *scaleOptional,
    const aclTensor *offsetOptional, const aclTensor *deqScale1Optional, const aclTensor *deqScale2Optional,
    const aclTensor *antiquantScale1Optional, const aclTensor *antiquantScale2Optional,
    const aclTensor *antiquantOffset1Optional, const aclTensor *antiquantOffset2Optional, const char *activation,
    int64_t innerPrecise, bool tokensIndexFlag, const aclTensor *y, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFFNV3的第二段接口，用于执行计算。
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小，由第一段接口aclnnFFNV3GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL stream流。
 * @return     aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFFNV3(void *workspace, uint64_t workspaceSize,
                                                              aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_FFN_V3_H// End content from: aclnn_ffn_v3.h

// Begin content from: aclnn_hardshrink.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_HARDSHRINK_H_
#define OP_API_INC_HARDSHRINK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardshrink的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 功能描述：以元素为单位，强制收缩λ范围内的元素。
 * 计算公式：如下
 * $$
 * Hardshrink(x)=
 * \begin{cases}
 * x, if x > λ \\
 * x, if x < -λ \\
 * 0, otherwise \\
 * \end{cases}
 * $$
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [in]   lambd
 * 输入Scalar，数据类型支持FLOAT。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnHardshrinkGetWorkspaceSize(const aclTensor* self, const aclScalar* lambd, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnHardshrink的第二段接口，用于执行计算。
 * 功能描述：以元素为单位，强制收缩λ范围内的元素。
 * 计算公式：如下
 * $$
 * Hardshrink(x)=
 * \begin{cases}
 * x, if x > λ \\
 * x, if x < -λ \\
 * 0, otherwise \\
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B -->C([l0op::HardShrink])
    C -->D([l0op::Cast])
    D -->E([l0op::ViewCopy])
    E -->F[(Out)]

    G((lambd)) -->C
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnHardshrinkGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardshrink(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSHRINK_H_
// End content from: aclnn_hardshrink.h

// Begin content from: aclnn_renorm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RENORM_H_
#define OP_API_INC_RENORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRenorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：返回一个张量，其中输入张量self沿维度dim的每个子张量都经过归一化，使得子张量的p范数低于maxNorm值。
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT, shape不超过8维。支持非连续Tensor，
 * 支持空Tensor传入，数据格式支持ND。
 * @param [in] p: host侧的aclScalar，数据类型支持FLOAT，表示范数，要求数值大于0。
 * @param [in] dim:
 * host侧的INT64值，表示指定求norm的维度方向，若不指定默认为-1，范围在[-self的维度数量，self的维度数量-1]。
 * @param [in] maxNorm: host侧的aclScalar，数据类型支持FLOAT，表示最大允许的归一化值，要求数值大于等于0。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，数据类型、shape与self一致。
 * 支持非连续Tensor，支持空Tensor传入，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRenormGetWorkspaceSize(const aclTensor* self, const aclScalar* p, int64_t dim,
                                                  const aclScalar* maxNorm, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief: aclnnRenorm的第二段接口，用于执行计算
 *
 * 算子功能：返回一个张量，其中输入张量self沿维度dim的每个子张量都经过归一化，使得子张量的p范数低于maxNorm值。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRenormGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRenorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceRenorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：返回一个张量，其中输入张量selfRef沿维度dim的每个子张量都经过归一化，使得子张量的p范数低于maxNorm值。
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT, shape不超过8维。支持非连续Tensor，
 * 支持空Tensor传入，数据格式支持ND。
 * @param [in] p: host侧的aclScalar，数据类型支持FLOAT，表示范数，要求数值大于0。
 * @param [in] dim:
 * host侧的INT64值，表示指定求norm的维度方向，若不指定默认为-1，范围在[-self的维度数量，self的维度数量-1]。
 * @param [in] maxNorm: host侧的aclScalar，数据类型支持FLOAT，表示最大允许的归一化值，要求数值大于等于0。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRenormGetWorkspaceSize(aclTensor* selfRef, const aclScalar* p, int64_t dim,
                                                         const aclScalar* maxNorm, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceRenorm的第二段接口，用于执行计算
 *
 * 算子功能：返回一个张量，其中输入张量selfRef沿维度dim的每个子张量都经过归一化，使得子张量的p范数低于maxNorm值。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceRenormGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRenorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_RENORM_H_// End content from: aclnn_renorm.h

// Begin content from: aclnn_instance_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_INSTANCE_NORM_H_
#define OP_API_INC_INSTANCE_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInstanceNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成计算 InstanceNorm 的值。
 *
 * @param [in] x: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] gamma: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] beta: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] dataFormat:
 * 字符串儿。表示输入tensor实际的输入数据排布， 当前仅支持NHWC和NCHW。
 * @param [in] eps:
 * double类型数据，norm计算时需要的epsilon参数。
 * @param [in] y: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 且数据类型和 x 保持一致，支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] mean: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 且数据类型和 x 保持一致，支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] variance: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。
 * 且数据类型和 x 保持一致，支持连续和非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInstanceNormGetWorkspaceSize(const aclTensor* x, const aclTensor* gamma,
                                                        const aclTensor* beta, const char* dataFormat, double eps,
                                                        aclTensor* y, aclTensor* mean, aclTensor* variance,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnInstanceNorm的第二段接口，用于执行计算。
 *
 * 算子功能：完成计算输入的k个极值及下标。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInstanceNormGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInstanceNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INSTANCE_NORM_H_
// End content from: aclnn_instance_norm.h

// Begin content from: aclnn_binary_cross_entropy_with_logits_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_BACKEARD_H_
#define OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_BACKEARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBinaryCrossEntropyWithLogitsBackwardGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 * 算子功能：求二元交叉熵反向传播的梯度值
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyWithLogitsBackwardGetWorkspaceSize(
    const aclTensor* gradOutput, const aclTensor* self, const aclTensor* target, const aclTensor* weightOptional,
    const aclTensor* posWeightOptional, int64_t reduction, aclTensor* out, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/*
 * @brief aclnnBinaryCrossEntropyWithLogitsBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyWithLogitsBackward(void* workspace, uint64_t workspaceSize,
                                                                aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_BACKEARD_H_
// End content from: aclnn_binary_cross_entropy_with_logits_backward.h

// Begin content from: aclnn_eye.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_EYE_H_
#define OP_API_INC_LEVEL2_ACLNN_EYE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEye的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回一个新的张量，该张量的对角线元素值均为1，其余位置元素值均为0。
 *
 * @param [in] n: int64_t类型整数。
 * @param [in] m: int64_t类型整数。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、Float32、INT32、INT16、INT8、UINT8、INT64、BOOL、BFLOAT16数据类型，支持非连续的Tensor，
 * 数据格式支持ND，维度只支持二维。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEyeGetWorkspaceSize(int64_t n, int64_t m, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnEye的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEyeGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEye(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_EYE_H_
// End content from: aclnn_eye.h

// Begin content from: aclnn_circular_pad2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CIRCULAR_PAD_H_
#define OP_API_INC_CIRCULAR_PAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCircularPad2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @domain aclnn_ops_train
 *
 * 算子功能：使用输入边界的循环填充输入tensor。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持BFLOAT16,FLOAT16, FLOAT32,INT8, INT32,数据格式支持ND，维度支持三维或四维。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * 前两个数值需小于self最后一维度的数值，后两个数值需小于self倒数第二维度的数值。
 * @param [in] out: npu device侧的aclTensor,
 * 数据类型、数据格式、维度与self一致，倒数第二维度的数值等于self倒数第二维度的
 * 数值加padding后两个值，最后一维度的数值等于self最后一维度的数值加padding前两个值。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnCircularPad2d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界的循环填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCircularPad2dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CIRCULAR_PAD_H_// End content from: aclnn_circular_pad2d.h

// Begin content from: aclnn_aminmax_all.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_AMINMAX_ALL_H_
#define OP_API_INC_AMINMAX_ALL_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAminmaxAll的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] minOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] maxOut: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnAminmaxAllGetWorkspaceSize(const aclTensor* self, aclTensor* minOut, aclTensor* maxOut,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAminmaxAll的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量的最小值和最大值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAminmaxAllGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAminmaxAll(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AMINMAX_ALL_H_
// End content from: aclnn_aminmax_all.h

// Begin content from: aclnn_linalg_cross.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LINALG_CROSS_H_
#define OP_API_INC_LINALG_CROSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLinalgCross的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成linalg_cross操作
 * @param [in] self: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32、UINT8、FLOAT16、FLOAT、FLOAT64、
 *  COMPLEX64、COMPLEX128、BFLOAT16, shape为非空，支持非连续的Tensor，与other有broadcast关系，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32、UINT8、FLOAT16、FLOAT、FLOAT64、
 *  COMPLEX64、COMPLEX128、BFLOAT16, shape为非空，支持非连续的Tensor，与self有broadcast关系，数据格式支持ND。
 * @param [in] dim: 输入INT，默认值为-1。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32、UINT8、FLOAT16、FLOAT、FLOAT64、
 *  COMPLEX64、COMPLEX128、BFLOAT16, shape与self和other的broadcast相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLinalgCrossGetWorkspaceSize(const aclTensor* self, const aclTensor* other, int64_t dim,
                                                       aclTensor* out, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnLinalgCross的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成linalg_cross操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLinalgCrossGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLinalgCross(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LINALG_CROSS_H_// End content from: aclnn_linalg_cross.h

// Begin content from: aclnn_soft_margin_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFT_MARGIN_LOSS_BACKWARD_H_
#define OP_API_INC_SOFT_MARGIN_LOSS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftMarginLossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：均方误差函数的反向传播。
 *
 * @param [in] gradOutput：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，数据类型需要与self相同，shape需要与self、
 * target满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] self：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与gradOutput、target满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] target：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，数据类型需要与self相同，shape需要与gradOutput、
 * self满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction：host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。'none'
 * 表示不应用 减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] out：npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要是target与self、gradOutput满足broadcast
 * 之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftMarginLossBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                  const aclTensor* target, int64_t reduction,
                                                                  aclTensor* out, uint64_t* workspaceSize,
                                                                  aclOpExecutor** executor);

/**
 * @brief aclnnSoftMarginLossBackward的第二段接口，用于执行计算。
 *
 * 算子功能：均方误差函数的反向传播。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口
 * aclnnSoftMarginLossBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftMarginLossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFT_MARGIN_LOSS_BACKWARD_H_
// End content from: aclnn_soft_margin_loss_backward.h

// Begin content from: aclnn_quant_matmul_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_QUANT_MATMUL_V3_
#define OP_API_INC_QUANT_MATMUL_V3_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现quantBatchMatmulV3计算
 * @brief aclnnQuantMatmulV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] x1: matmul左矩阵，数据类型支持：int8, int4, int32。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8, int4, int32。
 * @param [in] scale: 量化参数，数据类型支持：uint64_t, float32, int64_t, bfloat16。
 * @param [in] offset: 量化参数，数据类型支持：float32。
 * @param [in] bias: 偏置，数据类型支持：int32_t, bfloat16, float32。
 * @param [in] transposeX1: a矩阵是否转置，默认值：false。
 * @param [in] transposeX2: b矩阵是否转置，默认值：false。
 * @param [out] out: 计算结果，数据类型：half, int8, bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulV3GetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                         const aclTensor* scale, const aclTensor* offset,
                                                         const aclTensor* bias, bool transposeX1, bool transposeX2,
                                                         const aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmulV3的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnQuantMatmulV3GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulV3(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_V3_// End content from: aclnn_quant_matmul_v3.h

// Begin content from: aclnn_sum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_SUM_H_
#define OP_API_INC_SUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入tensors列表中每个tensor的元素和。
 *
 * @param [in] tensors: npu device侧的aclTensorList，数据类型支持整型，浮点类型，shape需要与out满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSumGetWorkspaceSize(const aclTensorList* tensors, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnSum的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入tensors列表中每个tensor的元素和。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnSumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SUM_H_
// End content from: aclnn_sum.h

// Begin content from: aclnn_foreach_addcmul_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCMUL_LIST_H_
#define ACLNN_FOREACH_ADDCMUL_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcmulListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcmulList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcmulList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcmul_list.h

// Begin content from: aclnn_adaptive_avg_pool3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AdaptiveAvgPool3dBackward_H_
#define OP_API_INC_AdaptiveAvgPool3dBackward_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveAvgPool3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：aclnnAdaptiveAvgPool3d反向运算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * @param [in] gradOutput: 正向运算结果，npu device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT32数据类型，
 * 且数据类型与self一致。支持非连续的Tensor，数据格式支持NCDHW、CDHW，且数据格式需要与self一致。
 * @param [in] self: 正向输入，npu device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT32数据类型,
 * 支持非连续的Tensor，数据格式支持NCDHW、CDHW
 * @param [out] out: 运算结果，npu device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT32数据类型，
 * 且数据类型与gradOutput一致。支持非连续的Tensor，数据格式支持NCDHW、CDHW，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnAdaptiveAvgPool3dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                     aclTensor* out, uint64_t* workspaceSize,
                                                                     aclOpExecutor** executor);

/**
 * @brief aclnnAdaptiveAvgPool3dBackward的第二段接口，用于执行计算。
 *
 * 算子功能：aclnnAdaptiveAvgPool3d反向运算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnAdaptiveAvgPool3dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAdaptiveAvgPool3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AdaptiveAvgPool3dBackward_H_// End content from: aclnn_adaptive_avg_pool3d_backward.h

// Begin content from: aclnn_foreach_abs.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ABS_H_
#define ACLNN_FOREACH_ABS_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAbsGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAbsGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAbs
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAbs(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_abs.h

// Begin content from: aclnn_glu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GLU_H_
#define OP_API_INC_GLU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGlu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：GLU是一个门控线性单元函数，它将输入张量沿着指定的维度dim平均分成两个张量，
 * 并将其前部分张量与后部分张量的Sigmoid函数输出的结果逐元素相乘.
 *
 * $$
 * GLU(a,b)=a \otimes \sigma(b)
 * $$
 *
 * a表示的是输入张量根据指定dim进行均分后的前部分张量，b表示后半部分张量。
 *
 * 计算图：
 * 1、当self的dtype等于out的dtype时：
 * ```mermaid
 * graph LR
 *    A[(self)] -->B([l0op::Contiguous])
 *    B -->D([l0op::SplitV])--a--> C3
 *    E((dim)) -->D--b-->D1([l0op::Sigmoid])-->C3([l0op::Mul])
 *    C3 -->F1([l0op::ViewCopy])--> J[(out)]
 * ```
 *
 * 2、当self的dtype不等于out的dtype时：
 * ```mermaid
 * graph LR
 *    A[(self)] -->B([l0op::Contiguous])
 *    B -->D([l0op::SplitV])--a--> C3
 *    E((dim)) -->D--b-->D1([l0op::Sigmoid])-->C3([l0op::Mul])
 *    C3 -->F0([l0op::Cast])-->F1([l0op::ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: 数据类型支持DOUBLE,FLOAT,BFLOAT16,FLOAT16数据类型，tensor的维度必须大于0，
 * 且shape必须在入参dim对应的维度上可以整除2，shape表示为$(*_1,N,*_2)$其中$*$表示任何数量的附加维，$N$表示dim指定的维度大小，
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] dim: 表示要拆分输入self的维度，数据类型支持INT64，取值范围[-self.dim，self.dim-1]。
 * @param [out] out: 数据类型支持DOUBLE,FLOAT,BFLOAT16,FLOAT16数据类型，数据类型必须可以由self cast得到，
 * shape为$(*_1,M,*_2)$其中$*$表示self中对应维度，$M = N /2$，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGluGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGlu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGlu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GLU_H_
// End content from: aclnn_glu.h

// Begin content from: aclnn_ceil.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CEIL_H_
#define OP_API_INC_LEVEL2_ACLNN_CEIL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCeil的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素向下取整的结果
 * 计算公式：
 * $$ out_{i} =ceil(self_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::ceil])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: 待进行ceil计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [in] out: ceil计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与self一致， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCeilGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnCeil的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnCeilGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCeil(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceCeil的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: 待进行ceil计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCeilGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceCeil的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnCeilGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCeil(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CEIL_H_// End content from: aclnn_ceil.h

// Begin content from: aclnn_addmv.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADDMV_H_
#define OP_API_INC_ADDMV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddmv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAddmvGetWorkspaceSize(const aclTensor* self, const aclTensor* mat, const aclTensor* vec,
                                                 const aclScalar* alpha, const aclScalar* beta, aclTensor* out,
                                                 int8_t cubeMathType, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnAddmv的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAddmv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_addmv.h

// Begin content from: aclnn_prompt_flash_attention_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License. 
 */

#ifndef ACLNN_PROMPT_FLASH_ATTENTION_V3_H_
#define ACLNN_PROMPT_FLASH_ATTENTION_V3_H_
// #include "aclnn/acl_meta.h"
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief The first interface of aclnnPromptFlashAttentionV3 is used to calculate the workspace size based on the specific calculation process.
 * @domain aclnn_ops_infer
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttentionV3GetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *pseShift,
    const aclTensor *attenMask, // attenMask of V3
    const aclIntArray *actualSeqLengths,
    const aclIntArray *actualSeqLengthsKv,
    const aclTensor *deqScale1,
    const aclTensor *quantScale1,
    const aclTensor *deqScale2,
    const aclTensor *quantScale2,
    const aclTensor *quantOffset2,
    int64_t numHeads, // q_n of V3
    double scaleValue,
    int64_t preTokens,
    int64_t nextTokens,
    char *inputLayout,
    int64_t numKeyValueHeads,
    int64_t sparseMode, // sparse of V3
    int64_t innerPrecise,
    const aclTensor *attentionOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief The second interface of aclnnPromptFlashAttentionV3 is used to perform calculations.
*/
__attribute__ ((visibility("default"))) aclnnStatus aclnnPromptFlashAttentionV3(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_prompt_flash_attention_v3.h

// Begin content from: aclnn_weight_quant_batch_matmul_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V3_H_
#define OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V3_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnWeightQuantBatchMatmulV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmulV3GetWorkspaceSize(
    const aclTensor* x, const aclTensor* weight, const aclTensor* antiquantScale,
    const aclTensor* antiquantOffsetOptional, const aclTensor* quantScaleOptional, const aclTensor* quantOffsetOptional,
    const aclTensor* biasOptional, int antiquantGroupSize, int innerPrecise, const aclTensor* y, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnWeightQuantBatchMatmulV3的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmulV3(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_WEIGHT_QUANT_BATCH_MATMUL_V3_H_// End content from: aclnn_weight_quant_batch_matmul_v3.h

// Begin content from: aclnn_lerp_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_LERP_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_LERP_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLerps的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：根据给定的权重，在起始和结束Tensor之间进行线性插值，返回插值后的Tensor。

 *
 * 计算图：
 * graph LR
 * ```mermaid
 * A1[(self)] --> B1([l0op::Contiguous])
 * B1 --> C([l0op::Lerp])
 * A2[(end)] --> B2([l0op::Contiguous])
 * B2 --> C
 * A3((weight)) --> C
 * C --> D([l0op::Cast])
 * D --> E([l0op::ViewCopy]) --> F[(out)]
 * ```
 *
 * @param [in] self: 公式中的输入start，数据类型支持FLOAT16、FLOAT，shape需要与end满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] end: 公式中的输入end，数据类型支持FLOAT16、FLOAT且与self的数据类型一致，shape需要与self满足broadcast关系。
 * 支持非连续Tensor，数据格式支持ND。
 * @param [in] weight: 公式中的输入weight，数据类型支持FLOAT16、FLOAT。
 * @param [in] out: 公式中的out，数据类型支持FLOAT16、FLOAT且与self的数据类型一致、shape与self和end
 * broadcast之后的shape一致。支持非连续Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLerpsGetWorkspaceSize(const aclTensor* self, const aclTensor* end, const aclScalar* weight,
                                                 aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLerps的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLerpsGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLerps(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnInplaceLerps的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：根据给定的权重，在起始和结束Tensor之间进行线性插值，返回插值后的Tensor。

 *
 * @param [in] selfRef: 公式中的输入start，数据类型支持FLOAT16、FLOAT，shape需要与end满足broadcast关系，且broadcast后的shape与selfRef一致。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] end: 公式中的输入end，数据类型支持FLOAT16、FLOAT且与self的数据类型一致，shape需要与selfRef满足broadcast关系，且broadcast后的shape与selfRef一致。
 * 支持非连续Tensor，数据格式支持ND。
 * @param [in] weight: 公式中的输入weight，数据类型支持FLOAT16、FLOAT。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLerpsGetWorkspaceSize(aclTensor* selfRef, const aclTensor* end,
                                                        const aclScalar* weight, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLerps的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLerpsGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLerps(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_LERP_SCALAR_H_// End content from: aclnn_lerp_scalar.h

// Begin content from: aclnn_fmod_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_FMOD_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_FMOD_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFmodTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回self除以other的余数。
 * 计算公式：$$ out_{i} = self_{i} - (other_{i} *\left \lfloor (self_{i}/other_{i}) \right \rfloor) $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(selfRef)] -->B([l0op::Contiguous])
 * B -->C([l0op::Cast])
 * C -->D([l0op::Mod])
 * E[(other)]-->F([l0op::Contiguous])
 * F -->H([l0op::Cast])
 * H --> D
 * D--> G([l0op::Cast])
 * G --> I([l0op::ViewCopy])
 * I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor
 * 数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与other的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与self的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持DOUBLE、BFLOAT16、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8，
 * shape需要是self与other broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFmodTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFmodTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnFmodTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFmodTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceFmodTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回selfRef除以other的余数。
 * 计算公式：$$ out_{i} = self_{i} - (other_{i} *\left \lfloor (self_{i}/other_{i}) \right \rfloor) $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(selfRef)] -->B([l0op::Contiguous])
 * B -->C([l0op::Cast])
 * C -->D([l0op::Mod])
 * E[(other)]-->F([l0op::Contiguous])
 * F -->H([l0op::Cast])
 * H --> D
 * D--> G([l0op::Cast])
 * G --> I([l0op::ViewCopy])
 * I --> J[(out)]
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor
 * 数据类型支持DOUBLE、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与other的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持DOUBLE、FLOAT16、FLOAT32、INT32、INT64、INT8、UNIT8
 * 且数据类型与self的数据类型需满足数据类型推导规则。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFmodTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFmodTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceFmodTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFmodTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FMOD_TENSOR_H_
// End content from: aclnn_fmod_tensor.h

// Begin content from: aclnn_pow_tensor_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_POW_TENSOR_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_POW_TENSOR_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPowTensorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、
 * INT16、INT8、UINT8、BOOL、COMPLEX64、COMPLEX128，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] exponent: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、
 * INT64、INT16、INT8、UINT8、BOOL、COMPLEX64、COMPLEX128，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、
 * INT16、INT8、UINT8、BOOL、COMPLEX64、COMPLEX128，且数据类型需要是self与other推导之后可转换的数据类型，
 * shape为self与other两者broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPowTensorTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* exponent,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnInplacePowTensorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplacePowTensorTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* exponent,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPowTensorTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnPowTensorTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPowTensorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplacePowTensorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_POW_TENSOR_TENSOR_H_
// End content from: aclnn_pow_tensor_tensor.h

// Begin content from: aclnn_dynamic_quant_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef ACLNN_DYNAMIC_QUANT_V2_H_
#define ACLNN_DYNAMIC_QUANT_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDynamicQuantV2GetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
__attribute__((visibility("default"))) aclnnStatus aclnnDynamicQuantV2GetWorkspaceSize(
    const aclTensor* x, const aclTensor* smoothScalesOptional, const aclTensor* groupIndexOptional, int64_t dstType,
    const aclTensor* yOut, const aclTensor* scaleOut, const aclTensor* offsetOut, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnDynamicQuantV2的第二段接口，用于执行计算。
 */
__attribute__((visibility("default"))) aclnnStatus aclnnDynamicQuantV2(void* workspace, uint64_t workspaceSize,
                                                                       aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // ACLNN_DYNAMIC_QUANT_V2_H_// End content from: aclnn_dynamic_quant_v2.h

// Begin content from: aclnn_grouped_matmul_v4.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_MATMUL_V4_H
#define OP_API_INC_GROUPED_MATMUL_V4_H
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

typedef enum {
    GMM_ACT_TYPE_NONE = 0LL,
    GMM_ACT_TYPE_RELU = 1LL,
    GMM_ACT_TYPE_GELU_TANH = 2LL,
    GMM_ACT_TYPE_GELU_ERR_FUNC = 3LL,
    GMM_ACT_TYPE_FAST_GELU = 4LL,
    GMM_ACT_TYPE_SILU = 5LL,
} GMMActType;

/**
 * @brief aclnnGroupedMatmulV4的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 表示公式中的x，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] weight:
 * 表示公式中的weight，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32、INT4数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] biasOptional:
 * 表示公式中的bias，数据类型支持FLOAT16、FLOAT32、INT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] scaleOptional: 表示量化参数，数据类型支持UINT64、BFLOAT16、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] offsetOptional: 表示量化参数，数据类型支持FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantScaleOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantOffsetOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] perTokenScaleOptional:
 * 表示per token量化参数，数据类型支持FLOAT32数据类型，数据格式支持ND，支持的最大长度为1个。
 * @param [in] groupListOptional: 可选参数，代表输入和输出分组轴上的索引情况，数据类型支持INT64，
 * 部分场景支持的最大长度为1024个（详见接口文档约束说明），其余场景支持的最大长度为128个。
 * @param [in] activationInputOptional: 可选参数，代表激活函数的反向输入。
 * @param [in] activationQuantScaleOptional: 可选参数，预留参数。
 * @param [in] activationQuantOffsetOptional: 可选参数，预留参数。
 * @param [in] splitItem:
 * 整数型参数，代表输出是否要做tensor切分，0/1代表输出为多tensor；2/3代表输出为单tensor，默认值为0。
 * @param [in] groupType:
 * 整数型参数，代表需要切分的轴，-1代表不需要切分；0代表需要切分M轴；1代表需要切分N轴；2代表需要切分K轴。
 * @param [in] groupListType:
 * 整数型参数，可取值0或1，0代表groupListOptional中数值为分组轴大小的cumsum结果（累积和），
 * 1代表groupListOptional中数值为分组轴上每组大小。
 * @param [in] actType:整数型参数，代表激活函数类型，各激活函数枚举值参考枚举类GMMActType。
 * @param [out] out: 表示公式中的out，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [out] activationFeatureOutOptional: 激活函数的输入数据。
 * @param [out] dynQuantScaleOutOptional: 预留参数。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV4GetWorkspaceSize(
    const aclTensorList *x, const aclTensorList *weight, const aclTensorList *biasOptional,
    const aclTensorList *scaleOptional, const aclTensorList *offsetOptional,
    const aclTensorList *antiquantScaleOptional, const aclTensorList *antiquantOffsetOptional,
    const aclTensorList *perTokenScaleOptional, const aclTensor *groupListOptional,
    const aclTensorList *activationInputOptional, const aclTensorList *activationQuantScaleOptional,
    const aclTensorList *activationQuantOffsetOptional,  int64_t splitItem, int64_t groupType,
    int64_t groupListType, int64_t actType, aclTensorList *out, aclTensorList *activationFeatureOutOptional,
    aclTensorList *dynQuantScaleOutOptional, uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnGroupedMatmulV4的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV4(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_grouped_matmul_v4.h

// Begin content from: aclnn_logical_and.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LogicalAnd_H_
#define OP_API_INC_LogicalAnd_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogicalAnd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成逻辑与计算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalAnd])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalAnd])
 * D([LogicalAnd])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128，
 * shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128,
 * shape需要与self满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128，
 * shape为self与other两者broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalAndGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogicalAnd的第二段接口，用于执行计算。
 *
 * 算子功能：完成逻辑与计算
 *
 * 实现说明:
 * api计算的基本路径:
 * ```mermaid
 * graph LR
 * A1[(self)] -->B1([Contiguous])-->C1([Cast])-->D([LogicalAnd])
 * A2[(other)]-->B2([Contiguous])-->C2([Cast])-->D([LogicalAnd])
 * D([LogicalAnd])-->E([Cast])-->F([ViewCopy])-->G[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalAndGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalAnd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceLogicalAnd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成逻辑与计算
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128，
 * shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、
 * BOOL、COMPLEX64、COMPLEX128,
 * shape需要与selfRef满足broadcast关系。支持非连续的Tensor，数据格式支持ND，且数据格式需要与selfRef一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalAndGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLogicalAnd的第二段接口，用于执行计算。
 *
 * 算子功能：完成逻辑与计算
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalAndGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalAnd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LogicalAnd_H_
// End content from: aclnn_logical_and.h

// Begin content from: aclnn_nll_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_NLL_LOSS_H_
#define OP_API_INC_NLL_LOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNLLLoss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：计算负对数似然损失值。
 *
 * @param [in] self: npu device侧的aclTensor，shape为(N,C)或者(C,)，其中N表示batch
 * size，C表示类别数。数据类型支持FLOAT，支持非连续的Tensor, 数据格式支持ND。
 * @param [in] target: npu device侧的aclTensor，表示真实标签，shape为(N,) 或者(1,)，其中每个元素的取值范围是[0, C-1]。
 * 数据类型支持INT64，UINT8 ，支持非连续的Tensor， 数据格式支持ND。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] weight: npu
 * device侧的aclTensor，表示每个类别的缩放权重，shape为(C,)。数据类型支持FLOAT，支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: host侧的int64_t，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。'none'
 * 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] ignoreIndex: host侧的int64_t，指定一个被忽略且不影响输入梯度的目标值。
 * @param [in] out: npu device侧的aclTensor，shape(N,)为或者(1,)。数据格式支持ND。
 * @param [in] totalWeightOut: npu device侧的aclTensor，shape为(1,)。数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLossGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                   const aclTensor* weight, int64_t reduction, int64_t ignoreIndex,
                                                   aclTensor* out, aclTensor* totalWeightOut, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnNLLLoss的第二段接口，用于执行计算。
 *
 * 算子功能：计算负对数似然损失值。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnNLLLossGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLoss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_nll_loss.h

// Begin content from: aclnn_upsample_nearest_3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_3D_BACKWARD_H_
#define OP_API_INC_UNAMPLE_NEAREST_3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest3dBackwardGetWorkspaceSize(
    const aclTensor* gradOut, const aclIntArray* outputSize, const aclIntArray* inputSize, double scalesD,
    double scalesH, double scalesW, aclTensor* gradInput, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest3dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_3D_BACKWARD_H_
// End content from: aclnn_upsample_nearest_3d_backward.h

// Begin content from: aclnn_nonzero.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_NONZERO_H_
#define OP_API_INC_NONZERO_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNonzero的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回包含输入张量所有非零元素索引的张量。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT64。数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNonzeroGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnNonzero的第二段接口，用于执行计算。
 *
 * 算子功能：返回包含输入张量所有非零元素索引的张量。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNonzeroGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNonzero(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NONZERO_H_
// End content from: aclnn_nonzero.h

// Begin content from: aclnn_prelu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PRELU_BACKWARD_H_
#define OP_API_INC_PRELU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPreluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成PRelu的反向。
 *
 * @param [in] gradOutput：反向传播的梯度值，即上一层的输出梯度。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32类型。
 * @param [in] self：PRelu的输出值。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32类型。
 * @param [in] weight：PRelu的权重。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32类型。
 * @param [out] gradInput：backward的输出，为输入的梯度值，即对输入进行求导后的结果。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32类型。
 * @param [out] gradWeight：backward的输出，为weight的梯度值，即对weight进行求导后的结果。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32类型。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPreluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                         const aclTensor* weight, aclTensor* gradInput,
                                                         aclTensor* gradWeight, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnPreluBackward的第二段接口，用于执行计算
 */
ACLNN_API aclnnStatus aclnnPreluBackward(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                         aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PRELU_BACKWARD_H_
// End content from: aclnn_prelu_backward.h

// Begin content from: aclnn_mse_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MSE_LOSS_H_
#define OP_API_INC_MSE_LOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMseLoss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与target满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] target: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLossGetWorkspaceSize(const aclTensor* self, const aclTensor* target, int64_t reduction,
                                                   aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMseLoss的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入x和目标y中每个元素之间的均方误差。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnMseLossGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLoss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MSE_LOSS_H_
// End content from: aclnn_mse_loss.h

// Begin content from: aclnn_index.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_INDEX_H_
#define OP_API_INC_INDEX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIndex的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnIndex(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnIndex的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnIndexGetWorkspaceSize(const aclTensor* self, const aclTensorList* indices, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

#ifdef __cplusplus
}
#endif
#endif  // OP_API_INC_INDEX_H_// End content from: aclnn_index.h

// Begin content from: aclnn_replication_pad3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REPLICATION_PAD3D_BACKWARD_H_
#define OP_API_INC_REPLICATION_PAD3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReplicationPad3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：边界填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，数据格式支持ND，
 * 维度支持四维或五维且与self和gradInput一致，shape需要与replication_pad3d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持四维或五维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为6，数值依次代表左右上下需要填充的值。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持四维或五维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad3dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                    const aclIntArray* padding, aclTensor* gradInput,
                                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad3dBackward的第二段接口，用于执行计算
 *
 * 算子功能：边界填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReplicationPad3dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REPLICATION_PAD3D_BACKWARD_H_// End content from: aclnn_replication_pad3d_backward.h

// Begin content from: aclnn_trans_convolution_weight.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRANS_CONVOLUTION_WEIGHT_H
#define OP_API_INC_TRANS_CONVOLUTION_WEIGHT_H

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief
 * aclnnCalculateConvolutionWeightSize用于计算调用aclnnconvolution传入的weighttensor需要占用的元素大小
 *
 *
 * @param [in] tensorShape: 用于表达该次Convolution载入矩阵的weight的Shape
 * @param [in] transposed: 用于表达该次Convolution类型， 是否反向
 * @param [in] groups: 用于表达该次Convolution的group
 * @param [in] dataType: 输入Weight的Datatype, 目前仅支持Float16
 * @param [out] weightTensorSize: 根据Convolution内部处理逻辑，计算该输入下Weight转换之后需要多少个元素的数据量
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCalculateConvolutionWeightSize(const aclIntArray* tensorShape, bool transposed,
                                                          int64_t groups, aclDataType dataType,
                                                          uint64_t* weightTensorSize);

/**
 * @brief aclnnTransConvolutionWeight的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将输入tensor转换为指定的dtype\format类型。
 *
 * @param [in] weightIn: 输入是一个待处理的Convolution的weightTensor，格式是正常的ND输入，数据类型支持Float16/Float32
 * 经过此接口处理后此tensor被刷新为预处理后的weightTensor格式根据亲和性进行私有格式的转换, 并且cast为float16数据类型
 * @param [in] transposed: 用于表达该次Convolution类型， 是否反向
 * @param [in] groups: 用于表达该次Convolution的group
 * @param [out] weightOut: 返回输入weight转换为私有格式后的tensor
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTransConvolutionWeightGetWorkspaceSize(const aclTensor* weightIn, bool transposed,
                                                                  const int64_t groups, aclTensor* weightOut,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnTransConvolutionWeight的第二段接口，用于执行计算。
 *
 * 算子功能：将输入tensor转换为指定的dtype\format类型。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnTransConvolutionWeightGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTransConvolutionWeight(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRANS_CONVOLUTION_WEIGHT_H_
// End content from: aclnn_trans_convolution_weight.h

// Begin content from: aclnn_ctc_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_CTCLOSS_H_
#define OP_API_INC_CTCLOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCtcLoss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 计算连接时序分类损失值。
 *
 * 定义$y_{k}^{t}$表示在时刻$t$时真实字符为$k$的概率。(一般地，$y_{k}^{t}$是经过softmax之后的输出矩阵中的一个元素)。
 * 将字符集$L^{'}$可以构成的所有序列的集合称为$L^{'T}$，将$L^{'T}$中的任意一个序列称为路径，并标记为$π$。$π$的分布为公式(1)：
 *
 * $$
 * p(π|x)=\prod_{t=1}^{T}y^{t}_{π_{t}} , \forall π \in L'^{T}. \tag{1}
 * $$
 *
 * 定义多对一(many to one)映射B: $L^{'T} \to L^{\leq T}$，通过映射B计算得到$l \in L^{\leq T}$的条件概率，
 * 等于对应于$l$的所有可能路径的概率之和，公式(2):
 *
 * $$
 * p(l|x)=\sum_{π \in B^{-1}(l)}p(π|x).\tag{2}
 * $$
 *
 * 将找到使$p(l|x)$值最大的$l$的路径的任务称为解码，公式(3)：
 *
 * $$
 * h(x)=^{arg \  max}_{l \in L^{ \leq T}} \ p(l|x).\tag{3}
 * $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 * A[(logProbs)] -->B([l0op::Contiguous])-->D([l0op::CTCLossV2])
 * A1[(targets)] -->B1([l0op::Contiguous])-->D
 * A2[(targetLengths)] -->B2([ConvertToTensor])-->D
 * A3[(inputLengths)] -->B3([ConvertToTensor])-->D
 * A4((blank)) -->D
 * A6((zeroInfinity)) -->D
 * D--negLogLikelihood--> F1([l0op::ViewCopy])--> J[(negLogLikelihoodOut)]
 * D--logAlpha-->H([l0op::ViewCopy])-->J1[(logAlphaOut)]
 * ```
 *
 * @param [in] logProbs(aclTensor*): 数据类型支持FLOAT,DOUBLE数据类型，shape为($T,N,C$)，
 * $T$为输入长度，$N$为批处理大小，$C$为类别数，必须大于0，包括空白标识，该Tensor表示输出的对数概率，
 * 支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] targets(aclTensor*): 数据类型支持INT64,INT32,BOOL,FLOAT,FLOAT16数据类型，当shape为($N,S$)，
 * $S$为不小于$targetLengths$中的最大值的值；或者shape为(SUM($targetLengths$))，假设$targets$是未填充的而且在1维内级联的；
 * 支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] inputLengths(aclIntArray*)：数据类型支持UINT8,INT8,INT16,INT32,INT64，数组长度为$N$，
 * 数组中的每个值必须小于等于$T$。
 * @param [in] targetLengths(aclIntArray*)：数据类型支持UINT8,INT8,INT16,INT32,INT64，数组长度为$N$，
 * 当targets的shape为($N,S$)时，数组中的每个值必须小于等于$S$。
 * @param [in] blank(int)：int整型，空白标识，默认为0，数值必须小于$C$大于等于0。
 * @param [in] zeroInfinity(bool)：bool类型，表示是否将无限损耗和相关梯度归零，默认值为$False$。
 * @param [out] negLogLikelihoodOut(aclTensor*): 输出的损失值，数据类型FLOAT,DOUBLE；
 * 输出一个大小为($N$)的Tensor，支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [out] logAlphaOut(aclTensor*): 数据类型支持FLOAT,DOUBLE数据类型(数据类型必须和logProbs一致)，
 * 表示输入到目标的可能跟踪的概率，该Tensor为3维，支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCtcLossGetWorkspaceSize(const aclTensor* logProbs, const aclTensor* targets,
                                                   const aclIntArray* inputLengths, const aclIntArray* targetlengths,
                                                   int64_t blank, bool zeroInfinity, aclTensor* negLogLikelihoodOut,
                                                   aclTensor* logAlphaOut, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnCtcLoss 的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCtcLossGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCtcLoss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CTCLOSS_H_
// End content from: aclnn_ctc_loss.h

// Begin content from: aclnn_foreach_tan.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_TAN_H_
#define ACLNN_FOREACH_TAN_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachTanGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachTanGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachTan
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachTan(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_tan.h

// Begin content from: aclnn_softmax_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SOFTMAX_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_SOFTMAX_BACKWARD_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftmaxBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnSoftmaxBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* output,
                                                           int64_t dim, aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnSoftmaxBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSoftmaxBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SOFTMAX_BACKWARD_H_
// End content from: aclnn_softmax_backward.h

// Begin content from: aclnn_moe_finalize_routing.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_FINALIZE_ROUTING_H_
#define ACLNN_MOE_FINALIZE_ROUTING_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeFinalizeRoutingGetWorkspaceSize
 * parameters :
 * expandedX : required
 * x1 : required
 * x2Optional : optional
 * bias : required
 * scales : required
 * expandedRowIdx : required
 * expandedExpertIdx : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRoutingGetWorkspaceSize(
    const aclTensor *expandedX,
    const aclTensor *x1,
    const aclTensor *x2Optional,
    const aclTensor *bias,
    const aclTensor *scales,
    const aclTensor *expandedRowIdx,
    const aclTensor *expandedExpertIdx,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeFinalizeRouting
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeFinalizeRouting(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_finalize_routing.h

// Begin content from: aclnn_log.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG_H_
#define OP_API_INC_LOG_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLog的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成自然对数的计算
 * 计算公式：
 * $$ output = log_e(self) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C -->D([Log])
 *     E[(Out)] -->F([Contiguous])
 *     F --> G([Cast])
 *     G -->D([Log])
 *     D --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128。
 *                   支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128。数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnLog的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLogGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog(void* workspace, uint64_t workspace_size, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceLog的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成自然对数的inplace计算。
 * 计算公式：
 * $$ selfRef_i = log_e(selfRef_i) $$
 *
 * @param [in] selfRef:
 * 公式中的输入`selfRef`，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLog的第二段接口，用于执行计算。
 *
 * 算子功能：完成自然对数的inplace计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceAddGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG_H_
// End content from: aclnn_log.h

// Begin content from: aclnn_im2col.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_IM2COL_H_
#define OP_API_INC_IM2COL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIm2col的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnIm2colGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                  const aclIntArray* dilation, const aclIntArray* padding,
                                                  const aclIntArray* stride, const aclTensor* out,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIm2col的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnIm2col(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_im2col.h

// Begin content from: aclnn_quantize.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_QUANTIZE_H_
#define OP_API_INC_QUANTIZE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 
 * 算子功能：对输入向量进行量化
 * 
 * @param [in] x: 输入Tensor，npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。
 * @param [in] scales: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。
 * @param [in] zeroPoints: npu device侧的aclTensor，数据类型支持INT32、INT8、UINT8、BFLOAT16，数据格式支持ND。
 * @param [in] axis: Host侧的整型，需要进行量化的轴，指定的轴不能超过输入x的维度数。
 * @param [in] dtype: 量化输出的数据类型，aclDataType类型，数据类型支持INT8、UINT8、INT32，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnQuantizeGetWorkspaceSize(const aclTensor* x, const aclTensor* scales,
    const aclTensor* zeroPoints, aclDataType dtype, int32_t axis, aclTensor* out, uint64_t* workspaceSize,
    aclOpExecutor** executor);


/**
 * @brief aclnnQuantize的第二段接口，用于执行计算。
 *
 * 算子功能：对输入Tensor进行量化。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTopkGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnQuantize(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_QUANTIZE_H_
// End content from: aclnn_quantize.h

// Begin content from: aclnn_prod.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_PROD_H_
#define OP_API_INC_PROD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnProd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：返回输入tensor的元素的乘积。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64、BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] dtype: host侧的aclDataType，输出tensor的数据类型，需要与out的数据类型一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64、BOOL、COMPLEX64、
 * COMPLEX128，且数据类型与dtype一致。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnProdGetWorkspaceSize(const aclTensor* self, const aclDataType dtype, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnProd的第二段接口，用于执行计算。
 *
 * 算子功能：返回输入tensor的元素的乘积。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnProdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnProd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnProdDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回输入tensor给定维度上每行的乘积。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64、BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] dim: host侧的int64，指定要缩减的维度。
 * @param [in] keepDim: host侧的bool，输出tensor是否保留维度。
 * @param [in] dtype: host侧的aclDataType，输出tensor的数据类型，需要与out的数据类型一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64、BOOL、COMPLEX64、
 * COMPLEX128，且数据类型与dtype一致。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnProdDimGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepDim,
                                                   const aclDataType dtype, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);
/**
 * @brief aclnnProdDim的第二段接口，用于执行计算。
 *
 * 算子功能：返回输入tensor给定维度上每行的乘积。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnProdDimGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnProdDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PROD_H_
// End content from: aclnn_prod.h

// Begin content from: aclnn_incre_flash_attention_v4.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_INCRE_FLASH_ATTENTION_V4_H_
#define ACLNN_INCRE_FLASH_ATTENTION_V4_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIncreFlashAttentionV4GetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] query : required
 * @param [in] key : dynamic
 * @param [in] value : dynamic
 * @param [in] pseShift : optional
 * @param [in] attenMask : optional
 * @param [in] actualSeqLengths : optional
 * @param [in] dequantScale1 : optional
 * @param [in] quantScale1 : optional
 * @param [in] dequantScale2 : optional
 * @param [in] quantScale2 : optional
 * @param [in] quantOffset2 : optional
 * @param [in] antiquantScale : optional
 * @param [in] antiquantOffset : optional
 * @param [in] blocktable : optional
 * @param [in] kvPaddingSize : optional
 * @param [in] numHeads : required
 * @param [in] scaleValue : optional
 * @param [in] inputLayout : optional
 * @param [in] numKeyValueHeads : optional
 * @param [in] blockSize : optional
 * @param [in] innerPrecise : optional
 * @param [out] attentionOut : required
 * @param [out] workspaceSize : size of workspace(output).
 * @param [out] executor : executor context(output).
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV4GetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
    const aclTensor *attenMask, const aclIntArray *actualSeqLengths, const aclTensor *dequantScale1,
    const aclTensor *quantScale1, const aclTensor *dequantScale2, const aclTensor *quantScale2,
    const aclTensor *quantOffset2, const aclTensor *antiquantScale, const aclTensor *antiquantOffset,
    const aclTensor *blocktable, const aclTensor *kvPaddingSize, int64_t numHeads, double scaleValue, char *inputLayout,
    int64_t numKeyValueHeads, int64_t blockSize, int64_t innerPrecise, const aclTensor *attentionOut,
    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * funtion: aclnnIncreFlashAttentionV4
 * @param [in] workspace : workspace memory addr(input).
 * @param [in] workspaceSize : size of workspace(input).
 * @param [in] executor : executor context(input).
 * @param [in] stream : acl stream.
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV4(void *workspace, uint64_t workspaceSize,
                                                                              aclOpExecutor *executor,
                                                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_incre_flash_attention_v4.h

// Begin content from: aclnn_hardshrink_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_HARDSHRINK_BACKWARD_H_
#define OP_API_INC_HARDSHRINK_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardshrinkBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能： 对输入Tensor完成hardShrink backward操作
 * @param [in] gradOutput: 计算输入, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] self: 计算输入, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [in] lambd:计算输入, 指定hardShrinkBackward分段的阈值，数据类型支持FLOAT类型。
 * @param [in] gradInput:计算输出, 数据类型支持FLOAT、FLOAT16, 数据格式支持ND, 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码， 成功返回ACLNN_SUCCESS, 失败返回对应错误码。
 */
ACLNN_API aclnnStatus aclnnHardshrinkBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                              const aclScalar* lambd, aclTensor* gradInput,
                                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnHardshrinkBackward的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成hardShrink backward操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnHardshrinkBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码,成功返回ACLNN_SUCCESS, 失败返回对应错误码。
 */
ACLNN_API aclnnStatus aclnnHardshrinkBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_HARDSHRINK_BACKWARD_H_// End content from: aclnn_hardshrink_backward.h

// Begin content from: aclnn_tanh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TANH_H_
#define OP_API_INC_TANH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成tanh函数计算
 * 计算公式：
 * $$ output_i = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Tanh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BOOL、UINT8、INT8、INT16、INT32、INT64、
 * BFLOAT16(仅ASCEND910B、ASCEND910_93支持), 支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BOOL、UINT8、INT8、INT16、INT32、INT64、
 * BFLOAT16(仅ASCEND910B、ASCEND910_93支持), 支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTanhGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnTanh的第二段接口，用于执行计算。
 * 算子功能：完成tanh函数计算
 * 计算公式：
 * $$ output_i = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Tanh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTanhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceTanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成tanh函数计算
 * 计算公式：
 * $$ output_i = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Tanh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BOOL、UINT8、INT8、INT16、INT32、INT64、
 * BFLOAT16(仅ASCEND910B、ASCEND910_93支持),支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTanhGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceTanh的第二段接口，用于执行计算。
 * 算子功能：完成tanh函数计算
 * 计算公式：
 * $$ output_i = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Tanh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceTanhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TANH_H
// End content from: aclnn_tanh.h

// Begin content from: aclnn_scale.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_SCALE_H_
#define OP_API_INC_LEVEL2_SCALE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScale的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 待进行Scale计算的入参。npu device侧的aclTensor，
 * 数据类型支持float16, bfloat16, float32, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] scale: npu device侧的aclTensor, 数据类型支持float, bf16, float16
 * @param [in] bias: npu device侧的aclTensor，数据类型支持float, bf16, float16
 * @param [in] axis:  host侧的aclScalar，数据类型int64_t
 * @param [in] numAxes:  host侧的aclScalar，数据类型int64_t
 * @param [in] scaleFromBlob:  host侧的aclScalar, 数据类型bool
 * @param [in] y: Scale计算的出参。npu device侧的aclTensor,
 * 数据类型支持int8, 数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScaleGetWorkspaceSize(const aclTensor* x, const aclTensor* scale, const aclTensor* bias,
                                                 int64_t axis, int64_t numAxes, bool scaleFromBlob,
                                                 aclTensor* y, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnScale的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnScaleGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScale(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_SCALE_H_
// End content from: aclnn_scale.h

// Begin content from: aclnn_max_unpool2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_UNPOOL2D_H_
#define OP_API_INC_MAX_UNPOOL2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxUnpool2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMaxUnpool2dGetWorkspaceSize(const aclTensor* self, const aclTensor* indices,
                                                       const aclIntArray* outputSize, aclTensor* out,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxUnpool2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMaxUnpool2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_UNPOOL2D_H_
// End content from: aclnn_max_unpool2d.h

// Begin content from: aclnn_constant_pad_nd.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CONSTANT_PAD_ND_H_
#define OP_API_INC_CONSTANT_PAD_ND_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnConstantPadNd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：使用输入边界填充输入tensor。
 * @param [in] self: 数据类型支持FLOAT16, FLOAT32, DOUBLE, INT8, INT16, INT32, INT64, UINT8,
 * COMPLEX64, COMPLEX128，支持非连续的Tensor，数据格式支持ND。
 * @param [in] pad: 数据类型支持UINT8、INT8、INT16、INT32、INT64，数组长度必须为偶数且不能超过self维度的两倍。
 * @param [in] out: 数据类型、数据格式与self一致，支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConstantPadNdGetWorkspaceSize(const aclTensor* self, const aclIntArray* pad,
                                                         const aclScalar* value, aclTensor* out,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnConstantPadNd的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnConstantPadNdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConstantPadNd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CONSTANT_PAD_ND_H_
// End content from: aclnn_constant_pad_nd.h

// Begin content from: aclnn_cosh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_COSH_H_
#define OP_API_INC_COSH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCosh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成cosh函数计算
 * 计算公式：
 * $$ out = cosh(self) (
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Cosh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、DOUBLE、BFLOAT16、FLOAT16、INT8、INT16、
 * INT32、INT64、UINT8、BOOL、COMPLEX64、COMPLEX128, 支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持 FLOAT、DOUBLE、BFLOAT16、FLOAT16、COMPLEX64、COMPLEX128, 支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCoshGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnCosh的第二段接口，用于执行计算。
 * 算子功能：完成cosh函数计算
 * 计算公式：
 * $$ out = cosh(self)
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Cosh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCoshGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCosh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceCosh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成cosh函数计算
 * 计算公式：
 * $$ out = cosh(self)
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Cosh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持 FLOAT、DOUBLE、FLOAT16、INT8、INT16、
 * INT32、INT64、UINT8、BOOL、COMPLEX64、COMPLEX128,支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCoshGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceCosh的第二段接口，用于执行计算。
 * 算子功能：完成cosh函数计算
 * 计算公式：
 * $$ out = cosh(self)
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Cosh])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceCoshGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCosh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_COSH_H
// End content from: aclnn_cosh.h

// Begin content from: aclnn_batch_norm_gather_stats_with_counts.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_SYNC_BATCH_NORM_GATHER_STATS_WITH_COUNTS_H
#define OP_API_INC_SYNC_BATCH_NORM_GATHER_STATS_WITH_COUNTS_H

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormGatherStatsWithCounts的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormGatherStatsWithCountsGetWorkspaceSize(
    const aclTensor* input, const aclTensor* mean, const aclTensor* invstd, aclTensor* runningMean,
    aclTensor* runningVar, double momentum, double eps, const aclTensor* counts, aclTensor* meanAll,
    aclTensor* invstdAll, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormGatherStatsWithCounts的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormGatherStatsWithCounts(void* workspace, uint64_t workspaceSize,
                                                          aclOpExecutor* executor, const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_batch_norm_gather_stats_with_counts.h

// Begin content from: aclnn_mm.h

/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MM_H_
#define OP_API_INC_MM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMmGetWorkspaceSize(const aclTensor* self, const aclTensor* mat2, aclTensor* out,
                                              int8_t cubeMathType, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMm的第二段接口，用于执行计算。
 */
// only for dims(2x2) patten
ACLNN_API aclnnStatus aclnnMm(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MM_H_
// End content from: aclnn_mm.h

// Begin content from: aclnn_range.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RANGE_H_
#define OP_API_INC_RANGE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRange的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 功能描述：从start起始到end结束按照step的间隔取值，并返回大小为 $ \lfloor \frac{end - start} {step} \rfloor + 1
 * $的1维张量。其中，步长step是张量中 相邻两个值的间隔。
 *
 * 计算公式：$$ out_{i+1}=out_i+step $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Start)]--> E([l0op::Arange])
 *     B[(End)]--> D[(Limit = End + Step)]
 *     C[(Step)]--> D
 *     D--> E
 *     C--> E
 *     E--> H([l0op::Cast])
 *     H--> M([l0op::ViewCopy])
 *     M--> N[(Out)]
 * ```
 *
 * 参数描述：
 * @param [in]   start
 * 获取值的范围的起始位置：host侧的aclScalar，数据类型支持整型，浮点数据类型。数据格式支持ND。需要满足在step大于0时输入的start小于end，或者step小于0时输入的start大于end。
 * @param [in]   end
 * 获取值的范围的结束位置：host侧的aclScalar，数据类型支持整型，浮点数据类型。数据格式支持ND。需要满足在step大于0时输入的start小于end，或者step小于0时输入的start大于end。
 * @param [in]   step
 * 获取值的步长：host侧的aclScalar，数据类型支持整型，浮点数据类型。数据格式支持ND。需要满足step不等于0。
 * @param [in]   out              指定的输出tensor：npu
 * device侧的aclTensor，数据类型支持整型，浮点数据类型，数据格式支持ND。
 * @param [out]  workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRangeGetWorkspaceSize(const aclScalar* start, const aclScalar* end, const aclScalar* step,
                                                 aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnRange的第二段接口，用于执行计算。
 *
 *
 * 功能描述：从start起始到end结束按照step的间隔取值，并返回大小为 $ \lfloor \frac{end - start} {step} \rfloor + 1
 * $的1维张量。其中，步长step是张量中 相邻两个值的间隔。
 *
 * 计算公式：$$ out_{i+1}=out_i+step $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Start)]--> E([l0op::Arange])
 *     B[(End)]--> D[(Limit = End + Step)]
 *     C[(Step)]--> D
 *     D--> E
 *     C--> E
 *     E--> H([l0op::Cast])
 *     H--> M([l0op::ViewCopy])
 *     M--> N[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRangeGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRange(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_range.h

// Begin content from: aclnn_std_mean_correction.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_STD_MEAN_CORRECTION_H_
#define OP_API_INC_STD_MEAN_CORRECTION_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnStdMeanCorrection的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnStdMeanCorrectionGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim,
                                                             int64_t correction, bool keepdim, aclTensor* stdOut,
                                                             aclTensor* meanOut, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnStdMeanCorrection的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnStdMeanCorrection(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_STD_MEAN_CORRECTION_H_
// End content from: aclnn_std_mean_correction.h

// Begin content from: aclnn_elu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ELU_H_
#define OP_API_INC_LEVEL2_ACLNN_ELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：对输入张量self中的每个元素x调用指数线性单元激活函数ELU，并将得到的结果存入输出张量out中。
 * 计算公式：如下
 * $$
 * ELU(x) =
 * \begin{cases}
 * scale \ast x, \quad x > 0\\
 * \alpha \ast scale \ast (exp(x \ast inputScale)-1), \quad x \leq 0
 * \end{cases}
 * $$
 *
 * 实现说明：如下
 *
 * 计算图：如下
 *
 * ```mermaid
 * graph LR
 *     A[(Self)] --> B([l0op::Contiguous])
 *     B --> C([l0op::Elu])
 *     C --> D([l0op::Cast])
 *     D --> E([l0op::ViewCopy])
 *     E --> F[(out)]
 *     G((alpha)) --> C
 *     H((scale)) --> C
 *     I((inputScale)) --> C
 * ```
 */

/**
 * @brief aclnnElu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: 表示ELU激活函数的输入，npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、BFLOAT16，支持非连续
 * 的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] alpha: 表示ELU激活函数的激活系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] scale：表示ELU激活函数的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] inputScale：表示ELU激活函数的输入的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] out: 表示ELU激活函数的输出，npu
 * device侧的aclTensor，数据类型需要是self可转换的数据类型，shape需要与self一致，
 * 支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEluGetWorkspaceSize(const aclTensor* self, const aclScalar* alpha, const aclScalar* scale,
                                               const aclScalar* inputScale, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnElu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnElu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceElu的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_infer。
 * @param [in] selfRef: 表示ELU激活函数的输入，npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、BFLOAT16，支持非
 * 连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] alpha: 表示ELU激活函数的激活系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] scale：表示ELU激活函数的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] inputScale：表示ELU激活函数的输入的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceEluGetWorkspaceSize(aclTensor* selfRef, const aclScalar* alpha,
                                                      const aclScalar* scale, const aclScalar* inputScale,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceElu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceEluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceElu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ELU_H_
// End content from: aclnn_elu.h

// Begin content from: aclnn_grid_sampler3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023 All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_GRID_SAMPLER3D_BACKWARD_H_
#define OP_API_INC_GRID_SAMPLER3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGridSampler3DBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnGridSampler3DBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* input,
                                                                 const aclTensor* grid, int64_t interpolationMode,
                                                                 int64_t paddingMode, bool alignCorners,
                                                                 const aclBoolArray* outputMask, aclTensor* inputGrad,
                                                                 aclTensor* gridGrad, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief aclnnGridSampler3DBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGridSampler3DBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GRID_SAMPLER3D_BACKWARD_H_
// End content from: aclnn_grid_sampler3d_backward.h

// Begin content from: aclnn_upsample_bicubic_2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_BICUBIC_H_
#define OP_API_INC_UNAMPLE_BICUBIC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBicubic2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             const bool alignCorners, const double scalesH,
                                                             const double scalesW, aclTensor* out,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleBicubic2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_BICUBIC_H_
// End content from: aclnn_upsample_bicubic_2d.h

// Begin content from: aclnn_weight_quant_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_weight_quant_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnWeightQuantMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bfloat16。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8,int4。
 * @param [in] bias: 偏置，数据类型支持：float16, bfloat16。
 * @param [in] antiquantScale: 对x2进行伪量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] antiquantOffset: 对x2进行伪量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] residual: 残差，数据类型支持：float16, bfloat16。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [in] antiquantGroupSize: per_group模式的groupSize输入。
 * @param [out] y: MatmulAllReduce+Add(residual)的结果，数据类型：同输入。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */

ACLNN_API aclnnStatus aclnnWeightQuantMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* antiquantScale,
    const aclTensor* antiquantOffset, const aclTensor* residual, const aclTensor* gamma, double epsilon,
    const char* group, const char* reduceOp, int64_t commTurn, int64_t streamMode, int64_t antiquantGroupSize,
    const aclTensor* y, const aclTensor* normOut, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnWeightQuantMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，
 *                             由第一段接口aclnnWeightQuantMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnWeightQuantMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize,
                                                                aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_// End content from: aclnn_weight_quant_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_grid_sampler3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023 All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_GRID_SAMPLER3D_H_
#define OP_API_INC_GRID_SAMPLER3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGridSampler3D的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnGridSampler3DGetWorkspaceSize(const aclTensor* input, const aclTensor* grid,
                                                         int64_t interpolationMode, int64_t paddingMode,
                                                         bool alignCorners, aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnGridSampler3D的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGridSampler3D(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GRID_SAMPLER3D_H_
// End content from: aclnn_grid_sampler3d.h

// Begin content from: aclnn_isin_tensor_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ISIN__TENSOR_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_ISIN__TENSOR_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIsInTensorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] element: npu device侧的aclTensor，支持非连续的Tensor，数据格式支持ND
 * @param [in] testElement: npu device侧的aclScalar
 * @param [in] assumeUnique
 * @param [in] out: npu device侧的aclTensor，数据类型是self可转化的数据类型。数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsInTensorScalarGetWorkspaceSize(const aclTensor* element, const aclScalar* testElement,
                                                            bool assumeUnique, bool invert, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIsInTensorScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnIsInTensorScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsInTensorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ISIN__TENSOR_SCALAR_H_// End content from: aclnn_isin_tensor_scalar.h

// Begin content from: aclnn_binary_cross_entropy_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_BINARY_CROSS_ENTROPY_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_BINARY_CROSS_ENTROPY_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBinaryCrossEntropyBackwardGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 * 算子功能：求二元交叉熵反向传播的梯度值
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyBackwardGetWorkspaceSize(
    const aclTensor* gradOutput, const aclTensor* self, const aclTensor* target, const aclTensor* weightOptional,
    int64_t reduction, aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/*
 * @brief aclnnBinaryCrossEntropyBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                      const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_BINARY_CROSS_ENTROPY_BACKWARD_H_
// End content from: aclnn_binary_cross_entropy_backward.h

// Begin content from: aclnn_tanh_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TANH_BACKWARD_H_
#define OP_API_INC_TANH_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTanhBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：激活函数tanh(x)的反向
 *
 * 实现说明：api
 * 计算的基本路径：如下所示
 * ```mermaid
 * graph LR
 *         A[(gradOutput)] -->B([l0op::Contiguous]) --> C([l0op::TanhGrad])
 *         D[(output)] -->E([l0op::Contiguous]) --> C
 *         C --> F([l0op::ViewCopy])
 *         F --> G[(gradInput)]
 * ```
 *
 * @param [in] gradOutput: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE，
 * 且数据类型与output一致,shape与output相同。支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [in] output: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] gradInput: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTanhBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* output,
                                                        aclTensor* gradInput, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnTanhBackward的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTanhBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTanhBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TANH_BACKWARD_H_
// End content from: aclnn_tanh_backward.h

// Begin content from: aclnn_foreach_add_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADD_SCALAR_H_
#define ACLNN_FOREACH_ADD_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_add_scalar.h

// Begin content from: aclnn_swish_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SWISH_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_SWISH_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSwishBackward的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 * 算子功能：求Swish反向传播的梯度值
 * @param [in] gradOutput: Device侧的aclTensor，公式中的gradOutput。支持非连续的Tensor，数据格式支持ND，
 * gradOutput、self与gradInput的shape和数据类型一致。
 * @param [in] self: Device侧的aclTensor，公式中的x。支持非连续的Tensor，数据格式支持ND，gradOutput、
 * self与gradInput的shape和数据类型一致。
 * @param [in] betaOptional: Host侧的aclScalar，公式中的beta。数据类型需要是可转换为FLOAT的数据类型。
 * 当betaOptional为空指针时，默认值为1.0。
 * @param [out] gradInput: Device侧的aclTensor，公式中的gradInput。支持非连续的Tensor，数据格式支持ND，
 * gradOutput、self与gradInput的shape和数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSwishBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self, 
                                                     const aclScalar* betaOptional, aclTensor* gradInput, 
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSwishBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAcosGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSwishBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SWISH_BACKWARD_H_// End content from: aclnn_swish_backward.h

// Begin content from: aclnn_strided_slice_assign_v2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_STRIDED_SLICE_ASSIGN_V2_H_
#define ACLNN_STRIDED_SLICE_ASSIGN_V2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnStridedSliceAssignV2GetWorkspaceSize
 * parameters :
 * varRef : required
 * inputValue : required
 * begin : required
 * end : required
 * strides : required
 * axesOptional : optional
 * varRef : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnStridedSliceAssignV2GetWorkspaceSize(
    aclTensor *varRef,
    const aclTensor *inputValue,
    const aclIntArray *begin,
    const aclIntArray *end,
    const aclIntArray *strides,
    const aclIntArray *axesOptional,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnStridedSliceAssignV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnStridedSliceAssignV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_strided_slice_assign_v2.h

// Begin content from: aclnn_trans_quant_param_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TRANS_QUANT_PARAM_V2_H
#define OP_API_INC_TRANS_QUANT_PARAM_V2_H

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现transQuantParamV2计算
 * @brief aclnnTransQuantParamV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] scale: 量化参数，数据类型支持： float32。
 * @param [in] offset: 量化参数，数据类型支持：float32。
 * @param [out] out: 计算结果，数据类型：uint64_t
 * @return aclnnStatus: 返回状态码
 */

ACLNN_API aclnnStatus aclnnTransQuantParamV2GetWorkspaceSize(const aclTensor* scale, const aclTensor* offset,
                                                             const aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnTransQuantParamV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnTransQuantParamV2GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnTransQuantParamV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TRANS_QUANT_PARAM_V2_H// End content from: aclnn_trans_quant_param_v2.h

// Begin content from: aclnn_blend_images_custom.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_BLEND_IMAGES_CUSTOM_H_
#define ACLNN_BLEND_IMAGES_CUSTOM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnBlendImagesCustomGetWorkspaceSize
 * parameters :
 * rgb : required
 * alpha : required
 * frame : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBlendImagesCustomGetWorkspaceSize(
    const aclTensor *rgb,
    const aclTensor *alpha,
    const aclTensor *frame,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnBlendImagesCustom
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBlendImagesCustom(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_blend_images_custom.h

// Begin content from: aclnn_swin_attention_score_quant.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_SWIN_ATTENTION_SCORE_QUANT_H_
#define ACLNN_SWIN_ATTENTION_SCORE_QUANT_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnSwinAttentionScoreQuantGetWorkspaceSize
 * parameters :
 * query : required
 * key : required
 * value : required
 * scaleQuant : required
 * scaleDequant1 : required
 * scaleDequant2 : required
 * biasQuantOptional : optional
 * biasDequant1Optional : optional
 * biasDequant2Optional : optional
 * paddingMask1Optional : optional
 * paddingMask2Optional : optional
 * queryTranspose : optional
 * keyTranspose : optional
 * valueTranspose : optional
 * softmaxAxes : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwinAttentionScoreQuantGetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *scaleQuant,
    const aclTensor *scaleDequant1,
    const aclTensor *scaleDequant2,
    const aclTensor *biasQuantOptional,
    const aclTensor *biasDequant1Optional,
    const aclTensor *biasDequant2Optional,
    const aclTensor *paddingMask1Optional,
    const aclTensor *paddingMask2Optional,
    bool queryTranspose,
    bool keyTranspose,
    bool valueTranspose,
    int64_t softmaxAxes,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnSwinAttentionScoreQuant
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwinAttentionScoreQuant(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_swin_attention_score_quant.h

// Begin content from: aclnn_baddbmm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BADDBMM_H_
#define OP_API_INC_BADDBMM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBaddbmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成加法计算
 * 计算公式：计算α与batch1、batch2的矩阵乘结果的乘积，再与β和self的乘积求和
 * $$ out = βself+α(batch1@batch2) $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且需要与batch1@batch2保持一致；shape需要与batch1@batch2满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] batch1: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch2保持一致，shape需要与batch2满足bmm输入约束关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] batch2: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch1保持一致，shape需要与batch1满足bmm输入约束关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] beta: host侧的aclScalar，默认为1
 * @param [in] alpha: host侧的aclScalar，默认为1
 * @param [in] cubeMathType:
 * INT8类型的枚举值，用于判断Cube单元应该使用那种计算逻辑进行运算，可通过此开关使能如HFLOAT32等功能
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，数据类型支持FLOAT、FLOAT16，dtype和format均需要与self、batch1@batch2保持一致。
 * 支持非连续的Tensor，数据格式支持ND。shape不做限制，可传入任意shape，不要求与self、batch1@batch2的shape保持一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBaddbmmGetWorkspaceSize(const aclTensor* self, const aclTensor* batch1,
                                                   const aclTensor* batch2, const aclScalar* beta,
                                                   const aclScalar* alpha, aclTensor* out, int8_t cubeMathType,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnBaddbmm的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：计算α与batch1、batch2的矩阵乘结果的乘积，再与β和self的乘积求和
 * $$ out = βself+α(batch1@batch2) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnBaddbmmGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBaddbmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

/**
 * @brief aclnnInplaceBaddbmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成加法计算
 * 计算公式：计算α与batch1、batch2的矩阵乘结果的乘积，再与β和self的乘积求和
 * $$ out = βself+α(batch1@batch2) $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且需要与batch1@batch2保持一致；shape需要与batch1@batch2满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] batch1: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch2保持一致，shape需要与batch2满足bmm输入约束关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] batch2: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16类型，且数据类型需要与batch1保持一致，shape需要与batch1满足bmm输入约束关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] beta: host侧的aclScalar，默认为1
 * @param [in] alpha: host侧的aclScalar，默认为1
 * @param [in] cubeMathType:
 * INT8类型的枚举值，用于判断Cube单元应该使用那种计算逻辑进行运算，可通过此开关使能如HFLOAT32等功能
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBaddbmmGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* batch1,
                                                          const aclTensor* batch2, const aclScalar* beta,
                                                          const aclScalar* alpha, int8_t cubeMathType,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBaddbmm的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：计算α与batch1、batch2的矩阵乘结果的乘积，再与β和self的乘积求和
 * $$ out = βself+α(batch1@batch2) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceBaddbmmGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBaddbmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BADDBMM_H_
// End content from: aclnn_baddbmm.h

// Begin content from: aclnn_masked_fill_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MASKEF_FILL_SCALAR_H_
#define OP_API_INC_MASKEF_FILL_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceMaskedFillScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：用value填充selfRef里面与mask矩阵中值为true的位置相对应的元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -->B([Contiguous])
 *  B  -->C([Unsqueeze])
 *  C  -->D([MaskedFill])
 *  D  -->I([Squeeze])
 *  I   --> J([ViewCopy])
 *  J   --> K[(out)]
 *
 *  A1[(mask)] -->B1([Contiguous])
 *  B1  -->C1([Cast])
 *  C1  -->D
 *
 *  A2[(value)]-->B2[(Cast)]
 *  B2-->D
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持BOOL、UINT8、INT8、INT16、INT32、INT64、FLOAT、
 *                      FLOAT16、BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128。
 *                      支持非连续的Tensor，数据格式支持ND。
 * @param [in] mask: npu device侧的aclTensor，数据类型支持BOOL。且shape与selfRef满足broadcast关系。数据格式支持ND。
 * @param [in] value: host侧的aclScalar，数据类型需要可转换成selfRef的数据类型。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedFillScalarGetWorkspaceSize(aclTensor* selfRef, const aclTensor* mask,
                                                                   const aclScalar* value, uint64_t* workspaceSize,
                                                                   aclOpExecutor** executor);
/**
 * @brief aclnnInplaceMaskedFillScalar的第二段接口，用于执行计算。
 *
 * 算子功能：用value填充selfRef里面与mask矩阵中值为true的位置相对应的元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -->B([Contiguous])
 *  B  -->C([Unsqueeze])
 *  C  -->D([MaskedFill])
 *  D  -->I([Squeeze])
 *  I   --> J([ViewCopy])
 *  J   --> K[(out)]
 *
 *  A1[(mask)] -->B1([Contiguous])
 *  B1  -->C1([Cast])
 *  C1  -->D
 *
 *  A2[(value)]-->B2[(Cast)]
 *  B2-->D
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceMaskedFillScalarGetWorkspaceSize。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceMaskedFillScalar(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MASKEF_FILL_SCALAR_H_// End content from: aclnn_masked_fill_scalar.h

// Begin content from: aclnn_weight_quant_batch_matmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_WEIGHT_QUANT_MM_H_
#define OP_API_INC_WEIGHT_QUANT_MM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnWeightQuantBatchMatmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmulGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* diagonalMatrix, const aclTensor* deqOffset,
    const aclTensor* deqScale, const aclTensor* addOffset, const aclTensor* mulScale, const aclTensor* bias,
    bool transposeX1, bool transposeX2, float antiquantScale, float antiquantOffset, aclTensor* out,
    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnWeightQuantBatchMatmul的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnWeightQuantBatchMatmul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_WEIGHT_QUANT_MM_H_
// End content from: aclnn_weight_quant_batch_matmul.h

// Begin content from: aclnn_trans_quant_param.h

/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_TRANS_QUANT_PARAM_H_
#define OP_API_INC_LEVEL2_ACLNN_TRANS_QUANT_PARAM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 将scale数据从float类型转换为硬件需要的uint64_t类型
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnTransQuantParam(const float* scaleArray, uint64_t scaleSize, const float* offsetArray,
                                           uint64_t offsetSize, uint64_t** quantParam, uint64_t* quantParamSize);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_TRANS_QUANT_PARAM_H_
// End content from: aclnn_trans_quant_param.h

// Begin content from: aclnn_min_dim.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MIN_DIM_H_
#define OP_API_INC_MIN_DIM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMinDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回张量在指定维度上的最小值的索引。
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMinWithValue])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [in] dim: host侧int64类型，指定了要进行最大值计算的维度。
 * @param [in] keepdim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持INT32、INT64。数据格式支持ND。支持非连续的Tensor。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinDimGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepdim,
                                                  aclTensor* out, aclTensor* indices, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnArgMax的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：返回张量在指定维度上的最小值的索引。
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMinWithValue])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnArgMaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MIN_DIM_H_// End content from: aclnn_min_dim.h

// Begin content from: aclnn_shrink.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SHRINK_H_
#define OP_API_INC_SHRINK_H_

// #include "aclnn/aclnn_base.h"
// #include"aclnn_util.h"

#ifdef __cplusplus
extern "C"{
#endif

/**
 * @brief aclnnShrink的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer、aclnn_ops_train
 * 功能描述：对输入张量进行非线性变换，根据输入值self与阈值lambd的关系，对输入通过偏移量bias进行缩放和偏移处理。
 * 计算公式：如下
 * $$
 * out=
 * \begin{cases}
 * x-bias, if x > lambd \\
 * x+bias, if x < -lambd \\
 * 0, otherwise \\
 * \end{cases}
 * $$
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，FLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [in]   lambd
 * 输入Scalar，数据类型支持FLOAT。
 * @param [in]   bias
 * 输入Scalar，数据类型支持FLOAT。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnShrinkGetWorkspaceSize(const aclTensor* self, const aclScalar* lambd, const aclScalar* bias, aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnShrink的第二段接口，用于执行计算。
 * 功能描述：对输入张量进行非线性变换，根据输入值self与阈值lambd的关系，对输入通过偏移量bias进行缩放和偏移处理。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnShrinkGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnShrink(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif //OP_API_INC_SHRINK_H_// End content from: aclnn_shrink.h

// Begin content from: aclnn_max_unpool3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_UNPOOL3D_H_
#define OP_API_INC_MAX_UNPOOL3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxUnpool3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMaxUnpool3dGetWorkspaceSize(const aclTensor* self, const aclTensor* indices,
                                                       const aclIntArray* outputSize, const aclIntArray* stride,
                                                       const aclIntArray* padding, aclTensor* outRef,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaxUnpool3d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMaxUnpool3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_UNPOOL3D_H_
// End content from: aclnn_max_unpool3d.h

// Begin content from: aclnn_apply_rotary_pos_emb.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_APPLY_ROTARY_POS_EMB_H_
#define ACLNN_APPLY_ROTARY_POS_EMB_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnApplyRotaryPosEmbGetWorkspaceSize
 * parameters :
 * queryRef : required
 * keyRef : required
 * cos : required
 * sin : required
 * layout : optional
 * queryRef : required
 * keyRef : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnApplyRotaryPosEmbGetWorkspaceSize(
    aclTensor *queryRef,
    aclTensor *keyRef,
    const aclTensor *cos,
    const aclTensor *sin,
    int64_t layout,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnApplyRotaryPosEmb
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnApplyRotaryPosEmb(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_apply_rotary_pos_emb.h

// Begin content from: aclnn_foreach_sub_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_SUB_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_SUB_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachSubScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素进行scalar相减运算后输出。
 * 计算公式：
 * out_{i}=x_{i}-scalar
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachSubScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachSubScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素进行scalar相减运算后输出。
 * 计算公式：
 * out_{i}=x_{i}-scalar
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachSubScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachSubScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sub_scalar_v2.h

// Begin content from: aclnn_deep_norm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_DEEP_NORM_H_
#define ACLNN_DEEP_NORM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnDeepNormGetWorkspaceSize
 * parameters :
 * x : required
 * gx : required
 * beta : required
 * gamma : required
 * alpha : optional
 * epsilon : optional
 * meanOut : required
 * rstdOut : required
 * yOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnDeepNormGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *gx,
    const aclTensor *beta,
    const aclTensor *gamma,
    double alpha,
    double epsilon,
    const aclTensor *meanOut,
    const aclTensor *rstdOut,
    const aclTensor *yOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnDeepNorm
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnDeepNorm(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_deep_norm.h

// Begin content from: aclnn_foreach_add_list_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_ADD_LIST_V2_H_
#define OP_API_INC_ACLNN_FOREACH_ADD_LIST_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachAddListV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：两个Tensor列表中的元素逐个相加，并返回一个新的Tensor列表。可以通过设置alpha参数来调整相加的系数。
 * 计算公式：
 * out_{i}=x1_{i}+x2_{i}*alpha
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x1
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   x2
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   alpha
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachAddListV2GetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclScalar *alpha,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachAddListV2的第二段接口，用于执行计算。
 * 功能描述：两个Tensor列表中的元素逐个相加，并返回一个新的Tensor列表。可以通过设置alpha参数来调整相加的系数。
 * 计算公式：
 * out_{i}=x1_{i}+x2_{i}*alpha
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachAddListV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachAddListV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_add_list_v2.h

// Begin content from: aclnn_kl_div.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_KL_DIV_H_
#define OP_API_INC_KL_DIV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnKlDiv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnKlDivGetWorkspaceSize(const aclTensor* self, const aclTensor* target, int64_t reduction,
                                                 bool logTarget, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnKlDiv的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnKlDiv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_KL_DIV_H_
// End content from: aclnn_kl_div.h

// Begin content from: aclnn_foreach_lerp_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LERP_SCALAR_H_
#define ACLNN_FOREACH_LERP_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLerpScalarGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * weight : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLerpScalarGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclScalar *weight,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLerpScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLerpScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_lerp_scalar.h

// Begin content from: aclnn_amax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AMAX_H_
#define OP_API_INC_AMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAmax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回张量在指定维度上每个切片的最大值。
 *
 *
 * @param [in] self:
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、BOOL。数据格式支持ND。
 * 支持非连续的Tensor。
 * @param [in] dim: host侧aclIntArray，指定了要进行最大值计算的维度， 数据类型支持INT32和INT64。
 * @param [in] keepDim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] out: device侧的aclTensor，数据类型需要与self相同。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAmaxGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAmax的第二段接口，用于执行计算。
 *
 * 算子功能：返回张量在指定维度上的每个切片的最大值。
 *
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAmaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAmax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AMAX_H_// End content from: aclnn_amax.h

// Begin content from: aclnn_im2col_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_IM2COL_BACKWARD_H_
#define OP_API_INC_IM2COL_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIm2colBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnIm2colBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclIntArray* inputSize,
                                                          const aclIntArray* kernelSize, const aclIntArray* dilation,
                                                          const aclIntArray* padding, const aclIntArray* stride,
                                                          aclTensor* out, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnIm2colBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnIm2colBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_im2col_backward.h

// Begin content from: aclnn_kthvalue.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_KTHVALUE_H_
#define OP_API_INC_KTHVALUE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnKthvalue的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成计算输入的第k个最小值及下标。
 *
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持INT32、FLOAT16、FLOAT32。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] k:
 * int64_t类型整数。表示计算维度上输出的第几个最小值。
 * @param [in] dim:
 * int64_t类型整数。表示计算维度。
 * @param [in] keepdim:
 * bool类型数据。True表示输出张量的大小与self相同，
 * False表示dim将被压缩，得到的张量维数比input少1。
 * @param [in] valuesOut:
 * dnpu device侧的aclTensor，数据类型支持INT32、FLOAT16、FLOAT32，且数据类型与self保持一致。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] indicesOut:
 * npu device侧的aclTensor，数据类型支持INT64。支持连续和非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnKthvalueGetWorkspaceSize(const aclTensor* self, int64_t k, int64_t dim, bool keepdim,
                                                    aclTensor* valuesOut, aclTensor* indicesOut,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnKthvalue的第二段接口，用于执行计算。
 *
 * 算子功能：完成计算输入的第k个最小值及下标。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnKthvalueGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnKthvalue(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_KTHVALUE_H_
// End content from: aclnn_kthvalue.h

// Begin content from: aclnn_avgpool2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AVGPOOL2D_BACKWARD_H_
#define OP_API_INC_AVGPOOL2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAvgPool2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnAvgPool2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                             const aclIntArray* kernelSize, const aclIntArray* stride,
                                                             const aclIntArray* padding, bool ceilMode,
                                                             bool countIncludePad, int64_t divisorOverride,
                                                             int8_t cubeMathType, aclTensor* gradInput,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAvgPool2dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAvgPool2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AVGPOOL2D_BACKWARD_H_// End content from: aclnn_avgpool2d_backward.h

// Begin content from: aclnn_ffn.h
/**
 * Copyright (c) 2023-2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file aclnn_ffn.h
 * \brief
 */

#ifndef OP_API_INC_FFN_H
#define OP_API_INC_FFN_H
// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFFN的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：该FFN算子提供MoeFFN和FFN的计算功能
 * 计算公式：y=activation(xW1+b1)W2+b2
 * @domain aclnn_ops_infer
 * @param [in]
 * x：必选参数，Device侧的aclTensor，公式中的输入x，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，支持输入的维度最少是2维[M,
 * K1]，最多是8维。
 * @param [in]
 * weight1：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W1，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K1, N1]/[K1, N1]。
 * @param [in]
 * weight2：必选参数，Device侧的aclTensor，专家的权重数据，公式中的W2，数据类型支持FLOAT16、BFLOAT16、INT8、INT4，数据格式支持ND，输入在有/无专家时分别为[E,
 * K2, N2]/[K2, N2]。
 * @param [in]
 * expertTokens：可选参数，Host侧的aclIntArray类型，代表各专家的token数，数据类型支持INT64，数据格式支持ND，若不为空时可支持的最大长度为256个。
 * @param [in]
 * bias1：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b1，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * bias2：可选参数，Device侧的aclTensor，权重数据修正值，公式中的b2，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * scale：可选参数，Device侧的aclTensor，量化参数，量化缩放系数，数据类型支持FLOAT32，数据格式支持ND，per-tensor下输入在有/无专家时均为一维向量，输入元素个数在有/无专家时分别为[E]/[1]；per-channel下输入在有/无专家时为二维向量/一维向量，输入元素个数在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * offset：可选参数，Device侧的aclTensor，量化参数，量化偏移量，数据类型支持FLOAT32，数据格式支持ND，一维向量，输入元素个数在有/无专家时分别为[E]/[1]。
 * @param [in]
 * deqScale1：可选参数，Device侧的aclTensor，量化参数，第一个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N1]/[N1]。
 * @param [in]
 * deqScale2：可选参数，Device侧的aclTensor，量化参数，第二个matmul的反量化缩放系数，数据类型支持UINT64、INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E,
 * N2]/[N2]。
 * @param [in]
 * antiquantScale1：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantScale2：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * antiquantOffset1：可选参数，Device侧的aclTensor，伪量化参数，第一个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N1]/[N1]，per-in-group下输入在有/无专家时分别为[E, G, N1]/[G, N1]。
 * @param [in]
 * antiquantOffset2：可选参数，Device侧的aclTensor，伪量化参数，第二个matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E,
 * N2]/[N2]，per-in-group下输入在有/无专家时分别为[E, G, N2]/[G, N2]。
 * @param [in]
 * activation：必选参数，Host侧的属性值，代表使用的激活函数，公式中的activation，当前支持fastgelu/gelu/relu/silu以及geglu/swiglu/reglu。
 * @param [in]
 * innerPrecise：可选参数，Host侧的int，表示高精度或者高性能选择。数据类型支持：INT64。该参数仅对FLOAT16生效，BFLOAT16和INT8不区分高精度和高性能。
 * @param [out] y：输出Tensor，公式中的输出y，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，输出维度与x一致。
 * @param [out] workspaceSize：返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor：返回op执行器，包含了算子计算流程。
 * @return      aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus
aclnnFFNGetWorkspaceSize(const aclTensor *x, const aclTensor *weight1, const aclTensor *weight2,
                         const aclIntArray *expertTokens, const aclTensor *bias1, const aclTensor *bias2,
                         const aclTensor *scale, const aclTensor *offset, const aclTensor *deqScale1,
                         const aclTensor *deqScale2, const aclTensor *antiquantScale1, const aclTensor *antiquantScale2,
                         const aclTensor *antiquantOffset1, const aclTensor *antiquantOffset2, const char *activation,
                         int64_t innerPrecise, const aclTensor *y, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFFN的第二段接口，用于执行计算。
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小，由第一段接口aclnnFFNGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL stream流。
 * @return     aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFFN(void *workspace, uint64_t workspaceSize,
                                                            aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_FFN_H// End content from: aclnn_ffn.h

// Begin content from: aclnn_grid_sampler2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GRID_SAMPLER2D_H_
#define OP_API_INC_GRID_SAMPLER2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGridSampler2D的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：提供一个输入tensor以及一个对应的flow-field网格，然后根据grid中每个位置提供的坐标信息，
 * 将input中对应位置的像素值填充到网格指定的位置，得到最终的输出。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(input)] --> B([l0op::Contiguous]) --> C([l0op::GridSampler2D])
 *     D[(grid)] --> E([l0op::Contiguous]) --> C
 *     F((interpolationMode)) --> C
 *     G((paddingMode)) --> C
 *     H((alignCorners)) --> C
 *     C --> I([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] input: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] grid: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] interpolationMode：host侧的int64_t， 表示插值模式，0：bilinear（双线性插值），1：nearest（最邻近插值），
 * 2（不支持）：bicubic（双三次插值）。
 * @param [in] paddingMode：host侧的int64_t，表示填充模式，即当（x,y）取值超过输入特征图采样范围时，返回一个特定值，
 * 有0：zeros、1：border、2：reflection三种模式。
 * @param [in] alignCorners：host侧的bool，表示设定特征图坐标与特征值的对应方式，设定为true时，特征值位于像素中心。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGridSampler2DGetWorkspaceSize(const aclTensor* input, const aclTensor* grid,
                                                         int64_t interpolationMode, int64_t paddingMode,
                                                         bool alignCorners, aclTensor* out, uint64_t* workspaceSize,
                                                         aclOpExecutor** executor);

/**
 * @brief aclnnGridSampler2D的第二段接口，用于执行计算。
 *
 * 算子功能：提供一个输入tensor以及一个对应的flow-field网格，然后根据grid中每个位置提供的坐标信息，
 * 将input中对应位置的像素值填充到网格指定的位置，得到最终的输出。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(input)] --> B([l0op::Contiguous]) --> C([l0op::GridSampler2D])
 *     D[(grid)] --> E([l0op::Contiguous]) --> C
 *     F((interpolationMode)) --> C
 *     G((paddingMode)) --> C
 *     H((alignCorners)) --> C
 *     C --> I([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGridSampler2DGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGridSampler2D(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GRID_SAMPLER2D_H_// End content from: aclnn_grid_sampler2d.h

// Begin content from: aclnn_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bfloat16。
 * @param [in] x2: matmul右矩阵，数据类型支持：float16, bfloat16。
 * @param [in] bias: 偏置，数据类型支持：float16, bfloat16。
 * @param [in] residual: 残差，数据类型支持：float16, bfloat16。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] y: MatmulAllReduce+Add(residual)的结果，数据类型：同输入。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* residual, const aclTensor* gamma,
    double epsilon, const char* group, const char* reduceOp, int64_t commTurn, int64_t streamMode, const aclTensor* y,
    const aclTensor* normOut, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                     const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MATMUL_ALL_REDUCE_ADD_EMS_NORM_// End content from: aclnn_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_foreach_pow_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_POW_LIST_H_
#define ACLNN_FOREACH_POW_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachPowListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachPowList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_pow_list.h

// Begin content from: aclnn_maximum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAXIMUM_H_
#define OP_API_INC_MAXIMUM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnmaximum的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算两个张量中每个元素的最大值，并返回一个新的张量。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaximumGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMaximum的第二段接口，用于执行计算。
 *
 * 算子功能：计算两个张量中每个元素的最大值，并返回一个新的张量。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMaximumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaximum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAXIMUM_H_
// End content from: aclnn_maximum.h

// Begin content from: aclnn_group_norm_swish.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_GROUP_NORM_SWISH_H_
#define ACLNN_GROUP_NORM_SWISH_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnGroupNormSwishGetWorkspaceSize
 * parameters :
 * x : required
 * gamma : required
 * beta : required
 * numGroups : required
 * dataFormatOptional : optional
 * eps : optional
 * activateSwish : optional
 * swishScale : optional
 * yOut : required
 * meanOut : required
 * rstdOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnGroupNormSwishGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *gamma,
    const aclTensor *beta,
    int64_t numGroups,
    char *dataFormatOptional,
    double eps,
    bool activateSwish,
    double swishScale,
    const aclTensor *yOut,
    const aclTensor *meanOut,
    const aclTensor *rstdOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnGroupNormSwish
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnGroupNormSwish(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_group_norm_swish.h

// Begin content from: aclnn_foreach_cosh.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_COSH_H_
#define ACLNN_FOREACH_COSH_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachCoshGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCoshGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachCosh
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachCosh(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_cosh.h

// Begin content from: aclnn_log1p.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG1P_H_
#define OP_API_INC_LOG1P_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLog1p的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] self: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、FLOAT16、
 *  DOUBLE、BFLOAT16，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、BFLOAT16、FLOAT16、DOUBLE, shape与self
 *  保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLog1pGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnLog1p的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLog1pGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog1p(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceLog1p的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] selfRef: npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE，shape为非空，支持非连续的Tensor， 数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceLog1pGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceLog1p的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLog1pGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog1p(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG1P_H_// End content from: aclnn_log1p.h

// Begin content from: aclnn_triu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_TRIU_H_
#define OP_API_INC_LEVEL2_ACLNN_TRIU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTriu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：将输入的self张量的最后二维（按shape从左向右数）沿主对角线的左下部分置零。参数diagonal可正可负，
 * 默认为零，正数表示主对角线向右上方向移，负数表示主对角线向左下方向移动。
 * 计算公式：下面用i表示遍历倒数第二维元素的序号（i是行索引），用j表示遍历最后一维元素的序号（j是列索引），
 * 用d表示diagonal，在(i, j)对应的二维坐标图中，i+d==j表示在对角线上，
 * $$
 * 对角线及其右上方，即i+d<=j,保留原值： out_{i,j} = self_{i,j}\\
 * 而位于对角线左下方的情况，即i+d>j,置零：out_{i,j} = 0
 * $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0::Contiguous]) --> C([l0op::Triu])
 *   C --> D([l0op::ViewCopy]) --> E[(out)]
 * ```
 *
 * @param [in] self: 待进行triu计算的入参。npu device侧的aclTensor，
 * 数据类型支持DOUBLE,FLOAT,FLOAT16,BFLOAT16(仅910B),INT16,INT32,INT64,INT8,UINT16,UINT32,UINT64,UINT8,BOOL。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] diagonal: 主对角线的偏移量，数据类型支持INT。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTriuGetWorkspaceSize(const aclTensor* self, int64_t diagonal, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnTriu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAllGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTriu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceTriu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持DOUBLE,FLOAT,FLOAT16,BFLOAT16(仅910B),INT16,INT32,INT64,INT8,UINT16,UINT32,UINT64,UINT8,BOOL。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] diagonal: 主对角线的偏移量，数据类型支持INT。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTriuGetWorkspaceSize(aclTensor* selfRef, int64_t diagonal, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceTriu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceTriuGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceTriu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_TRIU_H_
// End content from: aclnn_triu.h

// Begin content from: aclnn_fake_quant_per_tensor_affine_cachemask.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_fake_quant_per_tensor_affine_cachemask.h
 * \brief
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_TENSOR_AFFINE_CACHEMASK_TENSOR_QPARAMS_H_
#define OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_TENSOR_AFFINE_CACHEMASK_TENSOR_QPARAMS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFakeQuantPerTensorAffineCachemask的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：
 *   1）fake_quant_enabled >= 1: 对于输入数据self，使用scale和zero_point对输入self进行伪量化处理
 * 并根据quant_min和quant_max对伪量化输出进行值域更新，最终返回结果out及对应位置掩码mask。
 *   2) fake_quant_enabled < 1: 返回结果out为self.clone(）对象，掩码mask为全True。
 *
 * 计算图：如下
 * 场景一：当fake_quant_enabled小于1时，使用ViewCopy计算out，使用Fill计算mask。
 * ```mermaid
 * graph LR
 *   A1[(self)] -->B1(l0op::Contiguous)-->C1(l0op::ViewCopy)-->D1[(out)]
 *   A1[(self)] -->B1(l0op::Contiguous)-->D(l0op::Fill)-->C2(l0op::ViewCopy)-->D2[(mask)]
 * ```
 *
 * 场景二：当fake_quant_enabled大于等于1时，因为scale和zero_point的size大小都为1，故先broadcast后，再使用FakeQuantPerChannelAffineCachemask算子完成计算。
 * ```mermaid
 * graph LR
 *   A1[(self)] -->B1(l0op::Contiguous)-->C(l0op::FakeQuantPerChannelAffineCachemask)
 *   A2[(scale)] -->B2(l0op::Contiguous)-->F1(l0op::BroadcastTo)-->C(l0op::FakeQuantPerChannelAffineCachemask)
 *   A3[(zeroPoint)]-->B3(l0op::Contiguous)-->F2(l0op::BroadcastTo)-->C(l0op::FakeQuantPerChannelAffineCachemask)
 *   A4((quantMin)) --> C(l0op::FakeQuantPerChannelAffineCachemask)
 *   A5((quantMax)) --> C(l0op::FakeQuantPerChannelAffineCachemask)
 *   C(l0op::FakeQuantPerChannelAffineCachemask)-->D1(l0op::ViewCopy)-->E1[(out)]
 *   C(l0op::FakeQuantPerChannelAffineCachemask)-->D2(l0op::ViewCopy)-->E2[(mask)]
 * ```
 *
 * @param [in] self:
 * Device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。支持非连续的Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [in] scale: Device侧的aclTensor，表示输入伪量化的缩放系数。数据类型支持FLOAT16、FLOAT32，size大小为1。
 * @param [in] zeroPoint: Device侧的aclTensor，表示输入伪量化的零基准参数。数据类型支持INT32，size大小为1。
 * @param [in] fake_quant_enabled: Host侧的浮点型，表示是否进行伪量化计算，数据类型支持FLOAT。
 * @param [in] quantMin: Host侧的整型，表示输入数据伪量化后的最小值，数据类型支持INT。
 * @param [in] quantMax: Host侧的整型，表示输入数据伪量化后的最大值，数据类型支持INT。
 * @param [out] out:
 * Device侧的aclTensor，数据类型支持LOAT16、FLOAT32，支持非连续Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [out] mask: Device侧的aclTensor，数据类型支持BOOL，支持非连续Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFakeQuantPerTensorAffineCachemaskGetWorkspaceSize(
    const aclTensor* self, const aclTensor* scale, const aclTensor* zeroPoint, float fakeQuantEnbled, int64_t quantMin,
    int64_t quantMax, aclTensor* out, aclTensor* mask, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFakeQuantPerTensorAffineCachemask的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnFakeQuantPerTensorAffineCachemask获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFakeQuantPerTensorAffineCachemask(void* workspace, uint64_t workspaceSize,
                                                             aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_TENSOR_AFFINE_CACHEMASK_TENSOR_QPARAMS_H_
// End content from: aclnn_fake_quant_per_tensor_affine_cachemask.h

// Begin content from: aclnn_foreach_tanh.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_TANH_H_
#define ACLNN_FOREACH_TANH_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachTanhGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachTanhGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachTanh
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachTanh(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_tanh.h

// Begin content from: aclnn_ne_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_NE_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_NE_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNeScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other:
 * host侧的aclScalar，数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16,
 * 数据类型需要能转换成self的数据类型
 * @param [in] out: npu device侧的aclTensor，数据类型为bool，shape与self的shape一致，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNeScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNeScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNeScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

/**
 * @brief aclnnInplaceNeScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。selfRef也是输出结果。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,UINT64,INT32,INT8,UINT8,BOOL,UINT32,BFLOAT16,INT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNeScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceNeScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceNeScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNeScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_NE_SCALAR_H_// End content from: aclnn_ne_scalar.h

// Begin content from: aclnn_gather_nd.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GATHER_ND_H_
#define OP_API_INC_LEVEL2_ACLNN_GATHER_ND_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：对于维度为**r≥1**的输入张量`self`，和维度**q≥1**的输入张量`indices`，将数据切片收集到维度为**(q-1) + (r -
 * indices_shape[-1])**
 * 的输出张量out中。indices是一个**q**维的整型张量，可视作一个**q-1**维的由**索引对**构成的特殊张量(每个**索引对**是一个长度为**indices_shape[-1]**
 * 的一维张量，每个**索引对**指向**self**中一个切片)，具体计算逻辑为：
 *  1) 如果**indices_shape[-1]** > **r**，不合法场景。
 *  2) 如果**indices_shape[-1]** = **r**，则输出张量out的维度为**q-1**，即out的shape为 **[indices_shape[0:q-2]]**，
 *     out中元素为self的**索引对**位置的元素。（见例1）
 *  3) 如果**indices_shape[-1]** < **r**，则输出张量out的维度为 **(q-1) + (r -
 * indices_shape[-1])**，设**c**=**r**-**indices_shape[-1]**， 即out的shape为
 * **[indices_shape[0:q-2],self_shape[r-c:r-1]]**，`out`由`self`的**索引对**位置的切片组成。（见例2,3,4）
 * 关于**r**、**q**、**indices_shape[-1]** 的一些限制条件如下：
 *  1) 必须满足**r**≥1，**q**≥1。
 *  2) **indices_shape[-1]**的值必须满足在1(包含)和**r**(包含)之间。
 *  3) `indices`的每个元素，必须在[-**s**, **s-1**]范围内(**s**为**self_shape**各个轴上的值)，
 *     即-**self_shape[i]**≤indices[...,i]≤**self_shape[i]**-1。
 * 示例
 *  例1：
 *   self: [[0, 1],[2, 3]]       # self_shape=[2, 2], r=2
 *   indices: [[0, 0], [1, 1]]   # indices_shape=[2, 2], q=2, indices_shape[-1]=2
 *   out: [0, 3]                 # out_shape=[2]
 *  例2：
 *   self: [[0, 1],[2, 3]]       # self_shape=[2, 2], r=2
 *   indices: [[1], [0]]         # indices_shape=[2, 1], q=2, indices_shape[-1]=1
 *   out: [[2, 3], [0, 1]]       # out_shape=[2, 2]
 *  例3：
 *   self: [[[0, 1],[2, 3]], [[4, 5],[6, 7]]]   # self_shape=[2, 2, 2], r=3
 *   indices: [[0, 1], [1, 0]]                  # indices_shape=[2, 2], q=2, indices_shape[-1]=2
 *   out: [[2, 3], [4, 5]]                      # out_shape=[2, 2]
 *  例4：
 *   self: [[[0, 1],[2, 3]], [[4, 5],[6, 7]]]   # self_shape=[2, 2, 2], r=3
 *   indices: [[[0, 1]], [[1, 0]]]              # indices_shape=[2, 1, 2], q=3, indices_shape[-1]=2
 *   out: [[[2, 3]], [[4, 5]]]                  # out_shape=[2, 1, 2]
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   C[(indices)] --> D([l0op::Contiguous])
 *   B --> E([l0op::GatherNd])
 *   D --> E
 *   E --> F([l0op::ViewCopy])
 *   F --> G[(out)]
 * ```
 */

/**
 * @brief aclnnGatherNd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持INT64、INT32、INT8、UINT8、BOOL、FLOAT、FLOAT16、BFLOAT16，支
 * 持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] index: npu
 * device侧的aclTensor，数据类型支持INT64、INT32，支持非连续的Tensor，数据格式支持ND，数据维度不支持 8维以上。
 * @param [in] negativeIndexSupport: 属性，表示是否支持负数场景，数据类型支持bool。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT64、INT32、INT8、UINT8、BOOL、FLOAT、FLOAT16、BFLOAT16，数据
 * 类型需要与self一致，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGatherNdGetWorkspaceSize(const aclTensor* self, const aclTensor* indices,
                                                    bool negativeIndexSupport, aclTensor* out, uint64_t* workspaceSize,
                                                    aclOpExecutor** executor);

/**
 * @brief aclnnGatherNd的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGatherNdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGatherNd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GATHER_ND_H_
// End content from: aclnn_gather_nd.h

// Begin content from: aclnn_incre_flash_attention.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_INCRE_FLASH_ATTENTION_H_
#define ACLNN_INCRE_FLASH_ATTENTION_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIncreFlashAttention的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * funtion: aclnnIncreFlashAttentionGetWorkspaceSize
 * param [in] query : required
 * param [in] key : dynamic
 * param [in] value : dynamic
 * param [in] pseShift : optional
 * param [in] attenMask : optional
 * param [in] actualSeqLengths : optional
 * param [in] numHeads : required
 * param [in] scaleValue : optional
 * param [in] inputLayout : optional
 * param [in] numKeyValueHeads : optional
 * @param [out] attentionOut : required
 * @param [out] workspaceSize : size of workspace(output).
 * @param [out] executor : executor context(output).
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionGetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
    const aclTensor *attenMask, const aclIntArray *actualSeqLengths, int64_t numHeads, double scaleValue,
    char *inputLayout, int64_t numKeyValueHeads, const aclTensor *attentionOut, uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * funtion: aclnnIncreFlashAttention
 * param [in] workspace : workspace memory addr(input).
 * param [in] workspaceSize : size of workspace(input).
 * param [in] executor : executor context(input).
 * param [in] stream : acl stream.
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttention(void *workspace, uint64_t workspaceSize,
                                                                            aclOpExecutor *executor,
                                                                            const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_incre_flash_attention.h

// Begin content from: aclnn_dot.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_DOT_H_
#define OP_API_INC_LEVEL2_ACLNN_DOT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：计算两个一维张量的点积结果
 * 计算公式：如下
 * $$
 * self = [x_{1}, x_{2}, ..., x_{n}]
 * $$
 *
 * $$
 * tensor = [y_{1}, y_{2}, ..., y_{n}]
 * $$
 *
 * $$
 * out = x_{1}*y_{1} + x_{2}*y_{2} + ... + x_{n}*y_{n}
 * $$
 *
 * 实现说明：
 * api计算的基本路径一（self和tensor均不是空Tensor场景）：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous])
 *     B --> C([l0op::Dot])
 *     D[(tensor)] --> E([l0op::Contiguous])
 *     E --> C
 *     C --> F([l0op::ViewCopy])
 *     F --> G[(out)]
 * ```
 *
 * api计算的基本路径二（self和tensor均为空Tensor场景）：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Fill])
 *     C[(tensor)] --> B
 *     B --> D([l0op::ViewCopy])
 *     D --> E[(out)]
 * ```
 */

/**
 * @brief aclnnDot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，且数据类型需要与tensor一致，
 * 支持非连续的Tensor，数据格式支持ND，维度是1维，且shape需要与tensor一致。
 * @param [in] tensor: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，且数据类型需要与self一致，
 * 支持非连续的Tensor，数据格式支持ND，维度是1维，且shape需要与self一致。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，且数据类型需要与self、tensor
 * 一致，数据格式支持ND，维度是0维。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDotGetWorkspaceSize(const aclTensor* self, const aclTensor* tensor, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDot的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnDotGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDot(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_DOT_H_
// End content from: aclnn_dot.h

// Begin content from: aclnn_foreach_div_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_DIV_LIST_H_
#define ACLNN_FOREACH_DIV_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachDivListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachDivList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_div_list.h

// Begin content from: aclnn_max.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_H_
#define OP_API_INC_MAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，数据类型是self可转化的数据类型。数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaxGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnMax的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMaxGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_H_// End content from: aclnn_max.h

// Begin content from: aclnn_upsample_bilinear2d_aa_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef ACLNN_UPSAMPLE_BILINEAR2D_AABACKWARD_H_
#define ACLNN_UPSAMPLE_BILINEAR2D_AABACKWARD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBilinear2dAABackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：aclnnUpsampleBilinear2dAA的反向传播。
 *
 * @param [in] gradOutput: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与出参out的数据类型一致。
 * @param [in] outputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为2。表示输入gradOutput在H和W维度上的空间大小。
 * @param [in] inputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为4。表示输出out分别在N、C、H和W维度上的空间大小。
 * @param [in] scalesH: Host侧的浮点型，表示输出out的height维度乘数。
 * @param [in] scalesW: Host侧的浮点型，表示输出out的width维度乘数。
 * @param [out] out: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与入参gradOutput的数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleBilinear2dAABackwardGetWorkspaceSize(
    const aclTensor *gradOutput, const aclIntArray *outputSize, const aclIntArray *inputSize, bool alignCorners, 
    double scalesH, double scalesW, aclTensor *out, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleBilinear2dAABackward的第二段接口，用于执行计算。
 * 
 * 算子功能：aclnnUpsampleBilinear2dAA的反向传播。
 *
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小。
 * 由第一段接口aclnnUpsampleBilinear2dAABackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL Stream流。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleBilinear2dAABackward(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_upsample_bilinear2d_aa_backward.h

// Begin content from: aclnn_upsample_nearest_exact1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_
#define OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearestExact1d的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对由三个输入通道组成的输入信号应用最近邻精确插值算法进行上采样插值
 * 计算公式：
 * out(N, C, l) = self(N, C, min(floor((l + 0.5) * scales),  L- 1))
 * @domain aclnn_ops_train
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND、NCL。
 * @param [in]   outputSize
 * 输出的size大小，数据类型支持INT32、INT64。
 * @param [in]   scales
 * 输出的缩放系数，数据类型支持DOUBLE。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND、NCL。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnUpsampleNearestExact1dGetWorkspaceSize(const aclTensor *self, const aclIntArray *outputSize,
                                                        double scales, aclTensor *out, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleNearestExact1d的第二段接口，用于执行计算。
 * 
 * 功能描述：对由三个输入通道组成的输入信号应用最近邻精确插值算法进行上采样插值。
 * 
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnUpsampleNearestExact1dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。 
*/
ACLNN_API aclnnStatus aclnnUpsampleNearestExact1d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_EXACT1D_H_
// End content from: aclnn_upsample_nearest_exact1d.h

// Begin content from: aclnn_matmul.h

/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MATMUL_H_
#define OP_API_INC_MATMUL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMatmul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnMatmulGetWorkspaceSize(const aclTensor* self, const aclTensor* mat2, aclTensor* out,
                                                  int8_t cubeMathType, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnMatmul的第二段接口，用于执行计算。
 */
// for any shape mat multiply
ACLNN_API aclnnStatus aclnnMatmul(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MATMUL_H_
// End content from: aclnn_matmul.h

// Begin content from: aclnn_repeat.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_REPEAT_H_
#define OP_API_INC_LEVEL2_ACLNN_REPEAT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRepeat的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnRepeatGetWorkspaceSize(const aclTensor* self, const aclIntArray* repeats, aclTensor* out,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnRepeat的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnRepeat(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_repeat.h

// Begin content from: aclnn_unique_consecutive.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNIQUE_CONSECUTIVE_H_
#define OP_API_INC_UNIQUE_CONSECUTIVE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUniqueConsecutive的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：从每个连续的相同元素组中消除除第一个元素之外的所有元素。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *      A[(self)] ---> B([l0op::Contiguous])
 *      B ---> F([l0op::UniqueConsecutive])
 *      C((returnInverse)) --->F
 *      D((returnCounts)) --->F
 *      E((dim)) --->F
 *      F --> G[(valueOut)]
 *      F --> H[(inverseOut)]
 *      F --> I[(countsOut)]
 * ```
 * @param [in] self：数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、INT32、INT64、UINT8、UINT16、UINT32、
 *                   UINT64、COMPLEX64、COMPLEX128、BOOL，数据格式支持ND。
 * @param [in] returnInverse: 表示是否返回self中各元素在valueOut中对应元素的位置下标，True时返回，False时不返回。
 * @param [in] returnCounts: 表示是否返回valueOut中各元素在self中连续重复出现的次数，True时返回，False时不返回。
 * @param [in] dim: 表示进行去重的维度。
 * @param [in] valueOut: 第一个输出张量，返回消除连续重复元素后的结果，数据类型支持FLOAT、FLOAT16、DOUBLE、INT8、INT16、
 *                       INT32、INT64、UINT8、UINT16、UINT32、UINT64、COMPLEX64、COMPLEX128、BOOL，数据格式支持ND。
 * @param [in] inverseOut:
 * 第二个输出张量，当returnInverse为True时有意义，返回self中各元素在valueOut中对应元素的位置下标，
 *                         数据类型支持INT64，数据格式支持ND。
 * @param [in] countsOut: 第三个输出张量，当returnCounts为True时有意义，返回valueOut中各元素在self中连续重复出现的次数，
 *                        数据类型支持INT64，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUniqueConsecutiveGetWorkspaceSize(const aclTensor* self, bool returnInverse,
                                                             bool returnCounts, int64_t dim, aclTensor* valueOut,
                                                             aclTensor* inverseOut, aclTensor* countsOut,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUniqueConsecutive的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnUniqueConsecutiveGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUniqueConsecutive(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNIQUE_CONSECUTIVE_H_// End content from: aclnn_unique_consecutive.h

// Begin content from: aclnn_diag_flat.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_DIAG_FLAT_H_
#define OP_API_INC_DIAG_FLAT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnDiagFlat的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持所有数据类型，支持非连续的Tensor，数据格式支持ND，且数据格式需要与out一致。
 * @param [in] diagonal: host侧的基本数据类型,数据类型支持INT64，表示矩阵的偏移量。
 * @param [out] out: npu device侧的aclTensor, 数据类型支持所有数据类型，支持非连续的Tensor, 数据类型支持ND,
 * 表示输出张量。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDiagFlatGetWorkspaceSize(const aclTensor* self, int64_t diagonal, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDiagFlat的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnDiagFlatGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDiagFlat(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DIAG_FLAT_H_
// End content from: aclnn_diag_flat.h

// Begin content from: aclnn_circular_pad3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CIRCULAR_PAD_H_
#define OP_API_INC_CIRCULAR_PAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCircularPad3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @domain aclnn_ops_train
 *
 * 算子功能：使用输入边界的循环填充输入tensor。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持BFLOAT16,FLOAT16, FLOAT32,INT8, INT32,数据格式支持ND，维度支持四维或五维。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为6，数值依次代表左右上下前后需要填充的值。
 * 前两个数值需小于self最后一维度的数值，中间两个数值需小于self倒数第二维度的数值，后两个数值需小于self倒数第三维度的数值。
 * @param [in] out: npu device侧的aclTensor,
 * 数据类型、数据格式、维度与self一致，倒数第三维度的数值等于self倒数第三维度的
 * 数值加padding后两个值，倒数第二维度的数值等于self倒数第二维度的
 * 数值加padding中间两个值，最后一维度的数值等于self最后一维度的数值加padding前两个值。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnCircularPad3d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界的循环填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCircularPad3dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CIRCULAR_PAD_H_// End content from: aclnn_circular_pad3d.h

// Begin content from: aclnn_equal.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TENSOREQUAL_H_
#define OP_API_INC_TENSOREQUAL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEqual的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 计算两个Tensor是否有相同的大小和元素，返回一个Bool类型：
 *
 * $$ out = (self == other)  ?  True : False $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([l0op::Contiguous])
 *     B -->D([l0op::TensorEqual])
 *     E[(other)] -->F([l0op::Contiguous])
 *     F -->D --> F1([l0op::ViewCopy])
 *     F1 --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT32,INT8,UINT8,BOOL,DOUBLE,INT64,INT16,UINT16,UINT32,UINT64数据类型，
 * self与other数据类型一致，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT32,INT8,UINT8,BOOL,DOUBLE,INT64,INT16,UINT16,UINT32,UINT64数据类型，
 * self与other数据类型一致，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: 输出一个数据类型为BOOL类型的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqualGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnEqual的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEqualGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqual(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TENSOREQUAL_H_
// End content from: aclnn_equal.h

// Begin content from: aclnn_reflection_pad3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REFLECTION_PAD3D_BACKWARD_H_
#define OP_API_INC_REFLECTION_PAD3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：反射填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, DOUBLE, COMPLEX64,
 * COMPLEX128，数据格式支持ND，
 * 维度支持四维或五维且与self和gradInput一致，shape需要与reflection_pad3d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持四维或五维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为6，数值依次代表左右上下前后需要填充的值。
 * 前两个数值需小于self最后一维度的数值，中间两个数值需小于self倒数第二维度的数值,
 * 最后两个数值需小于self倒数第三维度的数值,。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持四维或五维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad3dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                   const aclIntArray* padding, aclTensor* gradInput,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad3dBackward的第二段接口，用于执行计算
 *
 * 算子功能：反射填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnReflectionPad3dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REFLECTION_PAD3D_BACKWARD_H_// End content from: aclnn_reflection_pad3d_backward.h

// Begin content from: aclnn_scatter_add.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SCATTER_OUT_H_
#define OP_API_INC_SCATTER_OUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScatterAdd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 将源tensor中的值按指定的轴方向和index tensor中的位置关系逐个填入输出tensor中，
 * 若有多于一个src值被填入到self的同一位置，那么这些值将会在这一位置上进行累加
 * @param [in] self: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8,
 * 支持非连续的Tensor，数据格式支持ND,
 * @param [in] dim: host侧的num, 数据类型支持INT64。
 * @param [in] index: npu device侧的aclTensor，数据类型支持INT32, int64类型，dim反向的维度数量需要与src相同。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] src: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT32, INT32, INT8,
 * UINT8类型，dim反向的维度数量需要与src相同。 支持非连续的Tensor，数据格式支持ND，且数据类型与self保持一致。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, INT32, INT8, UINT8,
 * 数据类型,数据格式,tensor shape需要与self保持一致
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnScatterAddGetWorkspaceSize(const aclTensor* self, int64_t dim, const aclTensor* index,
                                                      const aclTensor* src, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief: aclnnScatterAdd的第二段接口，用于执行计算
 *
 * 算子功能: 将源tensor中的值按指定的轴方向和index tensor中的位置关系逐个填入输出tensor中，
 * 若有多于一个src值被填入到self的同一位置，那么这些值将会在这一位置上进行累加
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnScatterAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScatterAdd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SCATTER_OUT_H_// End content from: aclnn_scatter_add.h

// Begin content from: acl_rfft1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACL_RFFT1D_H_
#define OP_API_INC_LEVEL2_ACL_RFFT1D_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclRfft1D First segment interface. Calculate the workspace size based on the specific calculation process.
 * Function description: Calculates the RFFT of the input tensor and outputs the result.
 * Calculation formula:
 * $$ out = Rfft(input) $$
 * Calculation chart:
 ```mermaid
 * graph LR
 * A[(self)]--->B([l0op::Rfft1D])
 * B--->C([l0op::ViewCopy])
 * C--->D[(out)]
 * ` ` `
 * @domain aclnn_ops_infer
 * Parameter description:
 * @param [in] Input
 * Input tensor. The type is FLOAT. The data format supports ND, but does not support discontinuous tensors.
 * @param [in] out
 * Output tensor. The type is FLOAT. The data format supports ND, but does not support discontinuous tensors.
 * @param [out] workspace_size: Returns the workspace size that a user needs to apply for on the NPU device.
 * @param [out] executor: Return the op executor, including the operator calculation process.
 * @return aclnnStatus: Return the status code.
 */
 
aclnnStatus aclRfft1DGetWorkspaceSize(const aclTensor* self, int64_t n, int64_t dim, int64_t norm, aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief A second interface of aclRfft1D, used to perform calculation.
 * @param [in] workspace: start address of the workspace memory allocated on the NPU device.
 * @param [in] workspace_size: size of the workspace applied on the NPU device, which is obtained by calling the first segment interface aclRfft1DGetWorkspaceSize.
 * @param [in] exector: op executor, including the operator calculation process.
 * @param [in] stream: acl stream.
 * @return aclnnStatus: returned status code
 */
                                    
aclnnStatus aclRfft1D(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACL_RFFT1D_H_
// End content from: acl_rfft1d.h

// Begin content from: aclnn_flash_attention_score_grad.h
/**
 * Copyright (c) 2023-2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef OP_API_INC_FLASH_ATTENTION_SCORE_GRAD_H_
#define OP_API_INC_FLASH_ATTENTION_SCORE_GRAD_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFlashAttentionScoreGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
aclnnStatus aclnnFlashAttentionScoreGradGetWorkspaceSize(
    const aclTensor *query, const aclTensor *keyIn, const aclTensor *value, const aclTensor *dy,
    const aclTensor *pseShiftOptional, const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional, const aclTensor *softmaxMaxOptional, const aclTensor *softmaxSumOptional,
    const aclTensor *softmaxInOptional, const aclTensor *attentionInOptional, const aclIntArray *prefixOptional,
    double scaleValue, double keepProb, int64_t preTokens, int64_t nextTokens,
    int64_t headNum, char *inputLayout, int64_t innerPrecise, int64_t sparseMode,
    const aclTensor *dqOut, const aclTensor *dkOut, const aclTensor *dvOut, const aclTensor *dpseOut,
    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionScoreGrad的第二段接口，用于执行计算。
 */
aclnnStatus aclnnFlashAttentionScoreGrad(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                         const aclrtStream stream);

/**
 * @brief aclnnFlashAttentionUnpaddingScoreGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
aclnnStatus aclnnFlashAttentionUnpaddingScoreGradGetWorkspaceSize(
    const aclTensor *query, const aclTensor *keyIn, const aclTensor *value, const aclTensor *dy,
    const aclTensor *pseShiftOptional, const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional, const aclTensor *softmaxMaxOptional, const aclTensor *softmaxSumOptional,
    const aclTensor *softmaxInOptional, const aclTensor *attentionInOptional, const aclIntArray *prefixOptional,
    const aclIntArray *actualSeqQLenOptional, const aclIntArray *actualSeqKvLenOptional, double scaleValue,
    double keepProb, int64_t preTokens, int64_t nextTokens, int64_t headNum,
    char *inputLayout, int64_t innerPrecise, int64_t sparseMode, const aclTensor *dqOut,
    const aclTensor *dkOut, const aclTensor *dvOut, const aclTensor *dpseOut, uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionUnpaddingScoreGrad的第二段接口，用于执行计算。
 */
aclnnStatus aclnnFlashAttentionUnpaddingScoreGrad(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                                  const aclrtStream stream);


/**
 * @brief aclnnFlashAttentionScoreGradV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
*/
aclnnStatus aclnnFlashAttentionScoreGradV2GetWorkspaceSize(
    const aclTensor *query, const aclTensor *keyIn, const aclTensor *value, const aclTensor *dy,
    const aclTensor *pseShiftOptional, const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional, const aclTensor *softmaxMaxOptional, const aclTensor *softmaxSumOptional,
    const aclTensor *softmaxInOptional, const aclTensor *attentionInOptional, const aclIntArray *prefixOptional,
    const aclIntArray *qStartIdxOptional, const aclIntArray *kvStartIdxOptional, double scaleValue,
    double keepProb, int64_t preTokens, int64_t nextTokens,
    int64_t headNum, char *inputLayout, int64_t innerPrecise, int64_t sparseMode,
    int64_t pseType, const aclTensor *dqOut, const aclTensor *dkOut, const aclTensor *dvOut,
    const aclTensor *dpseOut, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionScoreGradV2的第二段接口，用于执行计算。
*/
aclnnStatus aclnnFlashAttentionScoreGradV2(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                           const aclrtStream stream);

/**
 * @brief aclnnFlashAttentionUnpaddingScoreGradV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
*/
aclnnStatus aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSize(
    const aclTensor *query, const aclTensor *keyIn, const aclTensor *value, const aclTensor *dy,
    const aclTensor *pseShiftOptional, const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional, const aclTensor *softmaxMaxOptional, const aclTensor *softmaxSumOptional,
    const aclTensor *softmaxInOptional, const aclTensor *attentionInOptional, const aclIntArray *prefixOptional,
    const aclIntArray *actualSeqQLenOptional, const aclIntArray *actualSeqKvLenOptional,
    const aclIntArray *qStartIdxOptional, const aclIntArray *kvStartIdxOptional, double scaleValue,
    double keepProb, int64_t preTokens, int64_t nextTokens, int64_t headNum,
    char *inputLayout, int64_t innerPrecise, int64_t sparseMode, int64_t pseType,
    const aclTensor *dqOut, const aclTensor *dkOut, const aclTensor *dvOut, const aclTensor *dpseOut,
    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionUnpaddingScoreGradV2的第二段接口，用于执行计算。
*/
aclnnStatus aclnnFlashAttentionUnpaddingScoreGradV2(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_FLASH_ATTENTION_SCORE_GRAD_H_
// End content from: aclnn_flash_attention_score_grad.h

// Begin content from: aclnn_bitwise_and_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BITWISE_AND_TENSOR_H_
#define OP_API_INC_BITWISE_AND_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBitwiseAndTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与other构成互相推导关系，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与self构成互相推导关系，
 * shape需要与self满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要是self与other推导之后可转换的数据类型，
 * shape需要是self与other，broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseAndTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief aclnnBitwiseAndTensor的第二段接口，用于执行计算。
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明：
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnBitwiseAndTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnBitwiseAndTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

/**
 * @brief aclnnInplaceBitwiseAndTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明：
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与other构成互相推导关系，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持INT16,UINT16,INT32,INT64,INT8,UINT8,BOOL，且数据类型需要与self构成互相推导关系，
 * shape需要与self满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseAndTensorGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* other,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceBitwiseAndTenosr的第二段接口，用于执行计算。
 *
 * 算子功能：完成位与或者逻辑与计算
 * 计算公式：
 * $$ output_i = self_i\&other_i $$
 *
 * 实现说明：
 * 计算图一
 * 场景一：经过类型推导后，self和other的数据类型都为BOOL类型时，需要调用l0::LogicalAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([LogicalAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 * 计算图二
 * 场景二：经过类型推导后，self和other的数据类型都为INT类型时，需要调用l0::BitwiseAnd接口做计算：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C --> G([BitwiseAnd])
 *     D[(other)] -->E([Contiguous])
 *     E --> F([Cast])
 *     F --> G
 *     G --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceBitwiseAndTensorOutGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceBitwiseAndTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BITWISE_AND_TENSOR_H_
// End content from: aclnn_bitwise_and_tensor.h

// Begin content from: aclnn_softshrink.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFTSHRINK_H_
#define OP_API_INC_SOFTSHRINK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftshrink的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 功能描述：以元素为单位，强制收缩λ范围内的元素。
 * 计算公式：如下
 * $$
 * Softshrink(x)=
 * \begin{cases}
 * x-λ, if x > λ \\
 * x+λ, if x < -λ \\
 * 0, otherwise \\
 * \end{cases}
 * $$
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [in]   lambd
 * 输入Scalar，数据类型支持FLOAT。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnSoftshrinkGetWorkspaceSize(const aclTensor* self, const aclScalar* lambd, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnSoftshrink的第二段接口，用于执行计算。
 * 功能描述：以元素为单位，强制收缩λ范围内的元素。
 * 计算公式：如下
 * $$
 * Softshrink(x)=
 * \begin{cases}
 * x-λ, if x > λ \\
 * x+λ, if x < -λ \\
 * 0, otherwise \\
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B -->C([l0op::SoftShrink])
    C -->D([l0op::Cast])
    D -->E([l0op::ViewCopy])
    E -->F[(Out)]

    G((lambd)) -->C
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSoftshrinkGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftshrink(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFTSHRINK_H_
// End content from: aclnn_softshrink.h

// Begin content from: aclnn_upsample_nearest_exact2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef ACLNN_UPSAMPLE_NEAREST_EXACT2D_BACKWARD_H_
#define ACLNN_UPSAMPLE_NEAREST_EXACT2D_BACKWARD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearesrExact2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：aclnnUpsampleNearesrExact2d的反向传播。
 *
 * @param [in] gradOutput: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与出参out的数据类型一致。
 * @param [in] outputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为2。表示输入gradOutput在H和W维度上的空间大小。
 * @param [in] inputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为4。表示输出out分别在N、C、H和W维度上的空间大小。
 * @param [in] scalesH: Host侧的浮点型，表示输出out的height维度乘数。
 * @param [in] scalesW: Host侧的浮点型，表示输出out的width维度乘数。
 * @param [out] out: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与入参gradOutput的数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleNearestExact2dBackwardGetWorkspaceSize(
    const aclTensor *gradOutput, const aclIntArray *outputSize, const aclIntArray *inputSize,  
    double scalesH, double scalesW, aclTensor *out, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleNearestExact2dBackward的第二段接口，用于执行计算。
 * 
 * 算子功能：aclnnUpsampleNearestExact2d的反向传播。
 *
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小。
 * 由第一段接口aclnnUpsampleNearestExact2dBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL Stream流。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleNearestExact2dBackward(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_upsample_nearest_exact2d_backward.h

// Begin content from: aclnn_ones.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ONES_H_
#define OP_API_INC_ONES_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnOneHot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceOneGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnOneHot的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceOne(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ONES_H_
// End content from: aclnn_ones.h

// Begin content from: aclnn_background_replace.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_BACKGROUND_REPLACE_H_
#define ACLNN_BACKGROUND_REPLACE_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnBackgroundReplaceGetWorkspaceSize
 * parameters :
 * bkg : required
 * src : required
 * mask : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBackgroundReplaceGetWorkspaceSize(
    const aclTensor *bkg,
    const aclTensor *src,
    const aclTensor *mask,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnBackgroundReplace
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBackgroundReplace(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_background_replace.h

// Begin content from: aclnn_clamp.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CLAMP_H_
#define OP_API_INC_LEVEL2_ACLNN_CLAMP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnClamp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnClampGetWorkspaceSize(const aclTensor* self, const aclScalar* clipValueMin,
                                                 const aclScalar* clipValueMax, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnClamp的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnClamp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnClampMin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnClampMinGetWorkspaceSize(const aclTensor* self, const aclScalar* clipValueMin,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnClampMin的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnClampMin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

/**
 * @brief aclnnClampTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnClampTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* clipValueMin,
                                                       const aclTensor* clipValueMax, aclTensor* out,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnClampMinTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnClampMinTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* clipValueMin,
                                                          aclTensor* out, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnClampMinTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnClampMinTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);

/**
 * @brief aclnnInplaceClampMinTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceClampMinTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* clipValueMin,
                                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceClampMinTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceClampMinTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 const aclrtStream stream);

/**
 * @brief aclnnClampTensor的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnClampTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

/**
 * @brief aclnnClampMax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入的所有元素限制在[-inf,max]范围内。
 * 计算公式：
 * $$ {y}_{i} = min({{x}_{i}},max) $$
 *
 * @param [in] self: 输入tensor，数据类型支持FLOAT16、FLOAT、FLOAT64、INT8、UINT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] clipValueMax: 上界，数据类型需要可转换成self的数据类型。
 * @param [in] out: 输出，数据类型支持FLOAT16、FLOAT、FLOAT64、INT8、UINT8、INT16、INT32、INT64，
 * 且数据类型和self保持一致，shape和self保持一致，数据格式支持ND（[参考](#)）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnClampMaxGetWorkspaceSize(const aclTensor* self, const aclScalar* clipValueMax,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnClampMax的第二段接口，用于执行计算。
 *
 * 算子功能：将输入的所有元素限制在[-inf,max]范围内。
 * 计算公式：
 * $$ {y}_{i} = min({{x}_{i}},max) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnClampMaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnClampMax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceClampMax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入的所有元素限制在[-inf,max]范围内。
 * 计算公式：
 * $$ {y}_{i} = min({{x}_{i}},max) $$
 *
 * @param [in] selfRef: 输入tensor，数据类型支持FLOAT16、FLOAT、FLOAT64、INT8、UINT8、INT16、INT32、INT64。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] clipValueMax: 上界，数据类型需要可转换成self的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceClampMaxGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* clipValueMax,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceClampMax的第二段接口，用于执行计算。
 *
 * 算子功能：将输入的所有元素限制在[-inf,max]范围内。
 * 计算公式：
 * $$ {y}_{i} = min({{x}_{i}},max) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceClampMaxGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceClampMax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

/**
 * @brief aclnnClampMaxTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入的所有元素限制在[-inf, max]范围内。
 *
 * @param [in] self: 输入tensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64，
 * 且数据类型需要与max的数据类型需满足数据类型推导规则，shape需要与max满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] max: 输入上限值tensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64
 * 且数据类型需要与self的数据类型需满足数据类型推导规则，shape需要与max满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: 输出tensor，数据类型支持FLOAT16、FLOAT、DOUBLE、INT8、UINT8、INT16、INT32、INT64，
 * 且数据类型需要是self与max推导之后可转换的数据类型，shape需要是self与max
 * broadcast之后的shape。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnClampMaxTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* max, aclTensor* out,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceClampMaxTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceClampMaxTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* max,
                                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnClampMaxTensor的第二段接口，用于执行计算。
 *
 * 算子功能：将输入的所有元素限制在[-inf, max]范围内。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnClampMaxTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnClampMaxTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceClampMaxTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CLAMP_H_// End content from: aclnn_clamp.h

// Begin content from: aclnn_foreach_expm1.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_EXPM1_H_
#define ACLNN_FOREACH_EXPM1_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachExpm1GetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachExpm1GetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachExpm1
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachExpm1(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_expm1.h

// Begin content from: aclnn_masked_softmax_with_rel_pos_bias.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MASKED_SOFTMAX_WITH_REL_POS_BIAS_H_
#define ACLNN_MASKED_SOFTMAX_WITH_REL_POS_BIAS_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMaskedSoftmaxWithRelPosBiasGetWorkspaceSize
 * parameters :
 * x : required
 * attenMaskOptional : optional
 * relativePosBias : required
 * scaleValue : optional
 * innerPrecisionMode : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMaskedSoftmaxWithRelPosBiasGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *attenMaskOptional,
    const aclTensor *relativePosBias,
    double scaleValue,
    int64_t innerPrecisionMode,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMaskedSoftmaxWithRelPosBias
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMaskedSoftmaxWithRelPosBias(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_masked_softmax_with_rel_pos_bias.h

// Begin content from: aclnn_foreach_pow_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_POW_SCALAR_H_
#define ACLNN_FOREACH_POW_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachPowScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachPowScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_pow_scalar.h

// Begin content from: aclnn_affine_grid.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_AFFINE_GRID_H_
#define OP_API_INC_AFFINE_GRID_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAffineGrid的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：给定一组仿射矩阵(theta)，生成一个2D或3D的网络来表示仿射变换后图像的坐标在原图像上的坐标位置。
 *
 * @param [in] theta: npu device侧的aclTensor，表示仿射变换的仿射参数。
 * 数据类型支持FLOAT和FLOAT16。shape是(N, 2, 3)或(N, 3, 4)。支持非连续的Tensor，数据格式支持ND。
 * @param [in] size: host侧的aclIntArray，表示要要输出的图像的size。大小为4(N, C, H, W)或者5(N, C, D, H, W)。
 * @param [in] alignCorners: host侧的bool。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT和FLOAT16，且数据类型与theta一致。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAffineGridGetWorkspaceSize(const aclTensor* theta, const aclIntArray* size,
                                                      bool alignCorners, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnAffineGrid的第二段接口，用于执行计算。
 *
 * 算子功能：给定一组仿射矩阵(theta)，生成一个2D或3D的网络来表示仿射变换后图像的坐标在原图像上的坐标位置。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAffineGridGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAffineGrid(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AFFINE_GRID_H_
// End content from: aclnn_affine_grid.h

// Begin content from: aclnn_batch_matmul.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_ACLNN_BATCHMATMUL_H
#define OP_API_ACLNN_BATCHMATMUL_H

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchMatMul的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnBatchMatMulGetWorkspaceSize(const aclTensor* self, const aclTensor* mat2, aclTensor* out,
                                                       int8_t cubeMathType, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnBatchMatMul的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchMatMul(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_ACLNN_BATCHMATMUL_H// End content from: aclnn_batch_matmul.h

// Begin content from: aclnn_foreach_sub_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SUB_SCALAR_LIST_H_
#define ACLNN_FOREACH_SUB_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSubScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSubScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sub_scalar_list.h

// Begin content from: aclnn_erf.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ERF_H_
#define OP_API_INC_LEVEL2_ACLNN_ERF_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnErf的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素对应的误差函数的值
 * 计算公式：
 * $$ erf(x)=\frac{2}{\sqrt{\pi } } \int_{0}^{x} e^{-t^{2} } \mathrm{d}t $$
 *
 * 计算图一：如下所示
 * 场景：当输入类型在Erf算子支持的范围之内（FLOAT32、FLOAT16、BFLOAT16、FLOAT64）时，使用Erf算子完成计算。
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B([l0op::Contiguous])
 *     B -->C([l0op::Erf])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(out)]
 * ```
 *
 * 计算图二：如下所示
 * 场景：self的数据类型为BOOL，将self的数据类型CAST为FLOAT32，再使用Erf算子完成计算。
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B([l0op::Contiguous])
 *     B -->C([l0op::Cast])
 *     C -->D([l0op::Erf])
 *     D -->H([l0op::Cast])
 *     H --> E([l0op::ViewCopy])
 *     E --> F[(out)]
 * ```
 *
 * @param [in] self: 待进行erf计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16、BOOL、INT64，数据格式支持ND， 支持非连续的Tensor。
 * @param [in] out: erf计算的出参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16，数据格式支持ND， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErfGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnErf的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnErfGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnErf(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceErf的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：返回输入Tensor中每个元素对应的误差函数的值
 * 计算公式：
 * $$ erf(x)=\frac{2}{\sqrt{\pi } } \int_{0}^{x} e^{-t^{2} } \mathrm{d}t $$
 *
 * 计算图：如下所示
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::Erf])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(out)]
 * ```
 *
 * @param [in] selfRef: 待进行erf计算的入参。npu device侧的aclTensor，
 * 数据类型支持FLOAT64、FLOAT32、FLOAT16、BFLOAT16、BOOL，数据格式支持ND， 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErfGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnInplaceErf的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnErfGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceErf(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ERF_H_// End content from: aclnn_erf.h

// Begin content from: aclnn_le_tensor.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_LE_TENSOR_H_
#define OP_API_INC_LEVEL2_ACLNN_LE_TENSOR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：计算两个Tensor中的元素是否小于等于other的值，将self每个元素与other的值的比较结果写入out中
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE数据类型，
 * 数据类型需要与other构成相互推导关系，shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: npu device侧的aclTensor，
 * 数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE数据类型，
 * 数据类型需要与other构成相互推导关系，shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu device侧的aclTensor，
 * 数据类型支持INT8,UINT8,INT16,UINT16,INT32,INT64,FLOAT16,FLOAT,DOUBLE数据类型，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLeTensor的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLeTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceLeTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否小于等于other Tensor的值
 * 计算公式： $$ selfRef_{i} = (selfRef_{i} <= other_{i}) ? True : False $$
 *
 * @param [in] selfRef: 待进行le本地计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL、BFLOAT16、BFLOAT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: 待进行ge计算的入参,aclTensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLeTensor的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLeTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLeTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_LE_TENSOR_H_
// End content from: aclnn_le_tensor.h

// Begin content from: aclnn_gelu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GELU_BACKWARD_H_
#define OP_API_INC_GELU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成Gelu的反向。
 * Gelu正向（其中x可以为标量或者Tensor）：
 * $$
 * Gelu(x)=x \cdot \Phi(x)=x/2 \cdot [1+erf(x/\sqrt{2})]
 * $$
 * 其中erf的计算公式为：
 * $$
 * erf(x)=\frac{2}{\sqrt \pi}\sum^{\infty}_{n=0}{\frac{(-1)^n \cdot x^{2n+1}}{n! \cdot (2n+1)}}
 * $$
 * gradInput和gradOutput的关系可以表示为：
 * $$
 * gradInput = gradOutput \cdot (\frac{1}{2}+\frac{1}{2} \cdot \\
 * erf(\frac{x}{\sqrt2})+\frac{x}{\sqrt{2\pi}} \cdot e^{-\frac{x^2}{2}})
 * $$
 * 附：Gelu近似计算公式为：
 * $$
 * Gelu(x)=0.5x(1+tanh(\sqrt{2/\pi}(x+0.044715x^3)))
 * $$
 *
 * 实现说明
 * api计算基本路径：
 * ```mermaid
 * graph LR
 * A[(gradOutput)] -->B([l0op::Contiguous])
 * B --> C([l0op::GeluGrad])
 * D[(self)] --> E([l0op::Contiguous])
 * H[(gradInput)] --> I([l0op::Contiguous])
 * E --> C
 * I --> C
 * C --> F([l0op::ViewCopy])
 * F --> G[(gradInput)]
 * ```
 *
 * @param [in] gradOutput：反向传播的梯度值，即上一层的输出梯度，和正向输出的shape一致。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND，支持非连续的Tensor。
 * @param [in] self：Gelu的输出值。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND，支持非连续的Tensor。
 * @param [out] gradInput：backward的输出，为输入的梯度值，即对输入进行求导后的结果。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                        const aclTensor* gradInput, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnGeluBackward的第二段接口，用于执行计算
 */
ACLNN_API aclnnStatus aclnnGeluBackward(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                        const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GELU_BACKWARD_H_
// End content from: aclnn_gelu_backward.h

// Begin content from: aclnn_matmul_all_reduce.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023-2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_matmul_all_reduce.h
 * \brief
 */
#ifndef OP_API_INC_MATMUL_ALL_REDUCE_
#define OP_API_INC_MATMUL_ALL_REDUCE_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMatmulAllReduce的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bf16。
 * @param [in] x2: matmul右矩阵，数据类型支持：float16, bf16。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：0/1。
 * @param [out] output: 计算+通信的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduceGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                           const aclTensor* bias, const char* group,
                                                           const char* reduceOp, int64_t commTurn, int64_t streamMode,
                                                           const aclTensor* output, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnMatmulAllReduce的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnMatmulAllReduceGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduce(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MATMUL_ALL_REDUCE_// End content from: aclnn_matmul_all_reduce.h

// Begin content from: aclnn_eq_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_EQ_SCALAR_H_
#define OP_API_INC_LEVEL2_ACLNN_EQ_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEqScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 计算self中的元素的值与other的值是否相等，将self每个元素与other的值的比较结果写入out中：
 *
 * $$ out_i = (self_i == \mathit{other} )  ?  [True] : [False] $$
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([l0op::Contiguous])
 * B -->C1([l0op::Cast])-->D([l0op::Equal])
 *     F((other)) -->D
 *     D -->F1([l0op::ViewCopy])--> J[(out)]
 * ```
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持FLOAT16, FLOAT, INT64, UINT64, INT32, INT8, UINT8, BOOL, UINT32, BFLOAT16, DOUBLE, INT16, COMPLEX64,
 * COMPLEX128，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: host侧的aclScalar，数据类型支持FLOAT16, FLOAT, INT64, UINT64, INT32, INT8, UINT8, BOOL, UINT32,
 * BFLOAT16, DOUBLE, INT16, COMPLEX64, COMPLEX128
 * @param [in] out: npu device侧的aclTensor，数据类型为bool，shape与self的shape一致，数据格式支持ND
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnEqScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnEqScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEqScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

/**
 * @brief aclnnInplaceEqScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 计算self中的元素的值与other的值是否相等，将self每个元素与other的值的比较结果原地返回到self中：
 *
 * $$ self_i = (self_i == \mathit{other} )  ?  [True] : [False] $$
 *
 * 计算图：
*```mermaid
* graph LR
* A[(SelfRef)] -->B([l0op::Contiguous])
* B -->C1([l0op::Cast])-->D([l0op::Equal])
* F((other)) -->D
* D -->F1([l0op::ViewCopy])--> J[(selfRef)]
```
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16, FLOAT, INT64, UINT64, INT32, INT8, UINT8, BOOL, UINT32, BFLOAT16, DOUBLE, INT16, COMPLEX64,
 * COMPLEX128，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: host侧的aclScalar，数据类型支持FLOAT16, FLOAT, INT64, UINT64, INT32, INT8, UINT8, BOOL, UINT32,
 * BFLOAT16, DOUBLE, INT16, COMPLEX64, COMPLEX128
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceEqScalarGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceEqScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEqScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceEqScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_EQ_SCALAR_H_// End content from: aclnn_eq_scalar.h

// Begin content from: aclnn_ascend_anti_quant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ASCEND_ANTI_QUANT_H_
#define OP_API_INC_LEVEL2_ACLNN_ASCEND_ANTI_QUANT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAscendAntiQuant的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 待进行AscendAntiQuant计算的入参。npu device侧的aclTensor，
 * 数据类型支持int8, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] scale: npu device侧的aclTensor, 数据类型支持float, bf16
 * @param [in] offset: npu device侧的aclTensor，数据类型支持float, bf16
 * @param [in] dstType:  host侧的aclScalar, 数据类型int
 * @param [in] sqrtMode:  host侧的aclScalar，数据类型bool
 * @param [in] y: AscendAntiQuant计算的出参。npu device侧的aclTensor，
 * 数据类型支持float16, bf16, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendAntiQuantGetWorkspaceSize(const aclTensor* x, const aclTensor* scale,
                                                           const aclTensor* offset, int64_t dstType, bool sqrtMode,
                                                           const aclTensor* y, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnAscendAntiQuant的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAscendAntiQuantGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendAntiQuant(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ASCEND_ANTI_QUANT_H_// End content from: aclnn_ascend_anti_quant.h

// Begin content from: aclnn_nonzero_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_NONZERO_V2_H_
#define OP_API_INC_NONZERO_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNonzeroV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnNonzeroV2GetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnNonzeroV2的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnNonzeroV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_nonzero_v2.h

// Begin content from: aclnn_cummin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_CUMMIN_H_
#define OP_API_INC_LEVEL2_ACLNN_CUMMIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：计算self中的累积最小值，并返回该值以及其对应的索引
 * 计算公式：
 * $self_{i}$是输入张量self中，从维度dim视角来看的某个元素（其它维度下标不变，只dim维度下标依次递增），
 * $$
 * valuesOut_{i} = min(self_{1}, self_{2}, self_{3}, ......, self_{i})
 * $$
 * $$
 * indicesOut_{i} = argmin(self_{1}, self_{2}, self_{3}, ......, self_{i})
 * $$
 */

/**
 * @brief aclnnCummin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、DOUBLE、UINT8、INT8、INT16、INT32、INT64、
 * FLOAT16、BFLOAT16、BOOL，数据类型需要能转换成out的数据类型，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] dim: host侧的整数，数据类型支持INT64。
 * @param [in] valuesOut：npu device侧的aclTensor，数据类型支持FLOAT、DOUBLE、UINT8、INT8、INT16、INT32、INT64、
 * FLOAT16、BFLOAT16、BOOL，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上, 且shape必须与self一致。
 * @param [in] indicesOut：npu device侧的aclTensor，数据类型支持INT32、INT64。支持非连续的Tensor，数据格式支持ND，
 * 数据维度不支持8维以上, 且shape必须与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCumminGetWorkspaceSize(const aclTensor* self, int64_t dim, aclTensor* valuesOut,
                                                  aclTensor* indicesOut, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnCummin的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCumminGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCummin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_CUMMIN_H_
// End content from: aclnn_cummin.h

// Begin content from: aclnn_max_pool3d_with_argmax_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_GRAD_WITH_ARGMAX_H_
#define OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_GRAD_WITH_ARGMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxPool3dWithArgmaxBackward First segment interface. Calculate the workspace size based on the specific calculation process.
 * Function description: Calculates MaxPool3DGradWithArgmax from the gradOutput and self, at the result gradInput
 * @domain aclnn_ops_train
 * @param [in] gradOutput: gradient Tensor, which is the same as the positive output shape. It is aclTensor on the NPU device side. The data type can be float32/float16/bfloat16. Data format: ND and discontinuous tensors are supported. Only 4- or 5-dimensional tensors are supported.
     *  If there are four dimensions, the value of N is considered as 1, and the value of each dimension must be greater than 0.
     *  If the value is five dimensions, all dimensions except dimension 0 must be greater than 0. When dimension 0 is 0, gradInput is empty.
 * @param [in] self: aclTensor on the NPU device side, positive operator input. The data type can be float32/float16/bfloat16. . Data format: ND and discontinuous tensors are supported. Only 4- or 5-dimensional tensors are supported.
 * @param [in] indices: aclTensor on the NPU device side, which is the output of the forward operator. The index data with the maximum value on the DHW plane is int64 and ND is supported. Supports discontinuous tensors. Only 4- or 5-dimensional tensors are supported.
 * @param [in] kernelSize: aclIntArray type, indicating the maxpooling window size.
 * @param [in] stride: aclIntArray type, the step size of the window movement.
 * @param [in] padding: aclIntArray type, number of padding layers for each edge. The value is negative infinity.
 * @param [in] dilation: aclIntArray type, controls the stride of elements in the window.
 * @param [in] ceilMode: bool type, when true, the output shape is calculated using round-up method. By default is rounding down.
 * @param [in] gradInput: It is the same as the positive input shape. It is the aclTensor on the NPU device side. The data type can be float16, float32, or bfloat16. Added the data format ND and discontinuous tensors.
 * @param [out] workspaceSize: Returns the workspace size that the user needs to apply for on the npu device side.
 * @param [out] executor: Return the op executor, including the operator calculation process.
 * @return aclnnStatus: Return the status code.
 */

ACLNN_API aclnnStatus aclnnMaxPool3dWithArgmaxBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                       const aclTensor* indices, const aclIntArray* kernelSize,
                                                                       const aclIntArray* stride, const aclIntArray* padding,
                                                                       const aclIntArray* dilation, bool ceilMode,
                                                                       aclTensor* gradInput, uint64_t* workspaceSize,
                                                                       aclOpExecutor** executor);
/**
 * @brief A second interface of aclnnMaxPool3dWithArgmaxBackward, used to perform calculation.
 * @param [in] workspace: start address of the workspace memory allocated on the NPU device.
 * @param [in] workspace_size: size of the workspace applied on the NPU device, which is obtained by calling the first segment interface aclnnMaxPool3dWithIndicesBackwardGetWorkspaceSize.
 * @param [in] exector: op executor, including the operator calculation process.
 * @param [in] stream: acl stream.
 * @return aclnnStatus: returned status code
 */

ACLNN_API aclnnStatus aclnnMaxPool3dWithArgmaxBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, 
                                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MAX_POOL3D_GRAD_WITH_ARGMAX_H_// End content from: aclnn_max_pool3d_with_argmax_backward.h

// Begin content from: aclnn_moe_init_routing.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_INIT_ROUTING_H_
#define ACLNN_MOE_INIT_ROUTING_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeInitRoutingGetWorkspaceSize
 * parameters :
 * x : required
 * rowIdx : required
 * expertIdx : required
 * activeNum : required
 * expandedXOut : required
 * expandedRowIdxOut : required
 * expandedExpertIdxOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRoutingGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *rowIdx,
    const aclTensor *expertIdx,
    int64_t activeNum,
    const aclTensor *expandedXOut,
    const aclTensor *expandedRowIdxOut,
    const aclTensor *expandedExpertIdxOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeInitRouting
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeInitRouting(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_init_routing.h

// Begin content from: aclnn_argmin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ARGMIN_H_
#define OP_API_INC_ARGMIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnARGMIN的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：返回张量在指定维度上的最小值的索引。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMin])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、FLOAT64、INT8、
 * INT16、INT32、INT64、UINT8、BFLOAT16，数据格式支持ND。支持非连续的Tensor。
 * @param [in] dim: host侧int64类型，指定了要进行最小值计算的维度。
 * @param [in] keepdim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] out: npu device侧的aclTensor，数据类型支持INT64。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnArgMinGetWorkspaceSize(const aclTensor* self, int64_t dim, bool keepdim,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnArgMin的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：返回张量在指定维度上的最大值的索引。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *  A[(self)] -.->B([l0op::Contiguous])
 *  B --> C([l0op::ArgMin])
 *  C --> F([l0op::Cast])
 *  D([dim]) --> C
 *  F -.-> E([l0op::ViewCopy])
 *  E --> O[(Out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnArgMinGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnArgMin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ARGMIN_H_// End content from: aclnn_argmin.h

// Begin content from: aclnn_unique2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNIQUE2_H_
#define OP_API_INC_UNIQUE2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUnique2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回输入张量中的独特元素
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B ---> F([UniqueWithCountsAndSorting])
 *     C[(sorted)] --->F
 *     D[(returnInverse)] --->F
 *     E[(returnCounts)] --->F
 *     F --> G([valueOut])
 *     F --> H([inverseOut])
 *     F --> I([countsOut])
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持BOOL, FLOAT, FLOAT16, DOUBLE, UINT8, INT8, UINT16, INT16,
 * INT32, UINT32, UINT64, INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] sorted: 可选参数，默认False，表示是否对 valueOut 按升序进行排序。
 * @param [in] returnInverse: 可选参数，默认False，表示是否返回输入数据中各个元素在 valueOut 中的下标。
 * @param [in] returnCounts: 可选参数，默认False，表示是否返回 valueOut 中每个独特元素在原输入Tensor中的数目。
 * @param [in] valueOut: npu device侧的aclTensor, 第一个输出张量，输入张量中的唯一元素，数据类型支持BOOL, FLOAT,
 * FLOAT16, DOUBLE, UINT8, INT8, UINT16, INT16, INT32, UINT32, UINT64, INT64，数据格式支持ND。
 * @param [in] inverseOut: npu
 * device侧的aclTensor，第二个输出张量，当returnInversie为True时有意义，返回self中各元素在valueOut中出现的位置下
 *                      标，数据类型支持INT64，shape与self保持一致
 * @param [in] countsOut: npu
 * device侧的aclTensor，第三个输出张量，当returnCounts为True时有意义，返回valueOut中各元素在self中出现的次数，数据
 *                     类型支持INT64，shape与valueOut保持一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUnique2GetWorkspaceSize(const aclTensor* self, bool sorted, bool returnInverse,
                                                   bool returnCounts, aclTensor* valueOut, aclTensor* inverseOut,
                                                   aclTensor* countsOut, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnUnique2的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnUnique2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUnique2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNIQUE2_H_// End content from: aclnn_unique2.h

// Begin content from: aclnn_inverse.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_INVERSE_H_
#define OP_API_INC_LEVEL2_ACLNN_INVERSE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：取方阵的逆矩阵。
 * 计算公式：如下
 * $$
 * {A}^{-1}A = A{A}^{-1} = {I}_{n}
 * $$
 * 其中A是输入张量，${A}^{-1}$是A的逆，${I}_{n}$是n维的单位矩阵。
 *
 * 计算图一：如下
 * 场景：输入self的数据类型不是FLOAT16。
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::MatrixInverse])
 *   C --> D([l0op::Cast])
 *   D --> E([l0op::ViewCopy])
 *   E --> F[(out)]
 * ```
 *
 * 计算图二：如下
 * 场景：当输入self的数据类型是FLOAT16时，需要将FLOAT16转成FLOAT32，传给算子计算。
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::Cast])
 *   C --> D([l0op::MatrixInverse])
 *   D --> E([l0op::Cast])
 *   E --> F([l0op::ViewCopy])
 *   F --> G[(out)]
 * ```
 */

/**
 * @brief aclnnInverse的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128、FLOAT16，shape至少是2维，
 * 且最后两维的大小必须相同，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、BFLOAT16、COMPLEX64、COMPLEX128，且数据
 * 类型需要是self可转换的数据类型，shape需要与self的shape一致，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInverseGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnInverse的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInverseGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInverse(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_INVERSE_H_// End content from: aclnn_inverse.h

// Begin content from: aclnn_unique_dim.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNIQUE_DIM_H_
#define OP_API_INC_UNIQUE_DIM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUniqueDim的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：在某一dim轴上，对输入张量self做去重操作。。
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *   A[(self)] ---> B([l0op::Contiguous])
 *   B ---> F([l0op::UniqueDim])
 *   J((sorted)) ---> F
 *   C((returnInverse)) --->F
 *   E((dim)) --->F
 *   F --> G[(valueOut)] --> F
 *   F --> H[(inverseOut)] -->F
 *   F --> I[(countsOut)] --> F
 * ```
 * @param [in]
 * self：Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、UINT16、INT16、UINT32、INT32、UINT64、
 *                   INT64、FLOAT64 。支持非连续的Tensor，数据格式支持ND。
 * @param [in] sorted: 表示返回的输出结果valueOut是否排序。
 * @param [in] returnInverse: 表示是否返回self在dim轴上各元素在valueOut中对应元素的位置下标，True时返回，False时不返回。
 * @param [in] dim: Host侧的整型，指定做去重操作的维度，数据类型支持INT64，取值范围为[-self.dim(), self.dim())。
 * @param [in] valueOut:
 * 第一个输出张量，表示去重结果，Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、UINT16、
 *                       INT16、UINT32、INT32、UINT64、INT64、FLOAT64。支持非连续的Tensor，数据格式支持ND。
 * @param [in] inverseOut: 第二个输出张量，表示self在dim轴上各元素在valueOut中对应元素的位置下标，数据类型支持INT64。
 * @param [in] countsOut: 第三个输出张量，表示valueOut中的各元素在self中出现的次数，数据类型支持INT64。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUniqueDimGetWorkspaceSize(const aclTensor* self, bool sorted, bool returnInverse,
                                                     int64_t dim, aclTensor* valueOut, aclTensor* inverseOut,
                                                     aclTensor* countsOut, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnUniqueDim的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnUniqueDim获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUniqueDim(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNIQUE_DIM_H_// End content from: aclnn_unique_dim.h

// Begin content from: aclnn_group_norm_silu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUP_NORM_SILU_H_
#define OP_API_INC_GROUP_NORM_SILU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupNormSilu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成GroupNorm和Silu融合算子功能。
 * 计算公式是：
 * y=((x-Ex) / (sqrt(Var(x)+ϵ​x))​)∗ γ + β
 * 将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值
 *
 * 附：Silu计算公式为：
 * f(x) = x * sigmoid(x)
 * 当x大于0时,Silu激活函数将会放大x,而当x小于0时,Silu激活函数将会降低x,可以抑制过拟合
 * ```
 *
 * @param [in] self：计算输入，shape大于等于2维
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND，支持非连续的Tensor。
 * @param [in] gamma：计算输入，shape为1维，且等于c维度。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND，支持非连续的Tensor。
 * @param [in] beta：shape为1维，且等于c维度。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND，支持非连续的Tensor。
 * @param [in] group：计算属性，host侧的整数，数据类型支持INT64,分组信息
 * @param [in] esp：计算属性，host侧的浮点数，数据类型支持double,默认le-5
 * @param [out] out：y的输出。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND。
 * @param [out] meanOut：均值的输出。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND。
 * @param [out] rstdOut：1/sqrt(Var(x)+ϵ​x)结果输出
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupNormSiluGetWorkspaceSize(const aclTensor* self, const aclTensor* gamma,
                                                         const aclTensor* beta, int64_t group, double eps,
                                                         aclTensor* out, aclTensor* meanOut, aclTensor* rstdOut,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGroupNormSilu的第二段接口，用于执行计算
 */
ACLNN_API aclnnStatus aclnnGroupNormSilu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         const aclrtStream stream);

/**
 * @brief aclnnGroupNormSiluV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnGroupNormSiluV2GetWorkspaceSize(const aclTensor* self, const aclTensor* gamma,
                                                           const aclTensor* beta, int64_t group, double eps,
                                                           bool activateSilu, aclTensor* out, aclTensor* meanOut,
                                                           aclTensor* rstdOut, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnGroupNormSiluV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GROUP_NORM_SILU_H_
// End content from: aclnn_group_norm_silu.h

// Begin content from: aclnn_rms_norm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_RMS_NORM_H_
#define ACLNN_RMS_NORM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnRmsNormGetWorkspaceSize
 * parameters :
 * x : required
 * gamma : required
 * epsilon : optional
 * yOut : required
 * rstdOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRmsNormGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *gamma,
    double epsilon,
    const aclTensor *yOut,
    const aclTensor *rstdOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnRmsNorm
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRmsNorm(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_rms_norm.h

// Begin content from: aclnn_add_rms_norm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_ADD_RMS_NORM_H_
#define ACLNN_ADD_RMS_NORM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnAddRmsNormGetWorkspaceSize
 * parameters :
 * x1 : required
 * x2 : required
 * gamma : required
 * epsilon : optional
 * yOut : required
 * rstdOut : required
 * xOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddRmsNormGetWorkspaceSize(
    const aclTensor *x1,
    const aclTensor *x2,
    const aclTensor *gamma,
    double epsilon,
    const aclTensor *yOut,
    const aclTensor *rstdOut,
    const aclTensor *xOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnAddRmsNorm
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddRmsNorm(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_add_rms_norm.h

// Begin content from: aclnn_moe_token_unpermute.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_TOKEN_UNPERMUTE_H_
#define ACLNN_MOE_TOKEN_UNPERMUTE_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeTokenUnpermuteGetWorkspaceSize
 * parameters :
 * permutedTokens : required
 * sortedIndices : required
 * probsOptional : optional
 * paddedMode : optional
 * restoreShapeOptional : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenUnpermuteGetWorkspaceSize(
    const aclTensor *permutedTokens,
    const aclTensor *sortedIndices,
    const aclTensor *probsOptional,
    bool paddedMode,
    const aclIntArray *restoreShapeOptional,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeTokenUnpermute
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenUnpermute(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_token_unpermute.h

// Begin content from: aclnn_upsample_trilinear_3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UPSAMPLE_TRILINEAR_3D_BACKWARD_H_
#define OP_API_INC_UPSAMPLE_TRILINEAR_3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleTrilinear3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnUpsampleTrilinear3dBackwardGetWorkspaceSize(const aclTensor* gradOut,
                                                                       const aclIntArray* outputSize,
                                                                       const aclIntArray* inputSize, bool alignCorners,
                                                                       double scalesD, double scalesH, double scalesW,
                                                                       aclTensor* gradInput, uint64_t* workspaceSize,
                                                                       aclOpExecutor** executor);

/**
 * @brief aaclnnUpsampleTrilinear3dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleTrilinear3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UPSAMPLE_TRILINEAR_3D_BACKWARD_H_// End content from: aclnn_upsample_trilinear_3d_backward.h

// Begin content from: aclnn_remainder.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_REMAINDER_H_
#define OP_API_INC_LEVEL2_ACLNN_REMAINDER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

// tensor self, tensor other
/**
 * @brief aclnnRemainderTensorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] self: npu
 * device侧的aclTensor，self的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的数据类型
 * 必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。shape需要与other满足[broadcast关系](#)。支持[非连续的Tensor](#)，
 * 数据格式支持ND([参考](#))。
 * @param [in] other: npu
 * device侧的aclTensor。self的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的数据
 * 类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。shape需要与self满足[broadcast关系](#)。支持[非连续的Tensor]
 * (#)，数据格式支持ND([参考](#))。
 * @param [in] out: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、FLOAT16、FLOAT、DOUBLE、
 * COMPLEX64、COMPLEX128类型，且数据类型需要是self与other推导之后可转换的数据类型([参考](#))。shape需要是self与other
 * broadcast之后的shape。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRemainderTensorTensorGetWorkspaceSize(const aclTensor* self, const aclTensor* other,
                                                                 aclTensor* out, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnRemainderTensorTensor的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnRemainderTensorTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRemainderTensorTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

// tensor self, scalar other
/**
 * @brief aclnnRemainderTensorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] self: npu device侧的aclTensor，self的数据类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。
 * 支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] other:
 * host侧的aclScalar。self的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的数据类型必须
 * 属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。shape需要与self满足[broadcast关系](#)。支持[非连续的Tensor](#)，数据
 * 格式支持ND([参考](#))。
 * @param [in] out: npu device侧的aclTensor，数据类型支持UINT8、INT8、INT16、INT32、INT64、FLOAT16、FLOAT、DOUBLE、
 * COMPLEX64、COMPLEX128类型，且数据类型需要是self可转换的数据类型（[参考](#)）。shape需要与self一致。支持[非连续的Tensor](#)，
 * 数据格式支持ND（[参考](#)。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRemainderTensorScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other,
                                                                 aclTensor* out, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnRemainderTensorScalar的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnRemainderTensorScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRemainderTensorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

// scalar self, tensor other
/**
 * @brief aclnnRemainderScalarTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] self: host侧的aclScalar，self的数据类型需要能转换成other的数据类型。
 * @param [in] other: npu
 * device侧的aclTensor。other的数据类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。支持
 * [非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持INT32、INT64、FLOAT16、FLOAT、DOUBLE类型，且数据类型需要与other的数
 * 据类型一致（[参考](#)）。shape需要与other一致。支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRemainderScalarTensorGetWorkspaceSize(const aclScalar* self, const aclTensor* other,
                                                                 aclTensor* out, uint64_t* workspaceSize,
                                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnRemainderScalarTensor的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnRemainderScalarTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRemainderScalarTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);

// inplace
// tensor self, tensor other
/**
 * @brief aclnnInplaceRemainderTensorTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] selfRef: npu
 * device侧的aclTensor。selfRef的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的
 * 数据类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种，且需要是推导之后可转换的数据类型（[参考](#)）。shape需要与
 * other满足[broadcast关系](#)，并且shape与最终broadcast关系的结果一致。支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] other: npu
 * device侧的aclTensor。selfRef的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的数据
 * 类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种。shape需要与selfRef满足[broadcast关系](#)。支持
 * [非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRemainderTensorTensorGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                                        uint64_t* workspaceSize,
                                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceRemainderTensorTensor的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口
 * aclnnInplaceRemainderTensorTensorGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRemainderTensorTensor(void* workspace, uint64_t workspaceSize,
                                                        aclOpExecutor* executor, aclrtStream stream);

// tensor self, scalar other
/**
 * @brief aclnnInplaceRemainderTensorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] selfRef: npu
 * device侧的aclTensor。selfRef的数据类型与other的数据类型需满足[数据类型推导规则](#)，并且推导出的数
 * 据类型必须属于INT32、INT64、FLOAT16、FLOAT、DOUBLE类型中的一种，且需要是推导之后可转换的数据类型（[参考](#)）。支持
 * [非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] other: host侧的aclScalar。other的数据类型与selfRef的数据类型需满足[数据类型推导规则](#)，并且推导出的数据
 * 类型必须能转换为selfRef的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRemainderTensorScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                                        uint64_t* workspaceSize,
                                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceRemainderTensorScalar的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成remainder操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口
 * aclnnInplaceRemainderTensorScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRemainderTensorScalar(void* workspace, uint64_t workspaceSize,
                                                        aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_REMAINDER_H_// End content from: aclnn_remainder.h

// Begin content from: aclnn_foreach_sqrt.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SQRT_H_
#define ACLNN_FOREACH_SQRT_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSqrtGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSqrtGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSqrt
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSqrt(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sqrt.h

// Begin content from: aclnn_log2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG2_H_
#define OP_API_INC_LOG2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLog2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成自然对数的计算
 * 计算公式：
 * $$ output = log_2(self) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(Self)] -->B([Contiguous])
 *     B --> C([Cast])
 *     C -->D([Log])
 *     E[(Out)] -->F([Contiguous])
 *     F --> G([Cast])
 *     G -->D([Log])
 *     D --> H([Cast])
 *     H --> I([ViewCopy])
 *     I --> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog2GetWorkspaceSize(const aclTensor* self, const aclTensor* out, uint64_t* workspace_size,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnLog2的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLog2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLog2(void* workspace, uint64_t workspace_size, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceLog2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成自然对数的计算
 * 计算公式：
 * $$ output_i=log_2(self_i) $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(selfRef)] -->B([l0op::Contiguous])
 *     B -->D([l0op::Log])
 *     D --> I([l0op::ViewCopy])
 *     I --> J[(selfRef)]
 * ```
 *
 * @param [in] selfRef(aclTensor*): 数据类型支持FLOAT、FLOAT16、BFLOAT16（910B支持），
 * 支持[非连续的Tensor](https://)，数据格式支持ND，selfRef也是输出结果。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog2GetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLog2的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceLog2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLog2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG2_H_// End content from: aclnn_log2.h

// Begin content from: aclnn_x_log_y_scalar_self.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_X_LOG_Y_SCALAR_SELF_H_
#define OP_API_INC_X_LOG_Y_SCALAR_SELF_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnXLogYScalarSelf的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnXLogYScalarSelfGetWorkspaceSize(const aclScalar* self, const aclTensor* other,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnXLogYScalarSelf的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnXLogYScalarSelf(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_X_LOG_Y_SCALAR_SELF_H_// End content from: aclnn_x_log_y_scalar_self.h

// Begin content from: acl_stft.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACL_STFT_H_
#define OP_API_INC_LEVEL2_ACL_STFT_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclStft的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_fft
 * 算子功能：返回输入Tensor做Stft的结果
 * 计算公式：
 * $$ out = Stft(input) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A[(self)]--->B([l0op::Stft])
 *   B--->C([l0op::ViewCopy])
 *   C--->D[(out)]
 * ```
 *
 * @param [in] self: 待进行Stft计算的入参。npu device侧的aclTensor,
 * 数据类型支持FLOAT，DOUBLE，COMPLEX64，COMPLEX128，数据格式支持ND，不支持非连续Tensor
 * @param [in] out: abs计算的出参。npu device侧的aclTensor,
 * 数据类型支持FLOAT，DOUBLE，COMPLEX64，COMPLEX128，数据格式支持ND，数据格式支持ND
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
aclnnStatus aclStftGetWorkspaceSize(const aclTensor* self, const aclTensor* windowOptional, aclTensor* out, int64_t nFft,
                                    int64_t hopLength, int64_t winLength, bool normalized, bool onesided,
                                    bool returnComplex, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclStft的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAbsGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
aclnnStatus aclStft(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACL_STFT_H_// End content from: acl_stft.h

// Begin content from: aclnn_foreach_log1p.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LOG1P_H_
#define ACLNN_FOREACH_LOG1P_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLog1pGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog1pGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLog1p
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog1p(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_log1p.h

// Begin content from: aclnn_isclose.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ISCLOSE_H_
#define OP_API_INC_LEVEL2_ISCLOSE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIsClose的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：返回一个带有布尔元素的新张量，判断self和other在epsilon内是否相等。
 * 计算公式
 * $$ \left | input-other\right | = atol + rtol\times \left | other \right | $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT32、INT64、INT16、INT8、UINT8、BOOL、DOUBLE，
 * 支持非连续的Tensor，dtype与other的dtype必须一致，shape需要与other满足broadcast关系。数据格式支持ND。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT32、INT64、INT16、INT8、UINT8、BOOL、DOUBLE，
 * 支持非连续的Tensor，dtype与self的dtype必须一致，shape需要与self满足broadcast关系。数据格式支持ND。
 * @param [in] rtol: 绝对宽容。数据类型支持DOUBLE。
 * @param [in] atol: 相对公差。数据类型支持DOUBLE。
 * @param [in] equal_nan: NaN值比较选项。如果为True，则两个NaN将被视为相等。数据类型支持BOOL。
 * @param [out] out: npu
 * device侧的aclTensor，数据类型支持BOOL，支持非连续的Tensor，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsCloseGetWorkspaceSize(const aclTensor* self, const aclTensor* other, double rtol,
                                                   double atol, bool equal_nan, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnIsClose的第二段接口，用于执行计算。
 *
 * 算子功能：返回一个带有布尔元素的新张量，判断self和other在epsilon内是否相等。
 * 计算公式
 * $$ \left | input-other\right | = atol + rtol\times \left | other \right | $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnIsCloseGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsClose(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ISCLOSE_H_
// End content from: aclnn_isclose.h

// Begin content from: aclnn_foreach_minimum_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MINIMUM_LIST_H_
#define ACLNN_FOREACH_MINIMUM_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMinimumListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMinimumList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMinimumList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_minimum_list.h

// Begin content from: aclnn_moe_token_permute_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MOE_TOKEN_PERMUTE_GRAD_H_
#define ACLNN_MOE_TOKEN_PERMUTE_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMoeTokenPermuteGradGetWorkspaceSize
 * parameters :
 * permutedOutputGrad : required
 * sortedIndices : required
 * numTopk : required
 * paddedMode : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenPermuteGradGetWorkspaceSize(
    const aclTensor *permutedOutputGrad,
    const aclTensor *sortedIndices,
    int64_t numTopk,
    bool paddedMode,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMoeTokenPermuteGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMoeTokenPermuteGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_moe_token_permute_grad.h

// Begin content from: aclnn_replication_pad3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REPLICATION_PAD3D_H_
#define OP_API_INC_REPLICATION_PAD3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReplicationPad3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：3D边界填充。
 * @param [in] self: 数据类型支持FLOAT16, FLOAT32, DOUBLE, INT8, INT16, INT32, INT64, UINT8,
 * COMPLEX64, COMPLEX128，支持非连续的Tensor，数据格式支持ND，维度支持四维或五维，在最后三维做pad。
 * @param [in] padding: 数据类型为INT64，长度为6，数值依次代表左右上下前后需要填充的值。
 * @param [in] out: 数据类型、数据格式、维度与self一致，倒数第三维度的数值等于self倒数第三维度的
 * 数值加padding后两个值，倒数第二维度的数值等于self倒数第二维度的数值加padding中间两个值，最后一维度的数值等于
 * self最后一维度的数值加padding前两个值。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad3d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReplicationPad3dGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REPLICATION_PAD3D_H_// End content from: aclnn_replication_pad3d.h

// Begin content from: aclnn_swish.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SWISH_H_
#define OP_API_INC_LEVEL2_ACLNN_SWISH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSwish的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：Swish激活函数
 * @param [in] self: Device侧的aclTensor，公式中的input。支持非连续的Tensor，数据格式支持ND，self与out的shape和数据类型一致。
 * @param [in] betaOptional: Host侧的aclScalar，公式中的beta。数据类型需要是可转换为FLOAT的数据类型。
 * 当betaOptional为空指针时，默认值为1.0。
 * @param [out] out: Device侧的aclTensor，公式中的output。支持非连续的Tensor，数据格式支持ND，
 * self与out的shape和数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSwishGetWorkspaceSize(const aclTensor* self, const aclScalar* betaOptional, aclTensor* out, 
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSwish的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAcosGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSwish(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_swish.h

// Begin content from: aclnn_index_put_impl.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_INDEX_PUT_H_
#define OP_API_INC_INDEX_PUT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIndexPutImpl的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT、FLOAT16、INT64、INT32、INT16、INT8、UINT8、BOOL、DOUBLE，支持非连续的Tensor，数据格式支持ND。
 * @param [in] indices: npu device侧的aclTensorList，表示索引切片用。
 * @param [in] values: npu device侧的aclTensor，索引处更新的值。
 * @param [in] accumulate: bool 类型属性， True表示在索引处叠加value值, False表示在索引处替换value值。
 * @param [in] unsafe: bool 类型属性。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexPutImplGetWorkspaceSize(aclTensor* selfRef, const aclTensorList* indices,
                                                        const aclTensor* values, const bool accumulate,
                                                        const bool unsafe, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnIndexPutImpl的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aaclnnIndexPutImplGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIndexPutImpl(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_index_put_impl.h

// Begin content from: aclnn_foreach_round_off_number_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_ROUND_OFF_NUMBER_V2_H_
#define OP_API_INC_ACLNN_FOREACH_ROUND_OFF_NUMBER_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachRoundOffNumberV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入张量列表的每个张量进行四舍五入到指定的小数位数运算。
 * 计算公式：
 * out_i = round(x_i, roundMode)
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [in]   input
 * 输入Scalar，数据类型支持INT8。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachRoundOffNumberV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *roundMode,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachRoundOffNumberV2的第二段接口，用于执行计算。
 * 功能描述：对输入张量列表的每个张量进行四舍五入到指定的小数位数运算。
 * 计算公式：
 * out_i = round(x_i, roundMode)
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachRoundOffNumberV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachRoundOffNumberV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_round_off_number_v2.h

// Begin content from: aclnn_add_layer_norm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_ADD_LAYER_NORM_H_
#define ACLNN_ADD_LAYER_NORM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnAddLayerNormGetWorkspaceSize
 * parameters :
 * x1 : required
 * x2 : required
 * gamma : required
 * beta : required
 * biasOptional : optional
 * epsilon : optional
 * additionalOutput : optional
 * yOut : required
 * meanOut : required
 * rstdOut : required
 * xOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddLayerNormGetWorkspaceSize(
    const aclTensor *x1,
    const aclTensor *x2,
    const aclTensor *gamma,
    const aclTensor *beta,
    const aclTensor *biasOptional,
    double epsilon,
    bool additionalOutput,
    const aclTensor *yOut,
    const aclTensor *meanOut,
    const aclTensor *rstdOut,
    const aclTensor *xOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnAddLayerNorm
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnAddLayerNorm(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_add_layer_norm.h

// Begin content from: aclnn_incre_flash_attention_v3.h
/**
 * Copyright (c) 2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_INCRE_FLASH_ATTENTION_V3_H_
#define ACLNN_INCRE_FLASH_ATTENTION_V3_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIncreFlashAttentionV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * funtion: aclnnIncreFlashAttentionV3GetWorkspaceSize
 * @param [in] query : required
 * @param [in] key : dynamic
 * @param [in] value : dynamic
 * @param [in] pseShift : optional
 * @param [in] attenMask : optional
 * @param [in] actualSeqLengths : optional
 * @param [in] dequantScale1 : optional
 * @param [in] quantScale1 : optional
 * @param [in] dequantScale2 : optional
 * @param [in] quantScale2 : optional
 * @param [in] quantOffset2 : optional
 * @param [in] antiquantScale : optional
 * @param [in] antiquantOffset : optional
 * @param [in] blocktable : optional
 * @param [in] numHeads : required
 * @param [in] scaleValue : optional
 * @param [in] inputLayout : optional
 * @param [in] numKeyValueHeads : optional
 * @param [in] blockSize : optional
 * @param [in] innerPrecise : optional
 * @param [out] attentionOut : required
 * @param [out] workspaceSize : size of workspace(output).
 * @param [out] executor : executor context(output).
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV3GetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
    const aclTensor *attenMask, const aclIntArray *actualSeqLengths, const aclTensor *dequantScale1,
    const aclTensor *quantScale1, const aclTensor *dequantScale2, const aclTensor *quantScale2,
    const aclTensor *quantOffset2, const aclTensor *antiquantScale, const aclTensor *antiquantOffset,
    const aclTensor *blocktable, int64_t numHeads, double scaleValue, char *inputLayout, int64_t numKeyValueHeads,
    int64_t blockSize, int64_t innerPrecise, const aclTensor *attentionOut, uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * funtion: aclnnIncreFlashAttentionV3
 * @param [in] workspace : workspace memory addr(input).
 * @param [in] workspaceSize : size of workspace(input).
 * @param [in] executor : executor context(input).
 * @param [in] stream : acl stream.
 * @return aclnnStatus: 返回状态码
 */
__attribute__((visibility("default"))) aclnnStatus aclnnIncreFlashAttentionV3(void *workspace, uint64_t workspaceSize,
                                                                              aclOpExecutor *executor,
                                                                              const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_incre_flash_attention_v3.h

// Begin content from: aclnn_batch_matmul_reduce_scatter_all_to_all.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023-2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BATCH_MATMUL_REDUCE_SCATTER_ALL_TO_ALL_
#define OP_API_INC_BATCH_MATMUL_REDUCE_SCATTER_ALL_TO_ALL_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：实现bmm + reduceScatter + alltoall 融合计算
 * @brief aclnnBatchMatMulReduceScatterAlltoAll的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] x: 计算输入，Tensor，数据类型float16，bfloat16，必须为3维。BatchMatMul计算的左矩阵
 * @param [in] weight: 计算输入，Tensor，数据类型float16, bfloat16，必须为3维，类型与x保持一致。BatchMatMul计算的右矩阵
 * @param [in] biasOptional: 计算输入，Tensor，数据类型float16, float32。x为float16时，bias需为float16；x为bfloat16时，bias需为float32。支持两维或三维。BatchMatMul计算的bias。(由于要进行ReduceScatter通信，因此需要在通信之后再Add)
 * @param [in] groupEp: 计算输入，str。ep通信域名称，专家并行的通信域
 * @param [in] groupTp: 计算输入，str。tp通信域名称，Tensor并行的通信域
 * @param [in] epWorldSize: 计算输入，int。ep通信域size，支持2/4/8/16
 * @param [in] tpWorldSize: 计算输入，int。tp通信域size，支持2/4/8/16
 * @param [in] yShardType: 计算输入，int，默认值为0。0表示输出在H维度按tp分片，1表示输出在C维度按tp分片。当前仅支持shard_type等于1的场景
 * @param [out] out: 计算输出，Tensor，数据类型float16, bfloat16，必须为3维。最终计算结果，类型与输入x保持一致
 * @param [out] workspaceSize: 出参，返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 出参，返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回值，返回状态码
 * 因为集合通信及BatchMatMul计算所需，输入输出shape需满足以下数学关系：（其中ep=epWorldSize，tp=tpWorldSize） 按H轴进行ReduceScatter场景，即shard_type为0场景（暂不支持该场景）：
 * x: (E/ep, ep * C, M/tp)
 * weight：(E/ep, M/tp, H)
 * bias：(E/ep, 1, H/tp) 两维时为(E/ep, H/tp)
 * y：(E, C, H/tp)
 * 按C轴进行ReduceScatter场景，即shard_type为1场景：
 * x: (E/ep, ep * tp * C/tp, M/tp)
 * weight：(E/ep, M/tp, H)
 * bias：(E/ep, 1, H) 两维时为(E/ep, H)
 * y：(E, C/tp, H)
 * 数据关系说明：
 * 比如x.size(0)等于E/tp，y.size(0)等于E，则表示，y.size(0) = ep * x.size(0)，y.size(0)是ep的整数倍；其他关系类似
 * E的取值范围为[2, 512]，且E是ep的整数倍；
 * H的取值范围为：[1, 65535]；
 * M/tp的取值范围为：[1, 65535]；
 * E/ep的取值范围为：[1, 32]；
 * ep、tp均仅支持2、4、8、16；
 * groupEp和groupTp名称不能相同；
 * C大于0，上限为算子device内存上限；
 * 不支持跨超节点，只支持超节点内，ep域AlltoAll支持超节点内跨节点，tp域ReduceScatter仅支持超节点内单一节点；
 */
ACLNN_API aclnnStatus aclnnBatchMatMulReduceScatterAlltoAllGetWorkspaceSize(const aclTensor* x, const aclTensor* weight,
                                                                            const aclTensor* biasOptional,
                                                                            const char* groupEp, const char* groupTp,
                                                                            int64_t epWorldSize, int64_t tpWorldSize,
                                                                            int64_t yShardType, aclTensor* out,
                                                                            uint64_t* workspaceSize,
                                                                            aclOpExecutor** executor);

/**
 * @brief aclnnBatchMatMulReduceScatterAlltoAll的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnBatchMatMulReduceScatterAlltoAllGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnBatchMatMulReduceScatterAlltoAll(void* workspace, uint64_t workspaceSize,
                                                            aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BATCH_MATMUL_REDUCE_SCATTER_ALL_TO_ALL_// End content from: aclnn_batch_matmul_reduce_scatter_all_to_all.h

// Begin content from: aclnn_foreach_mul_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_MUL_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_MUL_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachMulScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入矩阵的每一个元素进行scalar相乘运算的输出。
 * 计算公式：
 * out_{i}=x_{i}*scalar
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   x
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [in]   scalar
 * 输入Scalar，数据类型支持FLOAT、FLOAT16和INT32。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachMulScalarV2GetWorkspaceSize(
    const aclTensorList *x,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachMulScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素进行scalar相乘运算的输出。
 * 计算公式：
 * out_{i}=x_{i}*scalar
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachMulScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachMulScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_mul_scalar_v2.h

// Begin content from: aclnn_upsample_nearest_exact1d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef ACLNN_UPSAMPLE_NEAREST_EXACT1D_BACKWARD_H_
#define ACLNN_UPSAMPLE_NEAREST_EXACT1D_BACKWARD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearesrExact1dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：aclnnUpsampleNearesrExact1d的反向传播。
 *
 * @param [in] gradOutput: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与出参out的数据类型一致。
 * @param [in] outputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为2。表示输入gradOutput在H和W维度上的空间大小。
 * @param [in] inputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为4。表示输出out分别在N、C、H和W维度上的空间大小。。
 * @param [in] scales: Host侧的浮点型，表示输出out的维度乘数。
 * @param [out] out: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与入参gradOutput的数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleNearestExact1dBackwardGetWorkspaceSize(
    const aclTensor *gradOutput, const aclIntArray *outputSize, const aclIntArray *inputSize,  
    double scales, aclTensor *out, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleNearestExact1dBackward的第二段接口，用于执行计算。
 * 
 * 算子功能：aclnnUpsampleNearestExact1d的反向传播。
 *
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小。
 * 由第一段接口aclnnUpsampleNearestExact1dBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL Stream流。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleNearestExact1dBackward(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_upsample_nearest_exact1d_backward.h

// Begin content from: aclnn_avgpool3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AVGPOOL3D_H_
#define OP_API_INC_AVGPOOL3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAvgPool3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：对输入self进行平均池化运算。
 *
 * @param [in] self: npu
 * device侧的aclTensor，支持空tensor场景，数据类型支持FLOAT16、BFLOAT16和FLOAT。支持4维或5维。支持非连续的Tensor。支持数据格式为ND。
 * @param [in] kernelSize: npu
 * device侧的aclIntArray，长度为1(kD=kH=kW)或3(kD,kH,kW)，表示池化窗口大小。数据类型支持INT32和INT64。数值必须大于0。
 * @param [in] stride: npu
 * device侧的aclIntArray，长度为0(默认为kernelSize)或1(sD=sH=sW)或3(sD,sH,sW)，表示池化操作的步长。
 * 数据类型支持INT32和INT64。数值必须大于0。
 * @param [in] padding: npu
 * device侧的aclIntArray，长度为1(padD=padH=padW)或3(padD,padH,padW)，表示在输入的D、H、W方向上padding补0的层数。
 * 数据类型支持INT32和INT64。数值在[0, kernelSize/2]的范围内。
 * @param [in] ceilMode: 数据类型支持BOOL。表示计算输出shape时，向下取整（False），否则向上取整。
 * @param [in] countIncludePad: 数据类型支持BOOL。表示平均计算中包括零填充（True），否则不包括。
 * @param [in] divisorOverride: 数据类型支持INT64。如果指定，它将用作平均计算中的除数，当值为0时，该属性不生效。
 * @param [out] output: npu
 * device侧的aclTensor，输出Tensor，数据类型支持FLOAT16、BFLOAT16和FLOAT。支持4维或5维。支持非连续的Tensor。支持数据格式为ND。
 * 数据类型、数据格式需要与self一致。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAvgPool3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                     const aclIntArray* stride, const aclIntArray* padding,
                                                     bool ceilMode, bool countIncludePad,
                                                     int64_t divisorOverride, aclTensor* out,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAvgPool3d的第二段接口，用于执行计算。
 *
 * 算子功能： 对输入self进行平均池化运算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAtan2GetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAvgPool3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AVGPOOL3D_H_
// End content from: aclnn_avgpool3d.h

// Begin content from: aclnn_cat.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CAT_H_
#define OP_API_INC_CAT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdaptiveAvgPool2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnCatGetWorkspaceSize(const aclTensorList* tensors, int64_t dim, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnCat的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnCat(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CAT_H_
// End content from: aclnn_cat.h

// Begin content from: aclnn_asin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ASIN_H_
#define OP_API_INC_ASIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAsin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=sin^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，BFLOAT16,
 * FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnAsinGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnAsin的第二段接口，用于执行计算。
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。。
 * 计算公式：
 * out_{i}=sin^{-1}(input_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Asin])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAsinGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAsin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAsin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=sin^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAsinGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAsin的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成asin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAsinGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAsin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_asin.h

// Begin content from: aclnn_fill_diagonal.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADD_H_
#define OP_API_INC_ADD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceFillDiagonal的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：以fillValue填充tensor对角线
 *
 * @param [in] selfRef: npu device侧的aclTensor，输入selfRef，数据类型支持FLOAT、FLOAT16、INT32、INT64。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持ND（[参考](#参考)）。
 * @param [in] fillValue:
 * host侧的aclScalar，输入fillValue，数据类型支持FLOAT、FLOAT16、DOUBLE、UINT8、INT8、INT16、INT32 INT64、BOOL。
 * @param [in] wrap: 输入wrap，数据类型支持BOOL，是否每经过`min(col, row)`行形成一条新的对角线，仅对二维输入Tensor有效。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillDiagonalGetWorkspaceSize(aclTensor* selfRef, const aclScalar* fillValue,
                                                               bool wrap, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);

/**
 * @brief aclnnInplaceFillDiagonal的第二段接口，用于执行计算。
 *
 * 算子功能：以fillValue填充tensor对角线
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceFillDiagonalGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillDiagonal(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_fill_diagonal.h

// Begin content from: aclnn_exp.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_EXP_H_
#define OP_API_INC_LEVEL2_ACLNN_EXP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：返回一个新的张量，该张量的每个元素都是输入张量对应元素的指数。
 * 计算公式：如下
 * $$
 *     out_{i} = e^{self_{i}}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B --> C([l0op::Exp])
 *     C --> D([l0op::Cast])
 *     D --> E([l0op::ViewCopy])
 *     E --> F[(out)]
 * ```
 */

/**
 * @brief aclnnExp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128、BOOL，
 * 支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型需要是self可转换的数据类型，shape需要与self一致，支持非连续的Tensor，数据
 * 格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExpGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnExp的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnExpGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceExp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、COMPLEX64、COMPLEX128，支持
 * 非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExpGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnInplaceExp的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceExpGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_EXP_H_
// End content from: aclnn_exp.h

// Begin content from: aclnn_multi_scale_deformable_attention_grad.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTENTION_GRAD_H_
#define OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTENTION_GRAD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMultiScaleDeformableAttentionGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * @param [in] value:
 * Device侧的aclTensor，特征图的特征值，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND
 * @param [in] spatialShape:
 * Device侧的aclTensor，存储每个尺度特征图的高和宽，数据类型支持INT32、INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] levelStartIndex:
 * Device侧的aclTensor，每张特征图的起始索引，数据类型支持INT32、INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] location:
 * Device侧的aclTensor，存储采样点的位置，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] attnWeight:
 * Device侧的aclTensor，存储每个采样点的权重，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gradOutput:
 * Device侧的aclTensor，反向算子的初始梯度，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gradValue:
 * Device侧的aclTensor，输入value对应的梯度，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gradLocation:
 * Device侧的aclTensor，输入location对应的梯度，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] gradAttnWeight:
 * Device侧的aclTensor，输入attnWeight对应的梯度，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMultiScaleDeformableAttentionGradGetWorkspaceSize(
    const aclTensor* value, const aclTensor* spatialShape, const aclTensor* levelStartIndex, const aclTensor* location,
    const aclTensor* attnWeight, const aclTensor* gradOutput, aclTensor* gradValue, aclTensor* gradLocation,
    aclTensor* gradAttnWeight, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMultiScaleDeformableAttentionGrad的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceMishGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMultiScaleDeformableAttentionGrad(void* workspace, uint64_t workspaceSize,
                                                             aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTENTION_GRAD_H_
// End content from: aclnn_multi_scale_deformable_attention_grad.h

// Begin content from: aclnn_upsample_trilinear_3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UPSAMPLE_TRILINEAR_3D_H_
#define OP_API_INC_UPSAMPLE_TRILINEAR_3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleTrilinear3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleTrilinear3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                               bool alignCorners, double scalesD, double scalesH,
                                                               double scalesW, aclTensor* out, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleTrilinear3d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleTrilinear3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UPSAMPLE_TRILINEAR_3D_H_// End content from: aclnn_upsample_trilinear_3d.h

// Begin content from: aclnn_masked_select.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MASKED_SELECT_H_
#define OP_API_INC_MASKED_SELECT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaskedSelect的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：根据一个布尔掩码张量（mask）中的值选择输入张量（self）中的元素作为输出,形成一个新的一维张量。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL
 * shape需要与mask满足[broadcast关系]()。支持[非连续的Tensor]()，数据格式支持ND.
 * @param [in] mask: npu
 * device侧的aclTensor，仅支持bool或uint8，如果为uint8，其值必须为0或1，shape需要与self满足[broadcast关系]()。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaskedSelectGetWorkspaceSize(const aclTensor* self, const aclTensor* mask, aclTensor* out,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnMaskedSelect的第二段接口，用于执行计算。
 *
 * 算子功能：对输入的 Tensor self， 根据mask进行按位掩码的选择操作，选择mask掩码中非零位置对应的元素，形成一个新的
 * Tensor，并返回。 实现说明： api计算的基本路径：
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMaskedSelect(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MASKED_SELECT_H_
// End content from: aclnn_masked_select.h

// Begin content from: aclnn_inplace_weight_quant_matmul_all_reduce_add_rms_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_inplace_weight_quant_matmul_all_reduce_add_rms_norm.h
 * \brief
 */
#ifndef OP_API_INC_INPLACE_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_
#define OP_API_INC_INPLACE_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceWeightQuantMatmulAllReduceAddRmsNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现MatmulAllReduce+AddRmsNorm融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bfloat16。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8,int4。
 * @param [in] bias: 偏置，数据类型支持：float16, bfloat16。
 * @param [in] antiquantScale: 对x2进行伪量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] antiquantOffset: 对x2进行伪量化计算参数，数据类型支持：float16, bfloat16。
 * @param [in] residual: 残差，数据类型支持：float16, bf16。输出MatmulAllReduce+Add(residual)的结果。
 * @param [in] gamma: RmsNorm归一化参数，数据类型支持：float16, bfloat16。
 * @param [in] epsilon: 防止除0错误，数据类型支持：double。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [in] antiquantGroupSize: per_group模式的groupSize输入。
 * @param [out] normOut: MatmulAllReduce+AddRmsNorm的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */

ACLNN_API aclnnStatus aclnnInplaceWeightQuantMatmulAllReduceAddRmsNormGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* antiquantScale,
    const aclTensor* antiquantOffset, const aclTensor* residual, const aclTensor* gamma, double epsilon,
    const char* group, const char* reduceOp, int64_t commTurn, int64_t streamMode, int64_t antiquantGroupSize,
    const aclTensor* normOut, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceWeightQuantMatmulAllReduceAddRmsNorm的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，
 *                             由第一段接口aclnnInplaceWeightQuantMatmulAllReduceAddRmsNormGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceWeightQuantMatmulAllReduceAddRmsNorm(void* workspace, uint64_t workspaceSize,
                                                                       aclOpExecutor* executor,
                                                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_INPLACE_WEIGHT_QUANT_MATMUL_ALL_REDUCE_ADD_RMS_NORM_// End content from: aclnn_inplace_weight_quant_matmul_all_reduce_add_rms_norm.h

// Begin content from: aclnn_foreach_add_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADD_SCALAR_LIST_H_
#define ACLNN_FOREACH_ADD_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_add_scalar_list.h

// Begin content from: aclnn_neg.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NEG_H_
#define OP_API_INC_NEG_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNeg的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对输入的每个元素完成相反数计算
 * 计算公式：
 * $$ output​=(-1) * self $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *      A[(self)] -->B([Contiguous])
 *      B -->D([Neg])
 *      D-->E([Cast])
 *      E-->F([ViewCopy])
 *      F-->G[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNegGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnInplaceNeg的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：对输入的每个元素完成相反数计算
 * 计算公式：
 * $$ selfRef ​=(-1) * selfRef $$
 *
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持整型，浮点类型，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNegGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnNeg的第二段接口，用于执行计算。
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = self_i+alpha*other_i $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 *  graph LR
 *      A[(self)] -->B([Contiguous])
 *      B -->D([Neg])
 *      D-->E([Cast])
 *      E-->F([ViewCopy])
 *      F-->G[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNegGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnNeg(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

/**
 * @brief aclnnInplaceNeg的第二段接口，用于执行计算。
 *
 * 算子功能：对输入的每个元素完成相反数计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceNegGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceNeg(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif /* OP_API_INC_NEG_H_ */// End content from: aclnn_neg.h

// Begin content from: aclnn_s_where.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SWHERE_H_
#define OP_API_INC_SWHERE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSWhere的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnSWhereGetWorkspaceSize(const aclTensor* condition, const aclTensor* self,
                                                  const aclTensor* other, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnSWhere的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSWhere(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SWHERE_H_
// End content from: aclnn_s_where.h

// Begin content from: aclnn_cos.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_COS_H_
#define OP_API_INC_COS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCos的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入矩阵的每一个元素求余弦后输出。
 * 计算公式：
 * out_{i}=cos(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE, COMPLEX64, COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE, COMPLEX64, COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnCosGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);
/**
 * @brief aclnnCos的第二段接口，用于执行计算。
 * 功能描述：对输入矩阵的每一个元素求余弦后输出。。
 * 计算公式：
 * out_{i}=cos(input_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Cos])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCos(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

/**
 * @brief aclnnInplaceCos的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=tan^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceCosGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceCos的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成cos操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCosGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceCos(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_cos.h

// Begin content from: aclnn_foreach_addcdiv_scalar_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACLNN_FOREACH_ADDCDIV_SCALAR_V2_H_
#define OP_API_INC_ACLNN_FOREACH_ADDCDIV_SCALAR_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnForeachAddcdivScalarV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对输入多个张量进行逐元素加、乘、除操作。
 * 计算公式：
 * out_i = {x}_{1i}+ \frac{{x}_{2i}}{{x}_{3i}}\times{scalar}
 * @domain aclnnop_math
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。数据格式支持ND。
  * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
  * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。数据格式支持ND。
  * @param [in]  input
 * 输入Scalar，数据类型支持FLOAT、FLOAT16。数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、INT32。数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnForeachAddcdivScalarV2GetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclScalar *scalar,
    aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnForeachAddcdivScalarV2的第二段接口，用于执行计算。
 * 功能描述：对输入多个张量进行逐元素加、乘、除操作。
 * 计算公式：
 * out_i = {x}_{1i}+ \frac{{x}_{2i}}{{x}_{3i}}\times{scalar}
 * @domain aclnnop_math
 * 参数描述：
 * param [in] workspace: 在npu device侧申请的workspace内存起址。
 * param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnForeachAddcdivScalarV2GetWorkspaceSize获取。
 * param [in] stream: acl stream流。
 * param [in] executor: op执行器，包含了算子计算流程。
 * return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnForeachAddcdivScalarV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcdiv_scalar_v2.h

// Begin content from: aclnn_gemm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GEMM_H_
#define OP_API_INC_GEMM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGemm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnGemmGetWorkspaceSize(const aclTensor* A, const aclTensor* B, const aclTensor* C, float alpha,
                                                float beta, int64_t transA, int64_t transB, aclTensor* out,
                                                int8_t cubeMathType, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGemm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGemm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GEMM_H_
// End content from: aclnn_gemm.h

// Begin content from: aclnn_reflection_pad3d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REFLECTION_PAD3D_H_
#define OP_API_INC_REFLECTION_PAD3D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad3d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：3D反射填充。
 * @param [in] self: 数据类型支持BFLOAT16,FLOAT16, FLOAT32, DOUBLE, INT8, INT16, INT32, INT64, UINT8, BOOL。
 * 支持非连续的Tensor，数据格式支持ND，维度支持四维或五维，在最后三维做pad。
 * @param [in] padding:
 * 数据类型为INT64，长度为6，数值依次代表左右上下前后需要填充的值。前两个数值需小于self最后一维度的数值，
 * 中间两个数值需小于self倒数第二维度的数值，后两个数值需小于self倒数第三维度的数值。
 * @param [in] out:
 * 数据类型、数据格式、维度与self一致，out倒数第三维度的数值等于self倒数第三维度的数值加padding后两个值，
 * out倒数第二维度的数值等于self倒数第二维度的数值加padding中间两个值，out最后一维度的数值等于self最后一维度的数值加padding
 * 前两个值。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad3dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad3d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界的反射填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReflectionPad3dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad3d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REFLECTION_PAD3D_H_// End content from: aclnn_reflection_pad3d.h

// Begin content from: aclnn_matmul_all_reduce_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MATMUL_ALL_REDUCE_V2_
#define OP_API_INC_MATMUL_ALL_REDUCE_V2_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMatmulAllReduceV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bf16。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。
 * @param [in] x3: add计算参数，数据类型支持：float16, bf16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：0/1。
 * @param [out] output: 计算+通信的结果，数据类型：同输入。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduceV2GetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                             const aclTensor* bias, const aclTensor* x3,
                                                             const char* group, const char* reduceOp, int64_t commTurn,
                                                             int64_t streamMode, const aclTensor* output,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMatmulAllReduceV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnMatmulAllReduceV2GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMatmulAllReduceV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MATMUL_ALL_REDUCE_V2_// End content from: aclnn_matmul_all_reduce_v2.h

// Begin content from: aclnn_mse_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MSE_LOSS_BACKWARD_H_
#define OP_API_INC_MSE_LOSS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMseLossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：均方误差函数的反向传播。
 *
 * @param [in] gradOutput：npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，数据类型需要与self相同，shape需要与self、target
 * 满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] self：npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，shape需要与gradOutput、target
 * 满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] target：npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，数据类型需要与self相同，shape需要与gradOutput、self
 * 满足broadcast关系。支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction：host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。'none'
 * 表示不应用减少， 'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] out：npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，shape需要是target与self、gradOutput
 * broadcast之后的shape。 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLossBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                           const aclTensor* target, int64_t reduction, aclTensor* out,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMseLossBackward的第二段接口，用于执行计算。
 *
 * 算子功能：均方误差函数的反向传播。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnMseLossBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMseLossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MSE_LOSS_BACKWARD_H_
// End content from: aclnn_mse_loss_backward.h

// Begin content from: aclnn_expand.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_EXPAND_H_
#define OP_API_INC_EXPAND_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnExpand的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：将输入张量self广播成指定size的张量。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous]) --> C([l0op::Expand])
 *     D((size)) --> E([sizeTensor]) --> C
 *     C --> F([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT、UINT8、INT8、INT32、INT64、BOOL，支持非连续的Tensor，数据格式支持ND。
 * @param [in] size: aclIntArray类型，数据类型支持INT。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT、UINT8、INT8、INT32、INT64、BOOL，需要和self保持一致，
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)），shape需要满足self的shap根据size的推导结果。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExpandGetWorkspaceSize(const aclTensor* self, const aclIntArray* size, aclTensor* out,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnExpand的第二段接口，用于执行计算。
 *
 * 算子功能：将输入张量self广播成指定size的张量。
 *
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous]) --> C([l0op::Expand])
 *     D((size)) --> E([sizeTensor]) --> C
 *     C --> F([l0op::ViewCopy]) --> Out[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnExpandGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExpand(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EXPAND_H_// End content from: aclnn_expand.h

// Begin content from: aclnn_bidirection_lstmv2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_BIDIRECTION_LSTMV2_H_
#define ACLNN_BIDIRECTION_LSTMV2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnBidirectionLSTMV2GetWorkspaceSize
 * parameters :
 * x : required
 * initH : required
 * initC : required
 * wIh : required
 * wHh : required
 * bIhOptional : optional
 * bHhOptional : optional
 * wIhReverseOptional : optional
 * wHhReverseOptional : optional
 * bIhReverseOptional : optional
 * bHhReverseOptional : optional
 * batchSizeOptional : optional
 * numLayers : optional
 * isbias : optional
 * batchFirst : optional
 * bidirection : optional
 * packed : optional
 * yOut : required
 * outputHOut : required
 * outputCOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBidirectionLSTMV2GetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *initH,
    const aclTensor *initC,
    const aclTensor *wIh,
    const aclTensor *wHh,
    const aclTensor *bIhOptional,
    const aclTensor *bHhOptional,
    const aclTensor *wIhReverseOptional,
    const aclTensor *wHhReverseOptional,
    const aclTensor *bIhReverseOptional,
    const aclTensor *bHhReverseOptional,
    const aclTensor *batchSizeOptional,
    int64_t numLayers,
    bool isbias,
    bool batchFirst,
    bool bidirection,
    bool packed,
    const aclTensor *yOut,
    const aclTensor *outputHOut,
    const aclTensor *outputCOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnBidirectionLSTMV2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBidirectionLSTMV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_bidirection_lstmv2.h

// Begin content from: aclnn_foreach_round_off_number.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ROUND_OFF_NUMBER_H_
#define ACLNN_FOREACH_ROUND_OFF_NUMBER_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachRoundOffNumberGetWorkspaceSize
 * parameters :
 * x : dynamic
 * roundMode : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachRoundOffNumberGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *roundMode,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachRoundOffNumber
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachRoundOffNumber(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_round_off_number.h

// Begin content from: aclnn_embedding_renorm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_EMBEDDING_RENORM_H_
#define OP_API_INC_EMBEDDING_RENORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEmbeddingRenorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：根据给定的max_norm和norm_type返回tensor在指定indices下的修正结果
 *
 * 计算公式：
 * 向量的范数计算公式如下，其中n为norm_type指定的范数值：
 * $$ ||X||_{p}=\sqrt[p]{\sum_{i=1}^nx_{i}^p} $$
 * $$ 其中X = (x_{1},x_{2}, ... , x_{n}) $$
 * 针对计算出的范数大于给定max_norm的场景，需要做归一化处理，对indices指定的0维元素乘以系数：
 * $$ scaler = \frac{max\_norm}{current\_norm + 1e^{-7}} $$
 *
 * 实现说明
 * api计算基本路径：
 *```mermaid
 * graph LR
 * A[(selfRef)]  --> B([l0op::contiguous])
 * B --> C([l0op::GatherV2])
 * D((axis=0)) --> C
 * E[(Indices)] --> F([l0op::contiguous])
 * F --> R([l0op::Reshape])
 * R --> C
 * C --> G([l0op::Renorm])
 * H((maxNorm)) --> G
 * I((normType)) --> G
 * J((dim=0)) --> G
 * U[(indicesGatherV2)] --> V([l0op::Contiguous])
 * V --> K([l0op::GatherV2])
 * L((axis=0)) --> K
 * G --> K
 * C --> M([l0op::Mul])
 * K --> M
 * B --> N([l0op::ScatterUpdate])
 * M --> N
 * R --> N
 * N --> O([l0op::ViewCopy])
 * O --> P[(selfRef)]
 * ```
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持BFLOAT16、FLOAT16、FLOAT32类型。
 * 支持非连续的Tensor，数据格式支持ND，dimension必须为2。
 * @param [in] indices: npu
 * device侧的aclTensor，数据类型支持INT64。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] maxNorm: 数据类型支持double、float。
 * @param [in] normType: 数据类型支持double、float。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingRenormGetWorkspaceSize(aclTensor* selfRef, const aclTensor* indices, double maxNorm,
                                                           double normType, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnEmbeddingRenorm的第二段接口，用于执行计算
 *
 * 算子功能：根据给定的max_norm和norm_type返回tensor在指定indices下的修正结果
 *
 * 计算公式：
 * 向量的范数计算公式如下，其中n为norm_type指定的范数值：
 * $$ ||X||_{p}=\sqrt[p]{\sum_{i=1}^nx_{i}^p} $$
 * $$ 其中X = (x_{1},x_{2}, ... , x_{n}) $$
 * 针对计算出的范数大于给定max_norm的场景，需要做归一化处理，对indices指定的0维元素乘以系数：
 * $$ scaler = \frac{max\_norm}{current\_norm + 1e^{-7}} $$
 *
 * 实现说明
 * api计算基本路径：
 *```mermaid
 * graph LR
 * A[(selfRef)]  --> B([l0op::contiguous])
 * B --> C([l0op::GatherV2])
 * D[(axis=0)] --> C
 * E[(Indices)] --> F([l0op::contiguous])
 * F --> C
 * C --> G([l0op::Renorm])
 * H((maxNorm)) --> G
 * I((normType)) --> G
 * J((dim=0)) --> G
 * F --> K([l0op::GatherV2])
 * L[(axis=0)] --> K
 * G --> K
 * C --> M([l0op::Mul])
 * K --> M
 * B --> N([l0op::ScatterUpdate])
 * M --> N
 * F --> N
 * N --> O([l0op::ViewCopy])
 * O --> P[(selfRef)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEmbeddingRenorm(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                           const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EMBEDDING_RENORM_H_
// End content from: aclnn_embedding_renorm.h

// Begin content from: aclnn_slogdet.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SLOGDET_H_
#define OP_API_INC_SLOGDET_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSlogdet的第一段接口，根据具体的计算流程，计算workspace大小.
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor计算行列式的符号和自然对数
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)]-->B([Contiguous])-->C([LogMatrixDeterminant])
 * D1([Cast])-->E1([ViewCopy])-->F1([signOut])
 * D2([Cast])-->E2([ViewCopy])-->F2([logOut])
 * ```
 *
 * @param [in] self: npu device侧的aclTensor, 数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128. shape满足(*, n, n)形式,
 * 其中`*`表示0或更多维度的batch. 支持非连续的Tensor. 数据格式支持ND.
 * @param [in] signOut: npu device侧的aclTensor, 数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128.
 * shape和self的batch一致. 支持非连续的Tensor. 数据格式支持ND.
 * @param [in] logOut: npu device侧的aclTensor, 数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128.
 * shape和self的batch一致. 支持非连续的Tensor. 数据格式支持ND.
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小.
 * @param [out] executor: 返回op执行器，包含算子计算流程.
 * @return aclnnStatus: 返回状态码， 成功返回ACLNN_SUCCESS, 失败返回对应错误码.
 */
ACLNN_API aclnnStatus aclnnSlogdetGetWorkspaceSize(const aclTensor* self, aclTensor* signOut, aclTensor* logOut,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSlogdet的第二段接口，用于执行计算.
 *
 * 算子功能：对输入的每个元素完成相反数计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址.
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSlogdetGetWorkspaceSize获取.
 * @param [in] executor: op执行器，包含了算子计算流程.
 * @param [in] stream: acl stream流.
 * @return aclnnStatus: 返回状态码.
 */
ACLNN_API aclnnStatus aclnnSlogdet(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SLOGDET_H_
// End content from: aclnn_slogdet.h

// Begin content from: aclnn_gelu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GELU_H_
#define OP_API_INC_LEVEL2_ACLNN_GELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：高斯误差线性单元激活函数
 * 计算公式：
 * $$ out_{i}=Gelu(self_{i})=self_{i}×Φ(self_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(Self)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::Gelu])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(out)]
 * ```
 *
 * @param [in] self: 待进行gelu计算的入参。npu device侧的aclTensor。
 * 数据类型支持FLOAT16、FLOAT32、BFLOAT16，且数据类型必须和out一样，数据格式支持ND，shape必须和out一样，支持非连续的Tensor。
 * @param [in] out: gelu计算的出参。
 * npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16，且数据类型必须和self一样，数据格式支持ND，shape必须和self一样，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeluGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnGelu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GELU_H_// End content from: aclnn_gelu.h

// Begin content from: aclnn_quant_matmul_v4.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_QUANT_MATMUL_V4
#define OP_API_INC_QUANT_MATMUL_V4

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulV4的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：相对于aclnnQuantBatchMatmulV3, 新增了pertokenScaleOptional量化参数
 * @param [in] x1: matmul左矩阵，数据类型支持：int8, int4, int32。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8, int4, int32。
 * @param [in] scale: 量化参数，数据类型支持：uint64_t, float32, bfloat16, int64_t。
 * @param [in] offset: 量化参数，数据类型支持：float32。
 * @param [in] pertokenScaleOptional: 量化参数，数据类型支持：float32。
 * @param [in] bias: 偏置，数据类型支持：int32_t, bfloat16, float16, float32。
 * @param [in] transposeX1: a矩阵是否转置，默认值：false。
 * @param [in] transposeX2: b矩阵是否转置，默认值：false。
 * @param [out] out: 计算结果，数据类型：half, int8, bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulV4GetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                         const aclTensor* scale, const aclTensor* offset,
                                                         const aclTensor* pertokenScaleOptional, const aclTensor* bias,
                                                         bool transposeX1, bool transposeX2, const aclTensor* out,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmulV4的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnQuantMatmulV4GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulV4(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_V4// End content from: aclnn_quant_matmul_v4.h

// Begin content from: aclnn_sin.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SIN_H_
#define OP_API_INC_SIN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、FLOAT16、
 *  DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128, shape与self
 *  保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSinGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief: aclnnSin的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceSin的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、
 *  FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceSinGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                      aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceSin的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSin(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SIN_H_// End content from: aclnn_sin.h

// Begin content from: aclnn_acos.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ACOS_H_
#define OP_API_INC_ACOS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAcos的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=cos^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，BFLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnAcosGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnAcos的第二段接口，用于执行计算。
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。。
 * 计算公式：
 * out_{i}=cos^{-1}(input_{i})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Acos])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAcos(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceAcos的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：从输入矩阵的每一个元素进行反余弦操作后输出。
 * 计算公式：
 * out_{i}=cos^{-1}(input_{i})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAcosGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAcos的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成acos操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAcosGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAcos(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_acos.h

// Begin content from: aclnn_prelu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PRELU_H_
#define OP_API_INC_PRELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPrelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnPreluGetWorkspaceSize(const aclTensor* self, const aclTensor* weight, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPrelu的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnPrelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_prelu.h

// Begin content from: aclnn_ascend_quant_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_V3_H_
#define OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_V3_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAscendQuantV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 待进行AscendQuantV3计算的入参。npu device侧的aclTensor，
 * 数据类型支持float16, bfloat16, float32, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] scale: npu device侧的aclTensor, 数据类型支持float, bf16, float16
 * @param [in] offset: npu device侧的aclTensor，数据类型支持float, bf16, float16
 * @param [in] sqrtMode:  host侧的aclScalar，数据类型bool
 * @param [in] roundMode:  host侧的aclScalar，数据类型string
 * @param [in] dstType:  host侧的aclScalar, 数据类型int
 * @param [in] axis:  host侧的aclScalar, 数据类型int
 * @param [in] y: AscendQuantV3计算的出参。npu device侧的aclTensor，
 * 数据类型支持int8, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendQuantV3GetWorkspaceSize(const aclTensor* x, const aclTensor* scale,
                                                         const aclTensor* offset, bool sqrtMode, const char* roundMode,
                                                         int32_t dstType, int32_t axis, const aclTensor* y,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAscendQuantV3的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAscendAntiQuantGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendQuantV3(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_V3_H_// End content from: aclnn_ascend_quant_v3.h

// Begin content from: aclnn_round.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ROUND_H_
#define OP_API_INC_ROUND_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRound的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入的值舍入到最接近的整数，若该值与两个整数距离一样则向偶数取整。
 * @param [in] self: npu device侧的aclTensor，数据类型支持BFLOAT16,FLOAT16, FLOAT32, DOUBLE, INT32, INT64，
 * 数据格式支持ND，shape不支持9D及以上。
 * @param [in] out: npu device侧的aclTensor，数据类型、shape与self一致，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRoundGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief: aclnnRound的第二段接口，用于执行计算
 *
 * 算子功能：将输入的值舍入到最接近的整数，若该值与两个整数距离一样则向偶数取整。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRoundGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRound(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnInplaceRound的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：将输入的值舍入到最接近的整数，若该值与两个整数距离一样则向偶数取整。
 * @param [in] selfRef: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT32, DOUBLE, INT32, INT64，
 * 数据格式支持ND，shape不支持9D及以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRoundGetWorkspaceSize(const aclTensor* selfRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceRound的第二段接口，用于执行计算
 *
 * 算子功能：将输入的值舍入到最接近的整数，若该值与两个整数距离一样则向偶数取整。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSinGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRound(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

/**
 * @brief aclnnRoundDecimals的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：将输入Tensor的元素四舍五入到指定的位数。
 * 参数描述：
 * @param [in]   self
 * 输入Tensor，数据类型支持FLOAT，BFLOAT16,FLOAT16，DOUBLE，INT32，INT64。支持非连续Tensor，数据格式支持ND。
 * @param [in]   decimals: 需要四舍五入到的位数，数据类型支持INT。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE，INT32，INT64。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnRoundDecimalsGetWorkspaceSize(const aclTensor* self, int64_t decimals, aclTensor* out,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnRoundDecimals的第二段接口，用于执行计算。
 * 功能描述：将输入Tensor的元素四舍五入到指定的位数。
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> D([l0op::Round])

    C((decimals)) --> D([l0op::Cast])
    D --> E([l0op::ViewCopy])
    E --> F[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRoundDecimalsGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRoundDecimals(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

/**
 * @brief aclnnInplaceRoundDecimals的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：将输入Tensor的元素四舍五入到指定的位数。
 * 参数描述：
 * @param [in]   selfRef
 * 输入Tensor，数据类型支持FLOAT，FLOAT16，DOUBLE，INT32，INT64。支持非连续Tensor，数据格式支持ND。
 * @param [in]   decimals: 需要四舍五入到的位数，数据类型支持INT。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRoundDecimalsGetWorkspaceSize(aclTensor* selfRef, int64_t decimals,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceRoundDecimals的第二段接口，用于执行计算
 * 算子功能： 将输入Tensor的元素四舍五入到指定的位数。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceRoundDecimalsGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRoundDecimals(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ROUND_H_// End content from: aclnn_round.h

// Begin content from: aclnn_minn.h
/*
 * Copyright (c) 2023 Huawei Technologies Co., Ltd.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MINN_H_
#define OP_API_INC_MINN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMinN的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入tensors列表中每个tensor对应元素求min。
 *
 * @param [in] tensors: npu device侧的aclTensorList，数据类型支持整型，浮点类型，shape需要与out满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor，数据类型支持整型，浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinNGetWorkspaceSize(const aclTensorList* tensors, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnMinN的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入tensors列表中每个tensor对应元素求min。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnSumGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMinN(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MINN_H_
// End content from: aclnn_minn.h

// Begin content from: aclnn_multi_scale_deformable_attn_function.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTN_FUNCTION_H_
#define OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTN_FUNCTION_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMultiScaleDeformableAttnFunction的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] value:
 * Device侧的aclTensor，特征图的特征值，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND
 * @param [in] spatialShape:
 * Device侧的aclTensor，存储每个尺度特征图的高和宽，数据类型支持INT32、INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] levelStartIndex:
 * Device侧的aclTensor，每张特征图的起始索引，数据类型支持INT32、INT64，支持非连续的Tensor，数据格式支持ND。
 * @param [in] location:
 * Device侧的aclTensor，存储采样点的位置，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] attnWeight:
 * Device侧的aclTensor，存储每个采样点的权重，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] output:
 * Device侧的aclTensor，MultiScaleDeformableAttnFunction的输出，数据类型支持FLOAT、FLOAT16、BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMultiScaleDeformableAttnFunctionGetWorkspaceSize(
    const aclTensor* value, const aclTensor* spatialShape, const aclTensor* levelStartIndex, const aclTensor* location,
    const aclTensor* attnWeight, aclTensor* output, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMultiScaleDeformableAttnFunction的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceMishGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMultiScaleDeformableAttnFunction(void* workspace, uint64_t workspaceSize,
                                                             aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MULTI_SCALE_DEFORMABLE_ATTN_FUNCTION_H_
// End content from: aclnn_multi_scale_deformable_attn_function.h

// Begin content from: aclnn_flip.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_FLIP_H_
#define OP_API_INC_LEVEL2_ACLNN_FLIP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFlip的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 对n维张量的指定维度进行反转（倒序），dims中指定的每个轴的计算公式：
 *
 * $$
 * \operatorname{out}(i_0, i_1,
 * \ldots, i_{n-1}) = \operatorname{input}(j_0, j_1, \ldots, j_{n-1})
 * $$
 *
 * 其中，$n$是输入张量的维度，$ j_k$ = $\operatorname{dimSize}(k)$ -1 - $i_k$，$\operatorname{dimSize}(k)$
 * 表示第$k$个轴的长度。
 *
 * 计算图：
 * ```mermaid
 * graph LR
 * 	A[(Self)] -->B([l0op::Contiguous])
 *     B -->D([l0op::Flip])
 *     G((dims)) --> D([l0op::ReverseV2])
 *     D -->R([l0op::ViewCopy])
 *     R --> J[(out)]
 * ```
 * @param [in] self: 待进行flip计算的入参。npu device侧的aclTensor，
 * 数据类型支持float16,float,int32,int16,int64,bool, int8, uint8,float64，complex64, complex128, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] dims: 表示需要翻转的轴。
 * @param [in] out: flip计算的出参。npu device侧的aclTensor，
 * 数据类型支持float16,float,int32,int16,int64,bool, int8, uint8,float64, complex64, complex128，数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFlipGetWorkspaceSize(const aclTensor* self, const aclIntArray* dims, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFlip的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnFlipGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFlip(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FLIP_H_// End content from: aclnn_flip.h

// Begin content from: aclnn_expm1.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_EXPM1_H_
#define OP_API_INC_EXPM1_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnExpm1的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：以输入的self为指数，计算自然常数e的幂，并对指数计算结果进行减1计算。对于self取值较小的场景，提供比直接用公式计算结果更高的精度。
 * 计算公式：$$ out_i = {e}^{self_i} - 1 $$
 * 实现说明-计算图
 *
 * ```mermaid
 * graph LR
 *  A[(self)] -->B([l0op::Contiguous])
 *  B --> D([l0op::Expm1])
 *  D --> H([l0op::Cast])
 *  H --> I([l0op::ViewCopy])
 *  I --> J[(out)]
 * ```
 *
 * @param [in] self：公式中输入`self`,数据类型支持INT64、BOOL、FLOAT、BFLOAT16、FLOAT16，shape需要与out一致。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）
 * @param [out] out：公式中输入`out`,数据类型支持FLOAT、BFLOAT16、FLOAT16，shape需要与self一致。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExpm1GetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnExpm1的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnExpm1GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnExpm1(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnInplaceExpm1的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：以输入的self为指数，计算自然常数e的幂，并对指数计算结果进行减1计算。对于self取值较小的场景，提供比直接用公式计算结果更高的精度。
 * 计算公式：$$ out_i = {e}^{self_i} - 1 $$
 * 实现说明-计算图
 *
 * ```mermaid
 * graph LR
 *  A[(selfRef)] -->B([l0op::Contiguous])
 *  B --> D([l0op::Expm1])
 *  D --> H([l0op::Cast])
 *  H --> I([l0op::ViewCopy])
 *  I --> J[(selfRef)]
 * ```
 *
 * @param [in] selfRef：公式中输入`selfRef`,数据类型支持FLOAT、FLOAT16。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExpm1GetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief aclnnInplaceExpm1的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceExpm1GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceExpm1(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                        const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EXPM1_H_
// End content from: aclnn_expm1.h

// Begin content from: aclnn_linalg_vector_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LINALG_VECTOR_NORM_H_
#define OP_API_INC_LINALG_VECTOR_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLinalgVectorNorm的第一段接口， 根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_infer
 * @param [in] self: npu device侧的aclTensor, 数据类型支持浮点数据类型
 * @param [in] ord: host侧aclScalar, 数据类型支持浮点数据类型
 * @param [in] dims: npu device侧的aclIntArray, 代表对应的降维轴Axis的信息
 * @param [in] keepDims: host侧的bool, 代表是否保留对应dim维度的信息
 * @param [in] dtype: 指定self计算时的数据类，在计算前将self转换成dtype指定类型进行计算，数据类型支持FLOAT、FLOAT16。
 * @param [in] out: 输出tensor
 * @param [in] workspaceSize: 返回用户需要的npu device侧申请的workspace大小
 * @param [in] executor: 返回op执行器，包含算子计算流程
 * @return aclnnStatus: 返回状态码
 * */
ACLNN_API aclnnStatus aclnnLinalgVectorNormGetWorkspaceSize(const aclTensor* self, const aclScalar* ord,
                                                            const aclIntArray* dims, bool keepDims,
                                                            const aclDataType dtype, aclTensor* out,
                                                            uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * aclnnLinalgVectorNorm的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存地址
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，
 * 由第一段接口aclnnLinalgVectorNormGetWorkspaceSize获取
 * @param [in] executor: 返回op执行器，包含算子计算流程
 * @param [in] stream: acl stream流
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnLinalgVectorNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LINALG_VECTOR_NORM_H_// End content from: aclnn_linalg_vector_norm.h

// Begin content from: aclnn_logsumexp.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOGSUMEXP_H_
#define OP_API_INC_LOGSUMEXP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogSumExp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、BFLOAT16、FLOAT32
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW。
 * @param [in] dim: 需要做logsumexp的维度，数据类型INT64
 * @param [in] keepDim: 是否缩减维度
 * @param [in] out: npu
 * npu device侧的aclTensor，数据类型支持FLOAT16、BFLOAT16、FLOAT32，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW，
 * 且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnLogSumExpGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepDim,
                                                     aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogSumExp的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogSoftmaxGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogSumExp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOGSUMEXP_H_
// End content from: aclnn_logsumexp.h

// Begin content from: aclnn_fake_quant_per_channel_affine_cachemask.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_fake_quant_per_channel_affine_cachemask.h
 * \brief
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_CHANNEL_AFFINE_CACHEMASK_H_
#define OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_CHANNEL_AFFINE_CACHEMASK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFakeQuantPerChannelAffineCachemask的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：
 *   对于输入数据self，使用scale和zero_point对输入self在指定轴axis上进行伪量化处理，并根据quant_min和quant_max对
 * 伪量化输出进行值域更新，最终返回结果out及对应位置掩码mask。
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *   A1[(self)] -->B1(l0op::Contiguous)-->C1(l0op::Transpose)-->D(l0op::FakeQuantPerChannelAffineCachemask)
 *   A2[(scale)] -->B2(l0op::Contiguous)-->D(l0op::FakeQuantPerChannelAffineCachemask)
 *   A3[(zeroPoint)]-->B3(l0op::Contiguous)-->D(l0op::FakeQuantPerChannelAffineCachemask)
 *   A4((quantMin)) --> D(l0op::FakeQuantPerChannelAffineCachemask)
 *   A5((quantMax)) --> D(l0op::FakeQuantPerChannelAffineCachemask)
 *   A6((axis))-->C1(l0op::Transpose)
 *   D(l0op::FakeQuantPerChannelAffineCachemask)-->C2(l0op::Transpose)-->E1(l0op::ViewCopy)-->F1[(out)]
 *   D(l0op::FakeQuantPerChannelAffineCachemask)-->C3(l0op::Transpose)-->EF2(l0op::ViewCopy)-->F2[(mask)]
 * ```
 * @param [in] self:
 * Device侧的aclTensor，数据类型支持FLOAT16、FLOAT32。支持非连续的Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [in] scale: Device侧的aclTensor，表示输入伪量化的缩放系数。数据类型支持FLOAT16、FLOAT32，size大小为1。
 * @param [in] zeroPoint: Device侧的aclTensor，表示输入伪量化的零基准参数。数据类型支持INT32，size大小为1。
 * @param [in] axis: Host侧的整型，表示计算维度，数据类型支持INT。
 * @param [in] quantMin: Host侧的整型，表示输入数据伪量化后的最小值，数据类型支持INT。
 * @param [in] quantMax: Host侧的整型，表示输入数据伪量化后的最大值，数据类型支持INT。
 * @param [out] out:
 * Device侧的aclTensor，数据类型支持LOAT16、FLOAT32，支持非连续Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [out] mask: Device侧的aclTensor，数据类型支持BOOL，支持非连续Tensor，[数据格式](common/数据格式.md)支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFakeQuantPerChannelAffineCachemaskGetWorkspaceSize(
    const aclTensor* self, const aclTensor* scale, const aclTensor* zeroPoint, int64_t axis, int64_t quantMin,
    int64_t quantMax, aclTensor* out, aclTensor* mask, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFakeQuantPerChannelAffineCachemask的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnFakeQuantPerChannelAffineCachemask获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFakeQuantPerChannelAffineCachemask(void* workspace, uint64_t workspaceSize,
                                                              aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_FAKE_QUANT_PER_CHANNEL_AFFINE_CACHEMASK_H_
// End content from: aclnn_fake_quant_per_channel_affine_cachemask.h

// Begin content from: aclnn_nll_loss2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NLLLOSS2DFORWARD_H_
#define OP_API_INC_NLLLOSS2DFORWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNLLLoss2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * @param [in] self: npu device侧的aclTensor，数据类型支持整型，浮点、复数数据类型。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW。
 * @param [in] target: npu device侧的aclTensor，数据类型支持整型，浮点、复数数据类型。
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW
 * @param [in] weight: npu device侧的aclTensor
 * @param [in] reduction: 返回用户需要在npu device侧申请的workspace大小。
 * @param [in] ignoreIndex: 返回op执行器，包含了算子计算流程。
 * @param [out] out: 返回用户返回損失值。
 * @param [out] totalWeightOut: 返回權重係數。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnNLLLoss2dGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                     const aclTensor* weight, int64_t reduction, int64_t ignoreIndex,
                                                     aclTensor* out, aclTensor* totalWeightOut, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);
/**
 * @brief aclnnNLLLoss2d的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnNLLLoss2dGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLoss2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NLLLOSS2DFORWARD_H_
// End content from: aclnn_nll_loss2d.h

// Begin content from: aclnn_foreach_pow_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_POW_SCALAR_LIST_H_
#define ACLNN_FOREACH_POW_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachPowScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachPowScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachPowScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_pow_scalar_list.h

// Begin content from: aclnn_logsoftmax_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOGSOFTMAX_BACKWARD_H_
#define OP_API_INC_LOGSOFTMAX_BACKWARD_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogSoftmaxBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnLogSoftmaxBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* output,
                                                              int64_t dim, aclTensor* out, uint64_t* workspaceSize,
                                                              aclOpExecutor** executor);

/**
 * @brief aclnnLogSoftmaxBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnLogSoftmaxBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOGSOFTMAX_BACKWARD_H_
// End content from: aclnn_logsoftmax_backward.h

// Begin content from: aclnn_ctc_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_CTCLOSSBACKWARD_H_
#define OP_API_INC_CTCLOSSBACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCtcLossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * _ctc_loss的反向传播。
 *
 *
 * 计算图：
 * ```mermaid
 * graph LR
 * A[(logProbs)] -->B([l0op::Contiguous])-->D([l0op::CTCLossV2Grad])
 * A1[(targets)] -->B1([l0op::Contiguous])--> D
 * A6[(gradOut)] -->B6([l0op::Contiguous])-->D
 * A7[(negLogLikelihood)] -->B7([l0op::Contiguous])-->D
 * A8[(logAlpha)] -->B8([l0op::Contiguous])-->D
 * A2[(targetLengths)] -->B2([ConvertToTensor])-->D
 * A3[(inputLengths)] -->B3([ConvertToTensor])-->D
 * A4((blank)) -->D
 * A5((zeroInfinity)) -->D
 * D--> F1([l0op::ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] gradOut(aclTensor*): 表示梯度更新系数，数据类型支持FLOAT,
 * DOUBLE数据类型(数据类型必须和logProbs一致)，该Tensor必须为1维，支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] logProbs(aclTensor*): 数据类型支持FLOAT,DOUBLE数据类型，shape为($T,N,C$)，
 * $T$为输入长度，$N$为批处理大小，$C$为类别数，必须大于0，包括空白标识，该Tensor表示输出的对数概率，
 * 支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] targets(aclTensor*): 数据类型支持INT64,INT32,BOOL,FLOAT,FLOAT16数据类型，当shape为($N,S$)，
 * $S$为不小于$targetLengths$中的最大值的值；或者shape为(SUM($targetLengths$))，假设$targets$是未填充的而且在1维内级联的；
 * 支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] inputLengths(aclIntArray*)：数据类型支持UINT8,INT8,INT16,INT32,INT64，数组长度为$N$，
 * 数组中的每个值必须小于等于$T$。
 * @param [in] targetLengths(aclIntArray*)：数据类型支持UINT8,INT8,INT16,INT32,INT64，数组长度为$N$，
 * 当targets的shape为($N,S$)时，数组中的每个值必须小于等于$S$。
 * @param [in] negLogLikelihood(aclTensor*)：数据类型支持FLOAT,DOUBLE数据类型(数据类型必须和logProbs一致)，
 * 表示相对于每个输入节点可微分的损失值，该Tensor必须为1维，支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] logAlpha(aclTensor*)：数据类型支持FLOAT,DOUBLE数据类型(数据类型必须和logProbs一致)，
 * 表示输入到目标的可能跟踪的概率，该Tensor必须为3维，支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [in] blank(int)：int整型，空白标识，默认为0，数值必须小于$C$大于等于0。
 * @param [in] zeroInfinity(bool)：bool类型，表示是否将无限损耗和相关梯度归零，默认值为$False$。
 * @param [out] out(aclTensor*): 表示CTC的损失梯度，数据类型支持FLOAT,DOUBLE，shape为($T,N,C$)，
 * 支持[非连续的Tensor](https://)，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCtcLossBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* logProbs,
                                                           const aclTensor* targets, const aclIntArray* inputLengths,
                                                           const aclIntArray* targetLengths,
                                                           const aclTensor* negLogLikelihood, const aclTensor* logAlpha,
                                                           int64_t blank, bool zeroInfinity, aclTensor* out,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnCtcLossBackward 的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnCtcLossBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCtcLossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CTCLOSSBACKWARD_H_
// End content from: aclnn_ctc_loss_backward.h

// Begin content from: aclnn_lt_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LTSCALAR_H_
#define OP_API_INC_LTSCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLtScalarGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Scalar中的元素，返回一个Bool类型的Tensor，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ? [True]
 * : [False] $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] self: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,INT32,UINT8,BOOL,UINT64,UINT32,DOUBLE,UINT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，shape需要与other满足broadcast关系。
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnLtScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLtScalar的第二段接口，用于执行计算。
 *
 * 算子功能：计算self Tensor中的元素是否小于(<)other
 * Scalar中的元素，返回一个Bool类型的Tensor，self<other的为True，否则为False 计算公式：$$ out = (self < other)  ? [True]
 * : [False] $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(Self)] -->B([Contiguous])
 * B-->C1([Cast])-->D([Less])
 * E[(other)] -->F([Contiguous])
 * F --> C2([Cast])-->D
 * D -->F1([ViewCopy])--> J[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLtScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLtScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceLtScalarGetWorkspaceSize的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算selfRef Tensor中的元素是否小于(<)other
 * Scalar中的元素，返回修改后的selfRef，selfRef<other的为True，否则为False 计算公式：$$ out = (selfRef < other)  ?
 * [True] : [False] $$
 *
 * 实现说明：api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(SelfRef)] --> B([l0op::Contiguous])
 * B-->C1([l0op::Cast])--> D([l0op::Less])
 * E((other)) --> C2([l0op::Cast])
 * C2 -->D
 * D --> C3([l0op::Cast])
 * C3 --> F1([l0op::ViewCopy])
 * F1 --> J[(selfRef)]
 * ```
 *
 * @param [in] selfRef: npu device侧的aclTensor，
 * 数据类型支持FLOAT16,FLOAT,INT64,INT32,UINT8,BOOL,UINT64,UINT32,DOUBLE,UINT16数据类型，
 * shape需要与other满足broadcast关系，支持非连续的Tensor，数据格式支持ND。
 * @param [in] other: host侧的aclScalar，shape需要与other满足broadcast关系。
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnInplaceLtScalarGetWorkspaceSize(const aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLtScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLtScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLtScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LTSCALAR_H_
// End content from: aclnn_lt_scalar.h

// Begin content from: aclnn_rms_norm_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_RMS_NORM_GRAD_H_
#define ACLNN_RMS_NORM_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnRmsNormGradGetWorkspaceSize
 * parameters :
 * dy : required
 * x : required
 * rstd : required
 * gamma : required
 * dxOut : required
 * dgammaOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRmsNormGradGetWorkspaceSize(
    const aclTensor *dy,
    const aclTensor *x,
    const aclTensor *rstd,
    const aclTensor *gamma,
    const aclTensor *dxOut,
    const aclTensor *dgammaOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnRmsNormGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnRmsNormGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_rms_norm_grad.h

// Begin content from: aclnn_relu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_RELU_H_
#define OP_API_INC_RELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = xi if xi > 0 else 0
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Relu])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持
 * FLOAT、FLOAT16、INT8、INT32、UINT8、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持
 * FLOAT、FLOAT16、INT8、INT32、UINT8、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReluGetWorkspaceSize(const aclTensor* self, const aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief aclnnRelu的第二段接口，用于执行计算。
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = xi if xi > 0 else 0
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Relu])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

/**
 * @brief aclnnInplaceRelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = xi if xi > 0 else 0
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Relu])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
```
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持
FLOAT、FLOAT16、INT8、INT32、UINT8、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceReluGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnInplaceRelu的第二段接口，用于执行计算。
 * 算子功能：完成加法计算
 * 计算公式：
 * $$ output_i = xi if xi > 0 else 0
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([L0::Contiguous])
 *     B --> C([L0::Relu])
 *     C --> D([L0::ViewCopy])
 *     D --> E[(out)]
 * ```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceReluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceRelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_RELU_H
// End content from: aclnn_relu.h

// Begin content from: aclnn_grouped_matmul_v3.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUPED_MATMUL_V3_H
#define OP_API_INC_GROUPED_MATMUL_V3_H
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupedMatmulV3的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 表示公式中的x，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] weight:
 * 表示公式中的weight，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] biasOptional:
 * 表示公式中的bias，数据类型支持FLOAT16、FLOAT32、INT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] scaleOptional: 表示量化参数，数据类型支持UINT64数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] offsetOptional: 表示量化参数，数据类型支持FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantScaleOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] antiquantOffsetOptional:
 * 表示伪量化参数，数据类型支持FLOAT16，BFLOAT16数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [in] groupListOptional: 可选参数，代表输入和输出分组轴上的索引情况，数据类型支持INT64，支持的最大长度为128个。
 * @param [in] splitItem:
 * 整数型参数，代表输出是否要做tensor切分，0/1代表输出为多tensor；2/3代表输出为单tensor，默认值为0。
 * @param [in] groupType:
 * 整数型参数，代表需要切分的轴，-1代表不需要切分；0代表需要切分M轴；1代表需要切分N轴；2代表需要切分K轴。
 * @param [out] y: 表示公式中的out，数据类型支持FLOAT16、BFLOAT16、INT8、FLOAT32数据类型，数据格式支持ND，支持的最大长度为128个。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV3GetWorkspaceSize(
    const aclTensorList* x, const aclTensorList* weight, const aclTensorList* biasOptional,
    const aclTensorList* scaleOptional, const aclTensorList* offsetOptional,
    const aclTensorList* antiquantScaleOptional, const aclTensorList* antiquantOffsetOptional,
    const aclTensor* groupListOptional, int64_t splitItem, int64_t groupType, const aclTensorList* y,
    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGroupedMatmulV3的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnGtTensorGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGroupedMatmulV3(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_grouped_matmul_v3.h

// Begin content from: aclnn_convolution_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_CONVOLUTION_BACKWARD_H_
#define OP_API_INC_CONVOLUTION_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnConvolutionBackward的第一段接口，计算并获取workspace大小
 * @domain aclnn_ops_train
 *
 * @param [in] gradOutput: npu，卷积输出梯度
 * device侧的aclTensor，数据类型浮点类型FLOAT16，FLOAT32，BFLOAT16
 * 支持非连续的Tensor，数据格式支持NCL、NCHW
 * @param [in] input: npu，卷积输入
 * device侧的aclTensor，数据类型浮点类型FLOAT16，FLOAT32，BFLOAT16
 * 支持非连续的Tensor，数据格式支持NCL、NCHW
 * @param [in] weight: npu, 卷积权重
 * device侧的aclTensor，数据类型与input一致
 * 支持非连续的Tensor，数据格式与input一致
 * @param [in] biasSizes: npu，偏置的shape
 * aclIntArray, shape为1
 * @param [in] stride: 步长
 * aclIntArray，数组长度可以为1或者input的维度-2（也等于kernel size -1），例：2D卷积的步长数组的有效长度是2位
 * @param [in] padding: 补边
 * aclIntArray，数组长度可以为1或者input的维度-2（也等于kernel size
 * -1），在NCHW格式下可为4维。例：2D卷积的padding数组的有效长度是2位
 * @param [in] dilation: kernel中元素的间隔，>1代表空洞卷积
 * aclIntArray，数组长度可以为1或者input的维度-2（也等于kernel size -1），例：2D卷积的dilation数组的有效长度是2位
 * @param [in] transposed: 是否转置
 * bool，True代表转置卷积
 * @param [in] outputPadding：转置卷积时生效，对输出的补边
 * aclIntArray，数组长度可以为1或者input的维度-2，值必须分别小于stride或者dilation的最大值，例：2D转置卷积的dilation数组的有效长度是2位
 * @param [in] groups：分组数，表示从输入通道到输出通道的块链接个数
 * int64，大于0且能整除input和output的通道数， input通道数 = weight通道数*groups
 * @param [in] outputMask：输出掩码, 指定输出中是否包含输入、权重、偏差的梯度
 * aclBoolArray, 反向传播过程输出掩码参数为True对应位置的梯度
 * @param [in] cubeMathType：用于判断Cube单元应该使用哪种计算逻辑进行运算
 * int8_t, Cube单元计算逻辑判断参数
 * @param [out] grad_input: 卷积输入梯度在npu device侧的aclTensor
 * @param [out] grad_input: 卷积权重梯度在npu device侧的aclTensor
 * @param [out] grad_bias: 卷积偏置梯度在npu device侧的aclTensor
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvolutionBackwardGetWorkspaceSize(
    const aclTensor* gradOutput, const aclTensor* input, const aclTensor* weight, const aclIntArray* biasSizes,
    const aclIntArray* stride, const aclIntArray* padding, const aclIntArray* dilation, bool transposed,
    const aclIntArray* outputPadding, int groups, const aclBoolArray* outputMask, int8_t cubeMathType,
    aclTensor* gradInput, aclTensor* gradWeight, aclTensor* gradBias, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnConvTbcBackward的第一段接口，计算并获取workspace大小
 * @domain aclnn_ops_train
 *
 * @param [in] self: npu，卷积输出梯度
 * device侧的aclTensor，数据类型浮点类型FLOAT16，FLOAT32
 * 支持非连续的Tensor，数据格式支持ND、NCHW
 * @param [in] input: npu，卷积输入
 * device侧的aclTensor，数据类型浮点类型FLOAT16，FLOAT32
 * 支持非连续的Tensor，数据格式支持ND、NCHW
 * @param [in] weight: npu, 卷积权重
 * device侧的aclTensor，数据类型与input一致
 * 支持非连续的Tensor，数据格式与input一致
 * @param [in] bias: npu，卷积偏置
 * device侧的aclTensor，数据类型与input一致
 * @param [in] pad: 补边
 * int64_t,（也等于kernel size -1），例：2D卷积的padding数组的有效长度是2位
 * @param [in] dilation: kernel中元素的间隔，>1代表空洞卷积
 * aclIntArray，数组长度需等于input的维度-2（也等于kernel size -1），例：2D卷积的dilation数组的有效长度是2位
 * @param [out] grad_input: 卷积输入梯度在npu device侧的aclTensor
 * @param [out] grad_input: 卷积权重梯度在npu device侧的aclTensor
 * @param [out] grad_bias: 卷积偏置梯度在npu device侧的aclTensor
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvTbcBackwardGetWorkspaceSize(const aclTensor* self, const aclTensor* input,
                                                           const aclTensor* weight, const aclTensor* bias,
                                                           int64_t pad, int8_t cubeMathType,
                                                           aclTensor* gradInput, aclTensor* gradWeight,
                                                           aclTensor* gradBias, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnConvolutionBackward的第二段接口，用于执行计算。
 *
 * 算子功能：完成卷积反向计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnConvTbcBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvolutionBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               const aclrtStream stream);

/**
 * @brief aclnnConvTbcBackward的第二段接口，用于执行计算。
 *
 * 算子功能：完成卷积反向计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnConvTbcbackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnConvTbcBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CONVOLUTION_BACKWARD_H_
// End content from: aclnn_convolution_backward.h

// Begin content from: aclnn_group_norm_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GROUP_NORM_BACKWARD_H_
#define OP_API_INC_GROUP_NORM_BACKWARD_H_

#include <array>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupNormBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnGroupNormBackwardGetWorkspaceSize(const aclTensor* gradOut, const aclTensor* input,
                                                             const aclTensor* mean, const aclTensor* rstd,
                                                             const aclTensor* gamma, int64_t N, int64_t C, int64_t HxW,
                                                             int64_t group, const aclBoolArray* outputMask,
                                                             aclTensor* gradInput, aclTensor* gradGammaOut,
                                                             aclTensor* gradBetaOut, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnGroupNormBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGroupNormBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GROUP_NORM_BACKWARD_H_
// End content from: aclnn_group_norm_backward.h

// Begin content from: aclnn_slice.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SLICE_H_
#define OP_API_INC_LEVEL2_ACLNN_SLICE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSlice的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnSliceGetWorkspaceSize(const aclTensor* self, int64_t dim, int64_t start, int64_t end,
                                                 int64_t step, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnSlice的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSlice(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SLICE_H_
// End content from: aclnn_slice.h

// Begin content from: aclnn_circular_pad2d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_CIRCULAR_PAD2D_BACKWARD_H_
#define OP_API_INC_CIRCULAR_PAD2D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnCircularPad2dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：循环填充的反向传播。
 * @param [in] gradOutput: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, BFLOAT16, 数据格式支持ND，
 * 维度支持三维或四维且与self和gradInput一致，shape需要与circular_pad2d正向传播的output一致。
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型与gradOutput一致，数据格式支持ND，维度支持三维或四维且与gradOutput和 gradInput一致，shape与gradInput一致。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * 前两个数值需小于self最后一维度的数值，后两个数值需小于self倒数第二维度的数值。
 * @param [in] gradInput: npu device侧的aclTensor, 数据类型与gradOutput一致，shape与self一致，数据格式支持ND，
 * 维度支持三维或四维且与gradOutput和self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad2dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                                   const aclIntArray* padding, aclTensor* gradInput,
                                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnCircularPad2dBackward的第二段接口，用于执行计算
 *
 * 算子功能：循环填充的反向传播。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnCircularPad2dBackwardGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnCircularPad2dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                 aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_CIRCULAR_PAD2D_BACKWARD_H_// End content from: aclnn_circular_pad2d_backward.h

// Begin content from: aclnn_multilabel_margin_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MULTILABEL_MARGIN_LOSS_H_
#define OP_API_INC_MULTILABEL_MARGIN_LOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMultilabelMarginLoss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnMultilabelMarginLossGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                                int64_t reduction, aclTensor* out, aclTensor* isTarget,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnMultilabelMarginLoss的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMultilabelMarginLoss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MULTILABEL_MARGIN_LOSS_H_
// End content from: aclnn_multilabel_margin_loss.h

// Begin content from: aclnn_upsample_bilinear2d_aa.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UPSAMPLE_BILINEAR2D_AA_H_
#define OP_API_INC_UPSAMPLE_BILINEAR2D_AA_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBilinear2dAA的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：对由多个输入通道组成的输入信号应用2D双线性抗锯齿采样。
 * @domain aclnn_ops_infer
 * 参数描述:
 * @Param [in] input 输入Tensor，数据类型支持FLOAT，FLOAT16，BFLOAT16。支持非连续Tensor，数据格式支持NCHW，shape维度仅支持4维的Tensor。数据类型与出参out的数据类型一致。
 * @Param [in] outputSize 输出空间大小，要求是二维数组，数据类型支持INT64，取值和out的H、W维度一样。
 * @Param [in] alignCorners 是否是角对齐。如果设置为true，则输入和输出张量按其角像素的中心点对齐，保留角像素处的值。如果设置为false，则输入和输出张量通过其角像素的角点对齐，并使用边缘值对边界外的值进行填充。
 * @Param [in] scalesH 空间大小的height维度乘数。
 * @Param [in] scalesW 空间大小的width维度乘数。
 * @Param [in] out 数据类型支持FLOAT、FLOAT16、BFLOAT16。支持非连续的Tensor，数据格式支持NCHW。shape维度仅支持4维的Tensor。数据类型与入参input的数据类型一致。
 * @Param [out] workspaceSize 返回用户需要在npu device侧申请的workspace大小。
 * @Param [out] executor 返回op执行器，包含了算子计算流程。
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2dAAGetWorkspaceSize(const aclTensor* input, const aclIntArray* outputSize,
                                                                bool alignCorners, double scalesH,
                                                                double scalesW, aclTensor* out,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleBilinear2dAA的第二段接口，用于执行计算。
 * 功能描述：对由多个输入通道组成的输入信号应用2D双线性抗锯齿采样。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleBilinear2dAA(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UPSAMPLE_BILINEAR2D_AA_H_// End content from: aclnn_upsample_bilinear2d_aa.h

// Begin content from: aclnn_weight_quant_matmul_all_reduce.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_weight_quant_matmul_all_reduce.h
 * \brief
 */
#ifndef OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_
#define OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnWeightQuantMatmulAllReduce的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：float16, bf16。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8,int4。
 * @param [in] bias: 偏置，数据类型支持：float16, bf16。
 * @param [in] antiquantScale: 对x2进行伪量化计算参数，数据类型支持：float16, bf16。
 * @param [in] antiquantOffset: 对x2进行伪量化计算参数，数据类型支持：float16, bf16。
 * @param [in] x3: add计算参数，数据类型支持：float16, bf16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，默认值：1。
 * @param [in] antiquantGroupSize: per_group模式的groupSize输入。
 * @param [out] output: 计算+通信的结果，数据类型：float16, bf16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnWeightQuantMatmulAllReduceGetWorkspaceSize(
    const aclTensor* x1, const aclTensor* x2, const aclTensor* bias, const aclTensor* antiquantScale,
    const aclTensor* antiquantOffset, const aclTensor* x3, const char* group, const char* reduceOp, int64_t commTurn,
    int64_t streamMode, int64_t antiquantGroupSize, const aclTensor* output, uint64_t* workspaceSize,
    aclOpExecutor** executor);

/**
 * @brief aclnnWeightQuantMatmulAllReduce的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnWeightQuantMatmulAllReduceGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnWeightQuantMatmulAllReduce(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                      const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_WEIGHT_QUANT_MATMUL_ALL_REDUCE_// End content from: aclnn_weight_quant_matmul_all_reduce.h

// Begin content from: aclnn_isneginf.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_ISNEGINF_H_
#define OP_API_INC_ISNEGINF_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIsNegInf的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入张量的元素是否为负无穷。
 * 实现说明-计算图
 * 计算图：如下
 * 场景1：浮点数场景
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0op::Contiguous])
 *   B --> D([l0op::IsNegInf])
 *   D --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 * 场景2：非浮点数，有界，返回False
 * ```mermaid
 * graph LR
 *   A[(self)] -->B([l0op::Fill])
 *   B --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 *
 * @param [in] self：输入`self`,数据类型支持FLOAT、FLOAT16、BFLOAT16(Ascend910B),
 * INT32、INT64、INT16、INT8、UINT8、BOOL。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] out：输出`out`,数据类型支持BOOL，shape需要与self一致。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsNegInfGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                    aclOpExecutor** executor);

/**
 * @brief aclnnIsNegInf的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnIsNegInfGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnIsNegInf(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                    const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ISNEGINF_H_
// End content from: aclnn_isneginf.h

// Begin content from: aclnn_logaddexp.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOG_ADD_EXP_H_
#define OP_API_INC_LOG_ADD_EXP_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogAddExp的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成幂和取对数计算（底为e）
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、DOUBLE，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogAddExpGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                                     uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogAddExp的第二段接口，用于执行计算。
 *
 * 算子功能：完成幂和取对数计算（底为e）
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnLogAddExpGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogAddExp(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOG_ADD_EXP_H_
// End content from: aclnn_logaddexp.h

// Begin content from: aclnn_pdist.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PDIST_H_
#define OP_API_INC_PDIST_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPdist的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算输入张量中每对行向量之间的p-范数距离。
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持浮点类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] p: 输入属性，数据类型支持FLOAT。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持浮点类型，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPdistGetWorkspaceSize(const aclTensor* self, float p, aclTensor* out,
                                                 uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPdist的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入张量中每对行向量之间的p-范数欧式距离。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnPdistGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPdist(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PDIST_H_
// End content from: aclnn_pdist.h

// Begin content from: aclnn_fused_infer_attention_score_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef ACLNN_FUSED_INFER_ATTENTION_SCORE_V2_H_
#define ACLNN_FUSED_INFER_ATTENTION_SCORE_V2_H_
// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief The first interface of aclnnFusedInferAttentionScoreV2 calculates the workspace size based on the specific calculation process.
 * @domain aclnn_ops_infer
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFusedInferAttentionScoreV2GetWorkspaceSize(
    const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShiftOptional,
    const aclTensor *attenMaskOptional, const aclIntArray *actualSeqLengthsOptional,
    const aclIntArray *actualSeqLengthsKvOptional, const aclTensor *deqScale1Optional,
    const aclTensor *quantScale1Optional, const aclTensor *deqScale2Optional, const aclTensor *quantScale2Optional,
    const aclTensor *quantOffset2Optional, const aclTensor *antiquantScaleOptional,
    const aclTensor *antiquantOffsetOptional, const aclTensor *blockTableOptional,
    const aclTensor *queryPaddingSizeOptional, const aclTensor *kvPaddingSizeOptional,
    const aclTensor *keyAntiquantScaleOptional, const aclTensor *keyAntiquantOffsetOptional,
    const aclTensor *valueAntiquantScaleOptional, const aclTensor *valueAntiquantOffsetOptional,
    const aclTensor *keySharedPrefixOptional, const aclTensor *valueSharedPrefixOptional,
    const aclIntArray *actualSharedPrefixLenOptional, int64_t numHeads, double scaleValue, int64_t preTokens,
    int64_t nextTokens, char *inputLayout, int64_t numKeyValueHeads, int64_t sparseMode, int64_t innerPrecise,
    int64_t blockSize, int64_t antiquantMode, bool softmaxLseFlag, int64_t keyAntiquantMode, int64_t valueAntiquantMode,
    const aclTensor *attentionOut, const aclTensor *softmaxLse, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief The second interface of aclnnFusedInferAttentionScoreV2 is used to perform calculations.
 */
__attribute__((visibility("default"))) aclnnStatus aclnnFusedInferAttentionScoreV2(void *workspace,
                                                                                   uint64_t workspaceSize,
                                                                                   aclOpExecutor *executor,
                                                                                   const aclrtStream stream);


#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_fused_infer_attention_score_v2.h

// Begin content from: aclnn_gt_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GTSCALAR_H_
#define OP_API_INC_GTSCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于other
 * Scalar的值，返回一个Bool类型的Tensor，对应输入Tensor中每个位置的大于判断是否成立 计算公式： $$ out_{i}= (self_i >
 * other) ? True : False $$
 *
 * @param [in] self: 待进行gt计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL、BFLOAT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: 待进行gt计算的入参,aclScalar。
 * @param [in] out: npu device侧的aclTensor，输出一个数据类型为BOOL类型的Tensor，数据格式支持ND。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGtScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGtScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGtScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGtScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceGtScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：判断输入Tensor中的每个元素是否大于other Scalar的值
 * 计算公式： $$ selfRef_{i} = (selfRef_{i} > other_{i}) ? True : False $$
 *
 * @param [in] selfRef: 待进行gt本地计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL、BFLOAT16，数据格式支持ND，支持非连续的Tensor。
 * @param [in] other: 待进行gt计算的入参,aclScalar。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGtScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceGtScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceGtScalarGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceGtScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GTTENSOR_H_
// End content from: aclnn_gt_scalar.h

// Begin content from: aclnn_logdet.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOGDET_H_
#define OP_API_INC_LOGDET_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogdet的第一段接口，根据具体的计算流程，计算workspace大小.
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor计算行列式的自然对数
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)]-->B([Contiguous])-->C([LogMatrixDeterminant])
 * D([Cast])-->E([ViewCopy])-->F([out])
 * ```
 *
 * @param [in] self: npu device侧的aclTensor, 数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128. shape满足(*, n, n)形式,
 * 其中`*`表示0或更多维度的batch. 支持非连续的Tensor. 数据格式支持ND.
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、DOUBLE、COMPLEX64、COMPLEX128. shape和self的batch一致.
 * 支持非连续的Tensor. 数据格式支持ND.
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小.
 * @param [out] executor: 返回op执行器，包含算子计算流程.
 * @return aclnnStatus: 返回状态码， 成功返回ACLNN_SUCCESS, 失败返回对应错误码.
 */
ACLNN_API aclnnStatus aclnnLogdetGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                  aclOpExecutor** executor);

/**
 * @brief aclnnLogdet的第二段接口，用于执行计算.
 *
 * 算子功能：对输入的每个元素完成相反数计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址.
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogdetGetWorkspaceSize获取.
 * @param [in] executor: op执行器，包含了算子计算流程.
 * @param [in] stream: acl stream流.
 * @return aclnnStatus: 返回状态码.
 */
ACLNN_API aclnnStatus aclnnLogdet(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOGDET_H_
// End content from: aclnn_logdet.h

// Begin content from: aclnn_any.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ANY_H_
#define OP_API_INC_LEVEL2_ACLNN_ANY_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAny的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnAnyGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, bool keepdim,
                                               aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAny的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAny(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ANY_H_// End content from: aclnn_any.h

// Begin content from: aclnn_addmm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ADDMM_H_
#define OP_API_INC_ADDMM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAddmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAddmmGetWorkspaceSize(const aclTensor* self, const aclTensor* mat1, const aclTensor* mat2,
                                                 const aclScalar* beta, const aclScalar* alpha, aclTensor* out,
                                                 int8_t cubeMathType, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @brief aclnnInplaceAddmm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnInplaceAddmmGetWorkspaceSize(const aclTensor* selfRef, const aclTensor* mat1,
                                                        const aclTensor* mat2, const aclScalar* beta,
                                                        const aclScalar* alpha, int8_t cubeMathType,
                                                        uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAddmm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAddmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                 const aclrtStream stream);

/**
 * @brief aclnnInplaceAddmm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnInplaceAddmm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_addmm.h

// Begin content from: aclnn_selu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SELU_H_
#define OP_API_INC_LEVEL2_ACLNN_SELU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成selu操作
 * @param [in] self: npu device侧的aclTensor, 数据类型支持INT8、INT32,
 * FLOAT、FLOAT16、shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持INT8、INT32, FLOAT、FLOAT16, shape与self
 *  保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSeluGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief: aclnnSelu的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成selu操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSeluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceSelu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 对输入Tensor完成selu操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT32,
 * FLOAT、FLOAT16，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceSeluGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceSelu的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成selu操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceSeluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSelu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_selu.h

// Begin content from: aclnn_soft_margin_loss.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFT_MARGIN_LOSS_H_
#define OP_API_INC_SOFT_MARGIN_LOSS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftMarginLoss的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：计算输入self和目标target的二分类逻辑损失函数。
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与target满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] target: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: host侧的int64，指定要应用到输出的缩减，支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] out: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftMarginLossGetWorkspaceSize(const aclTensor* self, const aclTensor* target,
                                                          int64_t reduction, aclTensor* out, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnSoftMarginLoss的第二段接口，用于执行计算。
 *
 * 算子功能：计算输入self和目标target的二分类逻辑损失函数。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnSoftMarginLossGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftMarginLoss(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFT_MARGIN_LOSS_H_
// End content from: aclnn_soft_margin_loss.h

// Begin content from: aclnn_upsample_nearest_1d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_UNAMPLE_NEAREST_1D_H_
#define OP_API_INC_UNAMPLE_NEAREST_1D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleNearest1d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest1dGetWorkspaceSize(const aclTensor* self, const aclIntArray* outputSize,
                                                             aclTensor* out, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleNearest1d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnUpsampleNearest1d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_NEAREST_1D_H_
// End content from: aclnn_upsample_nearest_1d.h

// Begin content from: aclnn_asinh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_ASINH_H_
#define OP_API_INC_ASINH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAsinh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入Tensor中的每个元素进行反双曲正弦操作后输出。
 * 计算公式：
 * out_{i}=ln(input_{i} + \sqrt{input_{i}^2 + 1})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [in]   out
 * 输出Tensor，数据类型支持FLOAT，BFLOAT16, FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnAsinhGetWorkspaceSize(const aclTensor* input, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);
/**
 * @brief aclnnAsinh的第二段接口，用于执行计算。
 * 功能描述：对输入Tensor中的每个元素进行反双曲正弦操作后输出。
 * 计算公式：
 * out_{i}=ln(input_{i} + \sqrt{input_{i}^2 + 1})
 * 实现说明：
 * api计算的基本路径：
```mermaid
graph LR
    A[(Self)] -->B([l0op::Contiguous])
    B --> C([l0op::Asinh])
    C --> G([l0op::Cast])
    G --> E([l0op::ViewCopy])
    E --> S[(Out)]
```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAsinhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAsinh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceAsinh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 功能描述：对输入Tensor中的每个元素进行反双曲正弦操作后输出。
 * 计算公式：
 * out_{i}=ln(input_{i} + \sqrt{input_{i}^2 + 1})
 * 参数描述：
 * @param [in]   input
 * 输入Tensor，数据类型支持INT8，INT16，INT32，INT64，UINT8，BOOL，FLOAT，FLOAT16，DOUBLE，COMPLEX64，COMPLEX128。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize   返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor         返回op执行器，包含了算子计算流程。
 * @return       aclnnStatus      返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceAsinhGetWorkspaceSize(aclTensor* inputRef, uint64_t* workspaceSize,
                                                        aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceAsinh的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成asinh操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAsinhGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceAsinh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_asinh.h

// Begin content from: aclnn_flatten.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at **
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_FLATTEN_H_
#define OP_API_INC_FLATTEN_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFlatten的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：将输入Tensor，基于给定的axis，扁平化为一个2D的Tensor。
 * 若self的shape为(d_0,d_1,...,d_n)，那么输出out的shape为(d_0 X d_1 ... X d_(axis-1), d_axis X d_(axis+1)... X d_n)。
 * 若axis取值为0，则输出out的shape为(1, d_0 X d_1 ... X d_n)。
 * 实现说明-计算图
 *
 * ```mermaid
 * graph LR
 *   A[(self)] --> B([l0op::Contiguous])
 *   B --> C([l0op::Flatten])
 *   D[(axis)] --> C
 *   C --> I([l0op::ViewCopy])
 *   I --> J[(out)]
 * ```
 *
 * @param [in]
 * self：输入`self`,数据类型支持INT8、INT16、INT32、INT64、UINT8、BOOL、BFLOAT16(Ascend910B)、FLOAT、FLOAT16。
 * 支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] axis：输入`axis`，int64_t类型整数。表示flatten计算的基准轴。取值范围为[-self.dim(),self.dim())。
 * @param [out]
 * out：输出`out`,数据类型支持INT8、INT16、INT32、INT64、UINT8、BOOL、BFLOAT16(Ascend910B)、FLOAT、FLOAT16。
 * shape为2D。支持[非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFlattenGetWorkspaceSize(const aclTensor* self, int64_t axis, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnFlatten的第二段接口，用于执行计算
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnFlattenGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnFlatten(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_FLATTEN_H_
// End content from: aclnn_flatten.h

// Begin content from: aclnn_binary_cross_entropy_with_logits.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_H_
#define OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBinaryCrossEntropyWithLogits的第一段接口，根据具体的计算流程，计算workspace大小
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyWithLogitsGetWorkspaceSize(
    const aclTensor* self, const aclTensor* target, const aclTensor* weightOptional, const aclTensor* posWeightOptional,
    int64_t reduction, aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/*
 * @brief aclnnBinaryCrossEntropyWithLogits的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBinaryCrossEntropyWithLogits(void* workspace, uint64_t workspaceSize,
                                                        aclOpExecutor* executor, const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_BINARY_CROSS_ENTROPY_WITH_LOGITS_H_
// End content from: aclnn_binary_cross_entropy_with_logits.h

// Begin content from: aclnn_quant_matmul_all_reduce_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_quant_matmul_all_reduce_v2.h
 * \brief
 */
#ifndef OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V2_
#define OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V2_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulAllReduceV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] biasOptional: 偏置，数据类型支持：int32。
 * @param [in] x3Optional: add操作参数，数据类型支持：float16,bfloat16。
 * @param [in] dequantScale: 去量化系数，数据类型支持：int64,uint64,bfloat16,float32。
 * @param [in] pertokenScaleOptional: per-token去量化系数，数据类型支持：float32。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] output: 计算+通信的结果，数据类型：float16,bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceV2GetWorkspaceSize(const aclTensor *x1, const aclTensor *x2,
                                                                  const aclTensor *biasOptional,
                                                                  const aclTensor *x3Optional,
                                                                  const aclTensor *dequantScale,
                                                                  const aclTensor *pertokenScaleOptional,
                                                                  const char* group, const char *reduceOp,
                                                                  int64_t commTurn, int64_t streamMode,
                                                                  const aclTensor *output, uint64_t *workspaceSize,
                                                                  aclOpExecutor **executor);

/**
 * @brief aclnnQuantMatmulAllReduceV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnQuantMatmulAllReduceV2GetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceV2(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                                  aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_ALL_REDUCE_V2_// End content from: aclnn_quant_matmul_all_reduce_v2.h

// Begin content from: aclnn_logsoftmax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LOGSOFTMAX_H_
#define OP_API_INC_LOGSOFTMAX_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAdd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持DOUBLE, FLOAT16, FLOAT32
 * 支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW。
 * @param [in] dim: 需要做logsoftmax的维度，数据类型INT64
 * @param [in] out: npu
 * npu device侧的aclTensor，数据类型支持DOUBLE, FLOAT16, FLOAT32，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW，
 * 且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogSoftmaxGetWorkspaceSize(const aclTensor* self, int64_t dim, aclTensor* out,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnLogSoftmax的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogSoftmaxGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogSoftmax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LOGSOFTMAX_H_
// End content from: aclnn_logsoftmax.h

// Begin content from: aclnn_max_unpool3d_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_MAX_UNPOOL3D_BACKWARD_H_
#define OP_API_INC_MAX_UNPOOL3D_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMaxUnpool3dBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnMaxUnpool3dBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                               const aclTensor* indices, const aclIntArray* outputSize,
                                                               const aclIntArray* stride, const aclIntArray* padding,
                                                               aclTensor* out, uint64_t* workspaceSize,
                                                               aclOpExecutor** executor);

/**
 * @brief aclnnMaxUnpool3dBackward的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnMaxUnpool3dBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                               aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_MAX_UNPOOL3D_BACKWARD_H_
// End content from: aclnn_max_unpool3d_backward.h

// Begin content from: aclnn_group_norm.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_GROUP_NORM_H_
#define OP_API_INC_GROUP_NORM_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGroupNorm的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnGroupNormGetWorkspaceSize(const aclTensor* self, const aclTensor* gamma,
                                                     const aclTensor* beta, int64_t N, int64_t C, int64_t HxW,
                                                     int64_t group, double eps, aclTensor* out, aclTensor* meanOut,
                                                     aclTensor* rstdOut, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);

/**
 * @brief aclnnGroupNorm的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnGroupNorm(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GROUP_NORM_H_
// End content from: aclnn_group_norm.h

// Begin content from: aclnn_take.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_TAKE_H_
#define OP_API_INC_LEVEL2_ACLNN_TAKE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTake的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：将输入的self张量视为一维数组，把index的值当作索引，从self中取值，输出shape与index一致的Tensor。
 * 计算公式：其中下标i表示从0遍历到index元素个数-1
 * $$ out_{i} = self_{index[i]}  $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 * A[(self)] --> B([l0op::Contiguous]) --> D([l0op::Gather])
 * I[(index)] --> IC([l0op::Contiguous]) --> D --> F1([l0op::ViewCopy]) --> J[(out)]
 * ```
 *
 * @param [in] self: 待进行take计算的入参。npu device侧的aclTensor,
 *     数据类型支持UINT64、INT64、UINT32、INT32、FLOAT32、UINT16、INT16、FLOAT16、INT8、UINT8、DOUBLE、COMPLEX64、COMPLEX128、BOOL，
 *     数据格式支持ND，支持非连续的Tensor，支持维度高于8的场景。
 * @param [in] index: take计算的入参。npu device侧的aclTensor，数据类型支持INT32、INT64。数据格式为ND。
 * @param [in] out: take计算的出参。npu device侧的aclTensor，数据类型同self，shape与index一致，数据格式为ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTakeGetWorkspaceSize(const aclTensor* self, const aclTensor* index, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnTake的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTakeGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTake(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_TAKE_H_
// End content from: aclnn_take.h

// Begin content from: aclnn_gelu_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at **
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GELUV2_H_
#define OP_API_INC_LEVEL2_ACLNN_GELUV2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif
/**
 * @brief aclnnGeluV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：高斯误差线性单元激活函数
 * 计算公式：
 * $$ y_{i}=Gelu(x_{i})=x_{i}×Φ(x_{i}) $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(x)]  --> B{l0op::Contiguous}
 *     B -->C([l0op::GeluV2])
 *     C --> D{l0op::ViewCopy}
 *     D --> E[(y)]
 *     F[approximate（可选）]-->G[getApproximateStr]-->C
 * ```
 *
 * @param [in] x: 待进行gelu_v2计算的入参。npu device侧的aclTensor。
 * 数据类型支持FLOAT16、FLOAT32、BFLOAT16，且数据类型必须和y一样，数据格式支持ND，shape必须和y一样，支持非连续的Tensor。
 * @param [in] approximate: gelu_v2计算的入参，指定高斯近似算法，默认值为0（表示：0: "none", 1: "tanh" ）。
 * @param [in] y: gelu_v2计算的出参。
 * npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16，且数据类型必须和x一样，数据格式支持ND，shape必须和x一样，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeluV2GetWorkspaceSize(const aclTensor* x, int64_t approximate, aclTensor* y,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnGeluV2的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeluv2GetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeluV2(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GELUV2_H_// End content from: aclnn_gelu_v2.h

// Begin content from: aclnn_roll.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_ACLNN_ROLL_H
#define OP_API_ACLNN_ROLL_H

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRoll的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成roll操作
 * @param [in] x: device侧的aclTensor，数据类型支持BFLOAT16,FLOAT16, FLOAT32, INT8, UINT8, INT32, UINT32，BOOL。支持
 * [非连续的Tensor](#)，数据格式支持ND（[参考](#)）。
 * @param [in] shifts: int64的数组，数组长度与dims保持一致。
 * @param [in] dims: int64的数组，数组长度与shifts保持一致，取值范围在[-x.dim(), x.dim() -
 * 1]之内，例如：x的维度是4，则取值范 围在[-4, 3]。
 * @param [in] out: device侧的aclTensor，数据类型和数据格式与输入x保持一致
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRollGetWorkspaceSize(const aclTensor* x, const aclIntArray* shifts, const aclIntArray* dims,
                                                aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnRoll的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成roll操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnRollGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRoll(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_ACLNN_ROLL_H// End content from: aclnn_roll.h

// Begin content from: aclnn_foreach_acos.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ACOS_H_
#define ACLNN_FOREACH_ACOS_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAcosGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAcosGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAcos
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAcos(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_acos.h

// Begin content from: aclnn_foreach_addcdiv_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADDCDIV_SCALAR_H_
#define ACLNN_FOREACH_ADDCDIV_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddcdivScalarGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * x3 : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivScalarGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensorList *x3,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddcdivScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddcdivScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_addcdiv_scalar.h

// Begin content from: aclnn_foreach_maximum_scalar_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_MAXIMUM_SCALAR_LIST_H_
#define ACLNN_FOREACH_MAXIMUM_SCALAR_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachMaximumScalarListGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalars : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumScalarListGetWorkspaceSize(
    const aclTensorList *x,
    const aclScalarList *scalars,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachMaximumScalarList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachMaximumScalarList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_maximum_scalar_list.h

// Begin content from: aclnn_foreach_sub_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_SUB_SCALAR_H_
#define ACLNN_FOREACH_SUB_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachSubScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachSubScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachSubScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_sub_scalar.h

// Begin content from: aclnn_pow.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_POW_TENSOR_SCALAR_H_
#define OP_API_INC_POW_TENSOR_SCALAR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPowTensorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、
 * UINT8、BOOL、COMPLEX64、COMPLEX128、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] exponent: npu
 * device侧的aclScalar，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、
 * UINT8、BOOL、COMPLEX64、COMPLEX128、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型）。
 * @param [in] out:npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、
 * UINT8、COMPLEX64、COMPLEX128、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 且数据类型、数据格式和shape与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPowTensorScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* exponent,
                                                           const aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);
/**
 * @brief aclnnPowTensorScalar的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnPowTensorScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnPowTensorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

/**
 * @brief aclnnInplacePowTensorScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、
 * UINT8、BOOL、COMPLEX64、COMPLEX128、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型），
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] exponent: npu
 * device侧的aclScalar，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、
 * UINT8、COMPLEX64、COMPLEX128、BFLOAT16（在Ascend910及之前芯片上不支持该数据类型）。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplacePowTensorScalarGetWorkspaceSize(const aclTensor* self, const aclScalar* exponent,
                                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplacePowTensorScalar的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小,
 * 由第一段接口aclnnInplacePowTensorScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplacePowTensorScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                  aclrtStream stream);

/**
 * @brief aclnnPowScalarTensor的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnPowScalarTensorGetWorkspaceSize(const aclScalar* self, const aclTensor* exponent,
                                                           const aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnPowScalarTensor(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_POW_TENSOR_SCALAR_H_
// End content from: aclnn_pow.h

// Begin content from: aclnn_linspace.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LINSPACE_H_
#define OP_API_INC_LINSPACE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLinspace的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 功能描述：创建一个大小为steps的1维向量，其值从start起始到end结束（包含）线性均匀分布。
 *
 * 计算公式：$$ out = (start, start + \frac{end - start}{steps - 1},...,
 *                    tart + (steps -2) * \frac{end - start}{steps - 1}, end) $$
 *
 * 参数描述：
 * @param [in]   start
 * 获取值的范围的起始位置：host侧的aclScalar，数据类型支持整型，浮点数据类型。数据格式支持ND。
 * @param [in]   end
 * 获取值的范围的结束位置：host侧的aclScalar，数据类型支持整型，浮点数据类型。数据格式支持ND。
 * @param [in]   steps
 * 张量的大小：host侧的aclScalar，数据类型支持整型。数据格式支持ND。需要满足steps大于等于0。
 * @param [in]   out              指定的输出tensor：npu
 * device侧的aclTensor，数据类型支持整型，浮点数据类型，数据格式支持ND。
 * @param [out]  workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out]  executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLinspaceGetWorkspaceSize(const aclScalar* start, const aclScalar* end, int64_t steps,
                                                    aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnLinspace的第二段接口，用于执行计算。
 *
 *
 * 功能描述：创建一个大小为steps的1维向量，其值从start起始到end结束（包含）线性均匀分布。
 *
 * 计算公式：$$ out = (start, start + \frac{end - start}{steps - 1},...,
 *                    start + (steps -2) * \frac{end - start}{steps - 1}, end) $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLinspaceGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLinspace(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_linspace.h

// Begin content from: aclnn_elu_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ELU_BACKWARD_H_
#define OP_API_INC_LEVEL2_ACLNN_ELU_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * 算子功能：完成ELU激活函数的反向计算，输出ELU激活函数正向输入的梯度。
 * 计算公式：无
 *
 * 实现说明：如下
 *
 * 计算图：如下
 *
 * ```mermaid
 * graph LR
 *     A[(gradOutput)] --> B([l0op::Contiguous])
 *     B --> C([l0op::EluGradV2])
 *     C --> D([l0op::Cast])
 *     D --> E([l0op::ViewCopy])
 *     E --> F[(gradInput)]
 *     G((alpha)) --> C
 *     H((scale)) --> C
 *     I((inputScale)) --> C
 *     L((isResult)) --> C
 *     J((selfOrResult)) --> K([l0op::Contiguous])
 *     K --> C
 * ```
 */

/**
 * @brief aclnnEluBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * @param [in] gradOutput：表示ELU激活函数正向输出的梯度，可自定义，类似于权重，npu
 * device侧的aclTensor，数据类型支持FLOAT、
 * FLOAT16、BFLOAT16（910B支持），支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] alpha：表示ELU激活函数的激活系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] scale：表示ELU激活函数的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] inputScale：表示ELU激活函数的输入的缩放系数，host侧的aclScalar，数据类型需要是可转换为FLOAT的数据类型。
 * @param [in] isResult：表示传给ELU反向计算的输入是否是ELU正向的输出，数据类型支持BOOL。
 * @param [in]
 * selfOrResult：当isResult为True时，表示ELU激活函数正向的输出，当isResult为False时，表示ELU激活函数正向的输入， npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16，数据类型需要与gradOutput一致。shape需要和gradOutput的shape一致，支持
 * 非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [in] gradInput：表示ELU激活函数正向输入的梯度，即对输入进行求导后的结果，npu
 * device侧的aclTensor，数据类型需要是
 * gradOutput可转换的数据类型，shape需要和gradOutput的shape一致，支持非连续的Tensor，数据格式支持ND，数据维度不支持8维以上。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEluBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclScalar* alpha,
                                                       const aclScalar* scale, const aclScalar* inputScale,
                                                       bool isResult, const aclTensor* selfOrResult,
                                                       aclTensor* gradInput, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnEluBackward的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnEluBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEluBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ELU_BACKWARD_H_
// End content from: aclnn_elu_backward.h

// Begin content from: aclnn_silent_check.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_SILENT_CHECK_H_
#define OP_API_INC_LEVEL2_ACLNN_SILENT_CHECK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSilentCheck的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能： 根据输入val判断是否触发静默检测错误。
 * 该算子为自定义算子语义, 无对应的tensorflow或pytorch接口。
 * @param [in] val: npu device侧的aclTensor，数据类型支持FLOAT32, FLOAT16, BFLOAT16。shape为[1]。
 * @param [in] inputGrad: npu device侧的aclTensor，数据类型支持FLOAT32, FLOAT16, BFLOAT16。
 * @param [in] sfda: npu device侧的aclTensor，数据类型支持FLOAT32。shape为[3]。
 * @param [in] step: npu device侧的aclTensor，数据类型支持INT64。shape为[1]。
 * @param [in] cMinSteps: 需要累积的步数，数据类型为INT32。
 * @param [in] cThreshL1: 数值上的L1阈值，数据类型为FLOAT。
 * @param [in] cCoeffL1: 跳变上的L1阈值，数据类型为FLOAT。
 * @param [in] cThreshL2: 数值上的L2阈值，数据类型为FLOAT。
 * @param [in] cCoeffL2: 跳变上的L2阈值，数据类型为FLOAT。
 * @param [in] npuAsdDetect: 环境变量NPU_ASD_DETECT，数据类型为INT32。
 * @param [in] result: 判断是否触发静默检测故障结果的aclTensor，数据类型为INT32。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSilentCheckGetWorkspaceSize(const aclTensor *val, aclTensor *inputGradRef,
                                                       aclTensor *sfdaRef, aclTensor *stepRef, const int32_t cMinSteps,
                                                       const float cThreshL1, const float cCoeffL1,
                                                       const float cThreshL2, const float cCoeffL2,
                                                       const int32_t npuAsdDetect, aclTensor* result,
                                                       uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief: aclnnSilentCheck的第二段接口，用于执行计算
 * @domain aclnn_ops_train
 * 算子功能： 根据输入val判断是否触发静默检测错误。
 * 该算子为自定义算子语义, 无对应的tensorflow或pytorch接口。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSilentCheckGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSilentCheck(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SILENT_CHECK_H_// End content from: aclnn_silent_check.h

// Begin content from: aclnn_bidirection_lstm.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_BIDIRECTION_LSTM_H_
#define ACLNN_BIDIRECTION_LSTM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnBidirectionLSTMGetWorkspaceSize
 * parameters :
 * x : required
 * initH : required
 * initC : required
 * wIh : required
 * wHh : required
 * bIhOptional : optional
 * bHhOptional : optional
 * wIhReverseOptional : optional
 * wHhReverseOptional : optional
 * bIhReverseOptional : optional
 * bHhReverseOptional : optional
 * numLayers : optional
 * isbias : optional
 * batchFirst : optional
 * bidirection : optional
 * yOut : required
 * outputHOut : required
 * outputCOut : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBidirectionLSTMGetWorkspaceSize(
    const aclTensor *x,
    const aclTensor *initH,
    const aclTensor *initC,
    const aclTensor *wIh,
    const aclTensor *wHh,
    const aclTensor *bIhOptional,
    const aclTensor *bHhOptional,
    const aclTensor *wIhReverseOptional,
    const aclTensor *wHhReverseOptional,
    const aclTensor *bIhReverseOptional,
    const aclTensor *bHhReverseOptional,
    int64_t numLayers,
    bool isbias,
    bool batchFirst,
    bool bidirection,
    const aclTensor *yOut,
    const aclTensor *outputHOut,
    const aclTensor *outputCOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnBidirectionLSTM
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnBidirectionLSTM(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_bidirection_lstm.h

// Begin content from: aclnn_quant_matmul_all_reduce.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*!
 * \file aclnn_quant_matmul_all_reduce.h
 * \brief
 */
#ifndef OP_API_INC_QUANT_MATMUL_ALL_REDUCE_
#define OP_API_INC_QUANT_MATMUL_ALL_REDUCE_

#include <string>

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
// #include "hccl/hccl.h"
// #include "hccl/hccl_types.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQuantMatmulAllReduce的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：实现mm+AllReduce融合计算
 * @param [in] x1: matmul左矩阵，数据类型支持：int8。
 * @param [in] x2: matmul右矩阵，数据类型支持：int8。
 * @param [in] bias: 偏置，数据类型支持：int32。
 * @param [in] x3: add操作参数，数据类型支持：float16,bfloat16。
 * @param [in] dequantScale: 去量化系数，数据类型支持：int64,uint64,bfloat16。
 * @param [in] group: 标识列组的字符串。
 * @param [in] reduceOp: reduce操作类型，默认值：sum。
 * @param [in] commTurn: 通信数据切分数，即总数据量/单次通信量，默认值：0。
 * @param [in] streamMode: acl流模式的枚举，类型支持：1。
 * @param [out] output: 计算+通信的结果，数据类型：float16,bfloat16。
 * @param [out] workspaceSize: 返回需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduceGetWorkspaceSize(const aclTensor* x1, const aclTensor* x2,
                                                                const aclTensor* bias, const aclTensor* x3,
                                                                const aclTensor* dequantScale, const char* group,
                                                                const char* reduceOp, int64_t commTurn,
                                                                int64_t streamMode, const aclTensor* output,
                                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnQuantMatmulAllReduce的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnQuantMatmulAllReduceGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQuantMatmulAllReduce(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QUANT_MATMUL_ALL_REDUCE_// End content from: aclnn_quant_matmul_all_reduce.h

// Begin content from: aclnn_foreach_add_list.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ADD_LIST_H_
#define ACLNN_FOREACH_ADD_LIST_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAddListGetWorkspaceSize
 * parameters :
 * x1 : dynamic
 * x2 : dynamic
 * alpha : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddListGetWorkspaceSize(
    const aclTensorList *x1,
    const aclTensorList *x2,
    const aclTensor *alpha,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAddList
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAddList(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_add_list.h

// Begin content from: aclnn_fill_scalar.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_FILL_H_
#define OP_API_INC_FILL_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnInplaceFillScalar的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：完成对tensor填充指定标量
 *
 *
 * @param [in] selfRef：npu device侧的aclTensor，数据类型支持FLOAT、FLOAT16、UINT8、INT8、
 * INT16、INT32、INT64、DOUBLE、COMPLEX64、COMPLEX128、BOOL、BFLOAT16。支持非连续的Tensor，
 * 数据格式ND，数据维度不支持8维以上。
 * @param [in] value: host侧的aclScalar，数据类型需要转换成self的数据类型。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillScalarGetWorkspaceSize(aclTensor* selfRef, const aclScalar* value,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnInplaceFillScalar的第二段接口，用于执行计算。
 *
 * 算子功能：完成对tensor填充指定标量
 *
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu
 * device侧申请的workspace大小，由第一段接口aclnnInplaceFillScalarGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceFillScalar(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_FILL_H_
// End content from: aclnn_fill_scalar.h

// Begin content from: aclnn_mrgba_custom.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_MRGBA_CUSTOM_H_
#define ACLNN_MRGBA_CUSTOM_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnMrgbaCustomGetWorkspaceSize
 * parameters :
 * rgb : required
 * alpha : required
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMrgbaCustomGetWorkspaceSize(
    const aclTensor *rgb,
    const aclTensor *alpha,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnMrgbaCustom
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnMrgbaCustom(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_mrgba_custom.h

// Begin content from: aclnn_index_add.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_INDEX_ADD_H_
#define OP_API_INC_INDEX_ADD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnIndexAdd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnIndexAddGetWorkspaceSize(const aclTensor* self, const int64_t dim, const aclTensor* index,
                                                    const aclTensor* source, const aclScalar* alpha, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnIndexAdd的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnIndexAdd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif// End content from: aclnn_index_add.h

// Begin content from: aclnn_rsqrt.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_RSQRT_H_
#define OP_API_INC_RSQRT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnRsqrt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成求算术平方根倒数的操作
 * @param [in] self: npu device侧的aclTensor,
 * 数据类型支持浮点、整数类型，shape为非空，支持非连续的Tensor，数据格式支持ND， 支持非连续的Tensor。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持浮点类型, shape与self保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnRsqrtGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);
/**
 * @brief: aclnnRsqrt的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成求算术平方根倒数的操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSigmoidGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnRsqrt(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceRsqrt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成求算术平方根倒数的操作
 * @param [in] self: npu device侧的aclTensor, 数据类型支持浮点类型，shape为非空，支持非连续的Tensor，数据格式支持ND
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceRsqrtGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                        aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnInplaceRsqrt(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                        aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_RSQRT_H_// End content from: aclnn_rsqrt.h

// Begin content from: aclnn_swi_glu.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_SWI_GLU_H_
#define ACLNN_SWI_GLU_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnSwiGluGetWorkspaceSize
 * parameters :
 * x : required
 * dim : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwiGluGetWorkspaceSize(
    const aclTensor *x,
    int64_t dim,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnSwiGlu
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwiGlu(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_swi_glu.h

// Begin content from: aclnn_flash_attention_score.h
/**
 * Copyright (c) 2023-2024 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_FLASH_ATTENTION_SCORE_H_
#define OP_API_INC_LEVEL2_ACLNN_FLASH_ATTENTION_SCORE_H_

// #include "aclnn/aclnn_base.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnFlashAttentionScore的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
aclnnStatus aclnnFlashAttentionScoreGetWorkspaceSize(
    const aclTensor *query, const aclTensor *key, const aclTensor *value, const aclTensor *realShiftOptional,
    const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional, const aclTensor *attenMaskOptional,
    const aclIntArray *prefixOptional, double scaleValue, double keepProb, int64_t preTokens, int64_t nextTokens,
    int64_t headNum, char *inputLayout, int64_t innerPrecise, int64_t sparseMode, const aclTensor *softmaxMaxOut,
    const aclTensor *softmaxSumOut, const aclTensor *softmaxOutOut, const aclTensor *attentionOutOut,
    uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionScore的第二段接口，用于执行计算。
 */
aclnnStatus aclnnFlashAttentionScore(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                     const aclrtStream stream);

/**
 * @brief aclnnFlashAttentionVarLenScore的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
aclnnStatus aclnnFlashAttentionVarLenScoreGetWorkspaceSize(
    const aclTensor *query, const aclTensor *key, const aclTensor *value, const aclTensor *realShiftOptional,
    const aclTensor *dropMaskOptional, const aclTensor *paddingMaskOptional, const aclTensor *attenMaskOptional,
    const aclIntArray *prefixOptional, const aclIntArray *actualSeqQLenOptional,
    const aclIntArray *actualSeqKvLenOptional, double scaleValue, double keepProb, int64_t preTokens,
    int64_t nextTokens, int64_t headNum, char *inputLayout, int64_t innerPrecise, int64_t sparseMode,
    const aclTensor *softmaxMaxOut, const aclTensor *softmaxSumOut, const aclTensor *softmaxOutOut,
    const aclTensor *attentionOutOut, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionVarLenScore的第二段接口，用于执行计算。
 */
aclnnStatus aclnnFlashAttentionVarLenScore(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                           const aclrtStream stream);


/**
 * @brief aclnnFlashAttentionScoreV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
*/
aclnnStatus aclnnFlashAttentionScoreV2GetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *realShiftOptional,
    const aclTensor *dropMaskOptional,
    const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional,
    const aclIntArray *prefixOptional,
    const aclIntArray *qStartIdxOptional,
    const aclIntArray *kvStartIdxOptional,
    double scaleValue,
    double keepProb,
    int64_t preTokens,
    int64_t nextTokens,
    int64_t headNum,
    char *inputLayout,
    int64_t innerPrecise,
    int64_t sparseMode,
    int64_t pseType,
    const aclTensor *softmaxMaxOut,
    const aclTensor *softmaxSumOut,
    const aclTensor *softmaxOutOut,
    const aclTensor *attentionOutOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionScoreV2的第二段接口，用于执行计算。
*/
aclnnStatus aclnnFlashAttentionScoreV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    const aclrtStream stream);

/**
 * @brief aclnnFlashAttentionVarLenScoreV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
*/
aclnnStatus aclnnFlashAttentionVarLenScoreV2GetWorkspaceSize(
    const aclTensor *query,
    const aclTensor *key,
    const aclTensor *value,
    const aclTensor *realShiftOptional,
    const aclTensor *dropMaskOptional,
    const aclTensor *paddingMaskOptional,
    const aclTensor *attenMaskOptional,
    const aclIntArray *prefixOptional,
    const aclIntArray *actualSeqQLenOptional,
    const aclIntArray *actualSeqKvLenOptional,
    const aclIntArray *qStartIdxOptional,
    const aclIntArray *kvStartIdxOptional,
    double scaleValue,
    double keepProb,
    int64_t preTokens,
    int64_t nextTokens,
    int64_t headNum,
    char *inputLayout,
    int64_t innerPrecise,
    int64_t sparseMode,
    int64_t pseType,
    const aclTensor *softmaxMaxOut,
    const aclTensor *softmaxSumOut,
    const aclTensor *softmaxOutOut,
    const aclTensor *attentionOutOut,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/**
 * @brief aclnnFlashAttentionVarLenScoreV2的第二段接口，用于执行计算。
*/
aclnnStatus aclnnFlashAttentionVarLenScoreV2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif // OP_API_INC_LEVEL2_ACLNN_FLASH_ATTENTION_SCORE_H_
// End content from: aclnn_flash_attention_score.h

// Begin content from: aclnn_foreach_asin.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_ASIN_H_
#define ACLNN_FOREACH_ASIN_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachAsinGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAsinGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachAsin
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachAsin(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_asin.h

// Begin content from: aclnn_gelu_backward_v2.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_GELU_BACKWARD_V2_H_
#define OP_API_INC_GELU_BACKWARD_V2_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeluBackwardV2的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 算子功能：完成Gelu的反向。
 * @param [in] gradOutput：反向传播的梯度值，即上一层的输出梯度，和正向输出的shape一致。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND，支持非连续的Tensor。
 * @param [in] self：gelu的输出值。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND，支持非连续的Tensor。
 * @param [in] approximate: gelu_backward计算的入参，指定高斯近似算法，可配置为"none"或"tanh"。
 * @param [out] gradInput：gelu_backward的输出，为输入的梯度值，即对输入进行求导后的结果。
 * npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、BFLOAT16类型，
 * 数据格式支持FRACTAL_NZ，NC1HWC0,ND，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeluBackwardV2GetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                          char *approximate, aclTensor* gradInput,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGeluBackwardV2的第二段接口，用于执行计算
 */
ACLNN_API aclnnStatus aclnnGeluBackwardV2(void* workspace, uint64_t workspace_size, aclOpExecutor* executor,
                                          const aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_GELU_BACKWARD_V2_H_
// End content from: aclnn_gelu_backward_v2.h

// Begin content from: aclnn_softmax.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_OP_API_INC_LEVEL2_OP_ACLNN_SOFTMAX_H_
#define OP_API_OP_API_INC_LEVEL2_OP_ACLNN_SOFTMAX_H_
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"
#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftmax的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnSoftmaxGetWorkspaceSize(const aclTensor* self, int64_t dim, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnSoftmax的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnSoftmax(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif
#endif  // OP_API_OP_API_INC_LEVEL2_OP_ACLNN_SOFTMAX_H_
// End content from: aclnn_softmax.h

// Begin content from: aclnn_einsum.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_EINSUM_H_
#define OP_API_INC_EINSUM_H_

#include <string>
// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnEinsum的第一段接口，根据具体的计算流程，计算workspace大小。
 * 功能描述：使用爱因斯坦求和约定对张量序列计算代数张量运算
 * @domain aclnn_ops_infer
 * @param [in]   tensors:
 * 输入TensorList，数据类型支持FLOAT16、FLOAT、INT16、UINT16、INT32、UINT32、INT64、UINT64。支持非连续Tensor，数据格式支持ND。
 * @param [in]   equation: 输入字符串，数据类型支持const char *。
 * @param [out]  output:
 * 输出Tensor，数据类型支持FLOAT16、FLOAT、INT16、UINT16、INT32、UINT32、INT64、UINT64。支持非连续Tensor，数据格式支持ND。
 * @param [out]  workspaceSize：返回用户需要在npu device侧申的的workspace大小。
 * @param [out]  executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnEinsumGetWorkspaceSize(const aclTensorList* tensors, const char* equation, aclTensor* output,
                                                  uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnEinsum的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAddGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnEinsum(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_EINSUM_H_// End content from: aclnn_einsum.h

// Begin content from: aclnn_softplus.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SOFTPLUS_H_
#define OP_API_INC_SOFTPLUS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSoftplus的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：激活函数softplus
 * 计算公式：
 * $$
 * Softplus(x) = \begin{cases}
 * \frac{1}{\beta} \log(1+\exp(\beta x)), \beta *x \le threshold \\
 * x, \beta *x > threshold
 * \end{cases}
 * $$
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持浮点类型，支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW。
 * @param [in] beta: host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [in] threshold：host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持浮点类型，支持非连续的Tensor，数据格式支持ND、NCHW、NHWC、HWCN、NDHWC、NCDHW。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftplusGetWorkspaceSize(const aclTensor* self, const aclScalar* beta,
                                                    const aclScalar* threshold, aclTensor* out, uint64_t* workspaceSize,
                                                    aclOpExecutor** executor);

/**
 * @brief aclnnSoftplus的第二段接口，用于执行计算。
 *
 * 算子功能：激活函数softplus
 * 计算公式：
 * $$
 * Softplus(x) = \begin{cases}
 * \frac{1}{\beta} \log(1+\exp(\beta x)), \beta *x \le threshold \\
 * x, \beta *x > threshold
 * \end{cases}
 * $$
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSoftplusGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSoftplus(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SOFTPLUS_H_// End content from: aclnn_softplus.h

// Begin content from: aclnn_replication_pad2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REPLICATION_PAD2D_H_
#define OP_API_INC_REPLICATION_PAD2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReplicationPad2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：使用输入边界填充输入tensor。
 * @param [in] self: 数据类型支持FLOAT16, FLOAT32, DOUBLE, INT8, INT16, INT32, INT64, UINT8,
 * COMPLEX64, COMPLEX128，支持非连续的Tensor，数据格式支持ND，维度支持三维或四维，在最后两维做pad。
 * @param [in] padding: 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * @param [in] out: 数据类型、数据格式、维度与self一致，倒数第二维度的数值等于self倒数第二维度的
 * 数值加padding后两个值，最后一维度的数值等于self最后一维度的数值加padding前两个值。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                            aclTensor* out, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);

/**
 * @brief: aclnnReplicationPad2d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReplicationPad2dGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReplicationPad2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REPLICATION_PAD2D_H_// End content from: aclnn_replication_pad2d.h

// Begin content from: aclnn_foreach_div_scalar.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_DIV_SCALAR_H_
#define ACLNN_FOREACH_DIV_SCALAR_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachDivScalarGetWorkspaceSize
 * parameters :
 * x : dynamic
 * scalar : required
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivScalarGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensor *scalar,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachDivScalar
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachDivScalar(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_div_scalar.h

// Begin content from: aclnn_signbit.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_SIGN_BIT_H_
#define OP_API_INC_LEVEL2_ACLNN_SIGN_BIT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSignbit的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * @param [in] self: npu device侧的aclTensor，
 * @param [in] out: npu device侧的aclTensor
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */

ACLNN_API aclnnStatus aclnnSignbitGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnSignbit的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSignbitGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSignbit(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_SIGN_BIT_H_// End content from: aclnn_signbit.h

// Begin content from: aclnn_sinc.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SINC_H_
#define OP_API_INC_SINC_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSinc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、FLOAT16、
 *  DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128, shape与self
 *  保持相同，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnSincGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** executor);

/**
 * @brief: aclnnSinc的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSincGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSinc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnInplaceSinc的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成sin操作
 * @param [in] selfRef: npu device侧的aclTensor, 数据类型支持INT8、INT16、INT32, INT64, UINT8、BOOL、FLOAT、
 *  FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，shape为非空，支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnInplaceSincGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspace_size,
                                                       aclOpExecutor** executor);

/**
 * @brief: aclnnInplaceSinc的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor原地完成sin操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceSincGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSinc(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SINC_H_// End content from: aclnn_sinc.h

// Begin content from: aclnn_batch_norm_stats.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_BATCH_NORMAL_STATS_H_
#define OP_API_BATCH_NORMAL_STATS_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnBatchNormStats的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 */
ACLNN_API aclnnStatus aclnnBatchNormStatsGetWorkspaceSize(const aclTensor* input, double eps, aclTensor* mean,
                                                          aclTensor* invstd, uint64_t* workspaceSize,
                                                          aclOpExecutor** executor);

/**
 * @brief aclnnBatchNormStats的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnBatchNormStats(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_BATCH_NORMAL_STATS_H_
// End content from: aclnn_batch_norm_stats.h

// Begin content from: aclnn_geglu.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GEGLU_H_
#define OP_API_INC_LEVEL2_ACLNN_GEGLU_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGeGlu的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：高斯误差线性单元激活函数
 * 计算公式：
 * $$ out_{i}=GeGlu(self_{i}) = A \cdot Gelu(B) $$
 * 其中，$A$表示$self$的前半部分，$B$表示$self$的后半部分。
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(self)] --> B([l0op::Contiguous])
 *     B --> C([l0op::GeGlu])
 *     C --> D([l0op::ViewCopy])
 *     D --> E[(out)]
 *     D --> F[(outGelu)]
 *
 *     G((dim)) --> C
 *     H((approximate)) --> C
```
 */

/**
 * @param [in] self: 待进行GeGlu计算的入参，npu
 * device侧的aclTensor，数据类型支持FLOAT32、FLOAT16、BFLOAT16（910B支持），支持
 * [非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] dim：可选入参，设定的slice轴，数据类型支持INT64。
 * @param [in] approximate：可选入参，GeGlu计算使用的激活函数索引，0表示使用`tanh`，1表示使用`none`，数据类型支持INT64。
 * @param [in] out: GeGlu计算的出参，npu
 * device侧的aclTensor，数据类型需要与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] outGelu: Gelu(B)计算的出参，npu
 * device侧的aclTensor，数据类型需要与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluGetWorkspaceSize(const aclTensor* self, int64_t dim, int64_t approximate,
                                                 aclTensor* out, aclTensor* outGelu, uint64_t* workspaceSize,
                                                 aclOpExecutor** executor);

/**
 * @param [in] self: 待进行GeGlu计算的入参，npu
 * @domain aclnn_ops_infer
 * device侧的aclTensor，数据类型支持FLOAT32、FLOAT16、BFLOAT16（910B支持），支持
 * [非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] dim：可选入参，设定的slice轴，数据类型支持INT64。
 * @param [in] approximate：可选入参，GeGlu计算使用的激活函数索引，0表示使用`tanh`，1表示使用`none`，数据类型支持INT64。
 * @param [in] out: GeGlu计算的出参，npu
 * device侧的aclTensor，数据类型需要与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [in] activateLeft:
 * 计算属性，host侧的布尔值，表示激活函数操作数据块的方向，默认值为false，表示对右边做activate。
 * @param [in] outGelu: Gelu(B)计算的出参，npu
 * device侧的aclTensor，数据类型需要与self一致，支持[非连续的Tensor](#)，数据格式支持ND([参考](#))。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluV3GetWorkspaceSize(const aclTensor* self, int64_t dim, int64_t approximate,
                                                   bool activateLeft, aclTensor* out, aclTensor* outGelu,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGeGlu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeGluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGlu(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

/**
 * @brief aclnnGeGlu的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGeGluGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGeGluV3(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GEGLU_H_
// End content from: aclnn_geglu.h

// Begin content from: aclnn_digamma.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/license/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_DIGAMMA_H_
#define OP_API_INC_LEVEL2_ACLNN_DIGAMMA_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

#define ACLNN_MAX_SHAPE_RANK 8

/**
 * @brief aclnnDigamma的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：返回输入Tensor中每个元素绝对值的结果
 * 计算公式：
 * $$ out_{i} = digamma(self_{i}) $$
 * @param [in] self: 待进行digamma计算的入参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE，数据格式支持ND，支持非连续的Tensor。
 * @param [in] out: digamma计算的出参。npu device侧的aclTensor,
 * 数据类型支持FLOAT、FLOAT16、DOUBLE，数据格式支持ND，支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包括算子计算流程
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnDigammaGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                   aclOpExecutor** executor);

/**
 * @brief aclnnDigamma的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnDigammaGetWorkspaceSize获取。
 * @param [in] exector: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnDigamma(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_DIGAMMA_H_// End content from: aclnn_digamma.h

// Begin content from: aclnn_foreach_log2.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_LOG2_H_
#define ACLNN_FOREACH_LOG2_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachLog2GetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog2GetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachLog2
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachLog2(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_log2.h

// Begin content from: aclnn_gather.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_GATHER_H_
#define OP_API_INC_LEVEL2_ACLNN_GATHER_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnGather的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：对输入tensor中指定的维度进行数据聚集
 * 计算公式：$$ gather(X,index,d)_{i_0,i_1,\cdots,i_{d-1},i_{d+1},\cdots,i_{n-1}} =
 * X_{i_0,i_1,\cdots,i_{d-1},index_{i_d},i_{d+1},\cdots,i_{n-1}} $$
 *
 * @param [in] self: 待进行gather计算的入参,npu device侧的aclTensor，
 * 数据类型支持FLOAT16、FLOAT32、INT32、INT64、INT8、UINT8、DOUBLE、UINT16、UINT32、UINT64、BOOL，数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [in] dim: host侧的int64。
 * @param [in] index: 待进行gather计算的入参,npu device侧的aclTensor，
 * 数据类型支持INT32、INT64数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [in] out: gather计算的出参。npu device侧的aclTensor，
 * 数据类型与self相同，数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGatherGetWorkspaceSize(const aclTensor* self, const int64_t dim, const aclTensor* index,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnGather的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnGatherGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnGather(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                  const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_GATHER_H_// End content from: aclnn_gather.h

// Begin content from: aclnn_avgpool2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_AVGPOOL2D_H_
#define OP_API_INC_AVGPOOL2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAvgPool2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnAvgPool2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* kernelSize,
                                                     const aclIntArray* strides, const aclIntArray* paddings,
                                                     const bool ceilMode, const bool countIncludePad,
                                                     const int64_t divisorOverride, const int8_t cubeMathType,
                                                     aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnAvgPool2d的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnAvgPool2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_AVGPOOL2D_H_
// End content from: aclnn_avgpool2d.h

// Begin content from: aclnn_ascend_quant.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_H_
#define OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnAscendQuant的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * @param [in] x: 待进行AscendQuant计算的入参。npu device侧的aclTensor，
 * 数据类型支持float16, bfloat16, float32, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [in] scale: npu device侧的aclTensor, 数据类型支持float, bf16, float16
 * @param [in] offset: npu device侧的aclTensor，数据类型支持float, bf16, float16
 * @param [in] sqrtMode:  host侧的aclScalar，数据类型bool
 * @param [in] roundMode:  host侧的aclScalar，数据类型string
 * @param [in] dstType:  host侧的aclScalar, 数据类型int
 * @param [in] y: AscendQuant计算的出参。npu device侧的aclTensor，
 * 数据类型支持int8, 数据格式支持ND，
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendQuantGetWorkspaceSize(const aclTensor* x, const aclTensor* scale,
                                                       const aclTensor* offset, bool sqrtMode, const char* roundMode,
                                                       int32_t dstType, const aclTensor* y, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);

/**
 * @brief aclnnAscendQuant的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnAscendAntiQuantGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnAscendQuant(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_ASCEND_QUANT_H_// End content from: aclnn_ascend_quant.h

// Begin content from: aclnn_std.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_STD_H_
#define OP_API_INC_STD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnStd的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：计算样本标准差
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     D([dim]) --> C([l0op::ReduceMean])
 *     A[(self)] --> B([l0op::Contiguous])
 *     B --> C
 *     E([keepdim=true]) --> C
 *     C --> H([l0op::Expand])
 *     J([dim]) --> I
 *     K([correction]) --> I
 *     O[(self)] --> I
 *     P --> I([l0op::ReduceStdWithMean])
 *     L([keepdim]) --> I
 *     H --> P[(meanOut)]
 *     I --> M([l0op::ViewCopy])
 *     M --> N[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [in] dim: host侧的aclIntArray，支持的数据类型为INT32、INT64。
 * @param [in] correction: host侧的整形，修正值。
 * @param [in] keepdim: host侧的布尔型，是否在输出张量中保留输入张量的维度。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16。数据格式支持ND。支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnStdGetWorkspaceSize(const aclTensor* self, const aclIntArray* dim, const int64_t correction,
                                               bool keepdim, aclTensor* out, uint64_t* workspaceSize,
                                               aclOpExecutor** executor);

/**
 * @brief aclnnStd的第二段接口，用于执行计算。
 *
 * 算子功能：计算样本标准差
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     D([dim]) --> C([l0op::ReduceMean])
 *     A[(self)] --> B([l0op::Contiguous])
 *     B --> C
 *     E([keepdim=true]) --> C
 *     C --> H([l0op::Expand])
 *     J([dim]) --> I
 *     K([correction]) --> I
 *     O[(self)] --> I
 *     P --> I([l0op::ReduceStdWithMean])
 *     L([keepdim]) --> I
 *     H --> P[(meanOut)]
 *     I --> M([l0op::ViewCopy])
 *     M --> N[(out)]
 * ```
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSubGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnStd(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                               const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_STD_H_
// End content from: aclnn_std.h

// Begin content from: aclnn_swi_glu_grad.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_SWI_GLU_GRAD_H_
#define ACLNN_SWI_GLU_GRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnSwiGluGradGetWorkspaceSize
 * parameters :
 * yGrad : required
 * x : required
 * dim : optional
 * out : required
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwiGluGradGetWorkspaceSize(
    const aclTensor *yGrad,
    const aclTensor *x,
    int64_t dim,
    const aclTensor *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnSwiGluGrad
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnSwiGluGrad(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_swi_glu_grad.h

// Begin content from: aclnn_hardtanh.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LEVEL2_ACLNN_HARDTANH_H_
#define OP_API_INC_LEVEL2_ACLNN_HARDTANH_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnHardtanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：将输入的所有元素限制在[clipValueMin,clipValueMax]范围内，若元素大于max则限制为max，
 * 若元素小于min则限制为min，否则等于元素本身，min默认值为-1.0，max默认值为1.0。
 * 计算公式：如下所示
 * $$
 * HardTanh(x) = \left\{\begin{matrix}
 * \begin{array}{l}
 * clipValueMax\ \ \ \ \ \ \ if \ \ x>clipValueMax \\
 * clipValueMin\ \ \ \ \ \ \ if\ \ x<clipValueMin \\
 * x\ \ \ \ \ \ \ \ \ \ \ otherwise \\
 * \end{array}
 * \end{matrix}\right.\begin{array}{l}
 * \end{array}
 * $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0::Contiguous])
 *     B --> E([l0::ClipByValue])
 *     C((clipValueMin)) --> E
 *     D((clipValueMax)) --> E
 *     E --> G([l0::ViewCopy])
 *     G --> H[(out)]
 * ```
 *
 * @param [in] self: 待进行erf计算的入参。npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT64、INT16、INT8、
 * UINT8、FLOAT64，数据格式支持ND，且数据格式需要与out一致，支持非连续的Tensor。
 * @param [in] out: erf计算的出参。npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16、INT32、INT64、INT16、INT8、UINT8、
 * FLOAT64，数据格式支持ND，且数据格式需要与self一致， 支持非连续的Tensor。
 * @param [in] clipValueMin: host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [in] clipValueMax: host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardtanhGetWorkspaceSize(const aclTensor* self, const aclScalar* clipValueMin,
                                                    const aclScalar* clipValueMax, aclTensor* out,
                                                    uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnHardtanh的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnHardtanhGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnHardtanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                    aclrtStream stream);

/**
 * @brief aclnnInplaceHardtanh的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 * 算子功能：将输入的所有元素限制在[clipValueMin,clipValueMax]范围内，若元素大于max则限制为max，
 * 若元素小于min则限制为min，否则等于元素本身，min默认值为-1.0，max默认值为1.0。
 * 计算公式：如下所示
 * $$
 * HardTanh(x) = \left\{\begin{matrix}
 * \begin{array}{l}
 * clipValueMax\ \ \ \ \ \ \ if \ \ x>clipValueMax \\
 * clipValueMin\ \ \ \ \ \ \ if\ \ x<clipValueMin \\
 * x\ \ \ \ \ \ \ \ \ \ \ otherwise \\
 * \end{array}
 * \end{matrix}\right.\begin{array}{l}
 * \end{array}
 * $$
 *
 * 计算图：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0::Contiguous])
 *     B --> E([l0::ClipByValue])
 *     C((clipValueMin)) --> E
 *     D((clipValueMax)) --> E
 *     E --> G([l0::ViewCopy])
 *     G --> H[(out)]
 * ```
 *
 * @param [in] selfRef: 待进行erf计算的入参。npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、INT32、INT64、INT16、INT8、
 * UINT8、FLOAT64，数据格式支持ND，且数据格式需要与out一致， 支持非连续的Tensor。
 * @param [in] clipValueMin: host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [in] clipValueMax: host侧的aclScalar，数据类型需要可转换成self的数据类型。
 * @param [out] workspace_size: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardtanhGetWorkspaceSize(aclTensor* selfRef, const aclScalar* clipValueMin,
                                                           const aclScalar* clipValueMax, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief aclnnInplaceHardtanh的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceHardtanhGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceHardtanh(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_HARDTANH_H_// End content from: aclnn_hardtanh.h

// Begin content from: aclnn_upsample_bicubic2d_aa_grad.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef ACLNN_UPSAMPLE_BICUBIC2D_AAGRAD_H_
#define ACLNN_UPSAMPLE_BICUBIC2D_AAGRAD_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBicubic2dAAGrad的第一段接口，根据具体的计算流程，计算workspace大小。
 *
 * 算子功能：aclnnUpsampleBicubic2dAA的反向传播。
 *
 * @param [in] gradOutput: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与出参out的数据类型一致。
 * @param [in] outputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为2。表示输入gradOutput在H和W维度上的空间大小。
 * @param [in] inputSize: Host侧的aclIntArray，数据类型支持INT64，size大小为4。表示输出out分别在N、C、H和W维度上的空间大小。
 * @param [in] scalesH: Host侧的浮点型，表示输出out的height维度乘数。
 * @param [in] scalesW: Host侧的浮点型，表示输出out的width维度乘数。
 * @param [out] out: Device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16。
 * 支持非连续的Tensor，数据格式支持NCHW，shape仅支持四维Tensor。数据类型与入参gradOutput的数据类型一致。
 * @param [out] workspaceSize: 返回用户需要在Device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleBicubic2dAAGradGetWorkspaceSize(
    const aclTensor *gradOutput, const aclIntArray *outputSize, const aclIntArray *inputSize, bool alignCorners, 
    double scalesH, double scalesW, aclTensor *out, uint64_t *workspaceSize, aclOpExecutor **executor);

/**
 * @brief aclnnUpsampleBicubic2dAAGrad的第二段接口，用于执行计算。
 * 
 * 算子功能：aclnnUpsampleBicubic2dAA的反向传播。
 *
 * @param [in] workspace: 在Device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在Device侧申请的workspace大小。
 * 由第一段接口aclnnUpsampleBicubic2dAAGradGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: 指定执行任务的AscendCL Stream流。
 * @return aclnnStatus: 返回状态码。
 */
__attribute__((visibility("default")))
aclnnStatus aclnnUpsampleBicubic2dAAGrad(void *workspace, uint64_t workspaceSize, aclOpExecutor *executor,
                                              aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_upsample_bicubic2d_aa_grad.h

// Begin content from: aclnn_sqrt.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SQRT_H_
#define OP_API_INC_SQRT_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnSqrt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：完成非负数平方根计算，负数情况返回nan。
 * $$
 * sqrt(x)=\begin{cases}
 * \sqrt x, & x\ge 0 \\
 * nan, &  x\lt 0
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] -->B([L0::Contiguous])
 * B --> C([L0::Sqrt])
 * C --> D([L0::ViewCopy])
 * D --> E[(out)]
 * ```
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、FLOAT64、COMPLEX64、COMPLEX128、
 * INT32、INT64、INT16、INT8、BOOL、BF16，支持非连续的Tensor。
 * 支持非连续的Tensor，数据格式支持ND
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、FLOAT64、COMPLEX64、COMPLEX128、
 * INT32、INT64、INT16、INT8、BOOL、BF16，支持非连续的Tensor
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] opExecutor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSqrtGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                aclOpExecutor** opExecutor);
/**
 * @brief aclnnSqrt的第二段接口，用于执行计算。
 * 算子功能：完成非负数平方根计算，负数情况返回nan。
 * $$
 * sqrt(x)=\begin{cases}
 * \sqrt x, & x\ge 0 \\
 * nan, &  x\lt 0
 * \end{cases}
 * $$
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] -->B([L0::Contiguous])
 * B --> C([L0::Sqrt])
 * C --> D([L0::ViewCopy])
 * D --> E[(out)]
 * ```
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnSqrtGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] opExecutor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnSqrt(void* workspace, uint64_t workspaceSize, aclOpExecutor* opExecutor, aclrtStream stream);
/**
 * @brief aclnnInplaceSqrt的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 * 算子功能：完成非负数平方根计算，负数情况返回nan。
 * $$
 * sqrt(x)=\begin{cases}
 * \sqrt x, & x\ge 0 \\
 * nan, &  x\lt 0
 * \end{cases}
 * $$
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 * A[(self)] -->B([L0::Contiguous])
 * B --> C([L0::Sqrt])
 * C --> D([L0::ViewCopy])
 * D --> E[(out)]
 * ```
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT32、FLOAT64、COMPLEX64、COMPLEX128、
 * INT32、INT64、INT16、INT8、BOOL、BF16，支持非连续的Tensor
 * 支持非连续的Tensor，数据格式支持ND
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSqrtGetWorkspaceSize(aclTensor* self, uint64_t* workspaceSize,
                                                       aclOpExecutor** executor);
/**
 * @brief aclnnInplaceSqrt的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceReluGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceSqrt(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);
#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SQRT_H_
// End content from: aclnn_sqrt.h

// Begin content from: aclnn_permute.h

/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_PERMUTE_H_
#define OP_API_INC_PERMUTE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnPermute的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 */
ACLNN_API aclnnStatus aclnnPermuteGetWorkspaceSize(const aclTensor* self, const aclIntArray* dims, aclTensor* out,
                                                   uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnPermute的第二段接口，用于执行计算。
 */
ACLNN_API aclnnStatus aclnnPermute(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_PERMUTE_H_
// End content from: aclnn_permute.h

// Begin content from: aclnn_reflection_pad2d.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_REFLECTION_PAD2D_H_
#define OP_API_INC_REFLECTION_PAD2D_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnReflectionPad2d的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能：使用输入边界的反射填充输入tensor。
 * @param [in] self: npu device侧的aclTensor, 数据类型支持BFLOAT16,FLOAT16, FLOAT32, DOUBLE, INT8, INT16,
 * INT32, INT64, UINT8, BOOL，数据格式支持ND，维度支持三维或四维。
 * @param [in] padding: npu device侧的aclIntArray数组, 数据类型为INT64，长度为4，数值依次代表左右上下需要填充的值。
 * 前两个数值需小于self最后一维度的数值，后两个数值需小于self倒数第二维度的数值。
 * @param [in] out: npu device侧的aclTensor,
 * 数据类型、数据格式、维度与self一致，倒数第二维度的数值等于self倒数第二维度的
 * 数值加padding后两个值，最后一维度的数值等于self最后一维度的数值加padding前两个值。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad2dGetWorkspaceSize(const aclTensor* self, const aclIntArray* padding,
                                                           aclTensor* out, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnReflectionPad2d的第二段接口，用于执行计算
 *
 * 算子功能： 使用输入边界的反射填充输入tensor。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnReflectionPad2dGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnReflectionPad2d(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_REFLECTION_PAD2D_H_// End content from: aclnn_reflection_pad2d.h

// Begin content from: aclnn_logical_not.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_LogicalNot_H_
#define OP_API_INC_LogicalNot_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnLogicalNot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成逻辑取反计算
 *
 * 实现说明：
 * api计算的基本路径：
 * ```mermaid
 * graph LR
 *     A[(self)] -->B([l0op::Contiguous])
 *     B --> L([l0op::Cast])
 *     L --> E([l0op::LogicalNot])
 *     E --> E1([l0op::Cast])
 *     E1 --> G([l0op::ViewCopy])
 *     G --> H[(out)]
 * ```
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * 支持非连续的Tensor，数据格式支持ND。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * shape与self一致，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalNotGetWorkspaceSize(const aclTensor* self, aclTensor* out, uint64_t* workspaceSize,
                                                      aclOpExecutor** executor);

/**
 * @brief aclnnLogicalNot的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalNotGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnLogicalNot(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

/**
 * @brief aclnnInplaceLogicalNot的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能：完成逻辑取反计算
 *
 * @param [in] selfRef: npu
 * device侧的aclTensor，数据类型支持FLOAT、FLOAT16、BFLOAT16、DOUBLE、INT32、INT64、INT16、INT8、UINT8、BOOL，
 * s持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalNotGetWorkspaceSize(aclTensor* selfRef, uint64_t* workspaceSize,
                                                             aclOpExecutor** executor);

/**
 * @brief aclnnInplaceLogicalNot的第二段接口，用于执行计算。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnLogicalNotGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceLogicalNot(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LogicalNot_H_
// End content from: aclnn_logical_not.h

// Begin content from: aclnn_non_max_suppression.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2024. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_NON_MAX_SUPPRESSION_H_
#define OP_API_INC_NON_MAX_SUPPRESSION_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

ACLNN_API aclnnStatus aclnnNonMaxSuppressionGetWorkspaceSize(const aclTensor* boxes, const aclTensor* scores,
                                                             aclIntArray* maxOutputBoxesPerClass,
                                                             aclFloatArray* iouThreshold, aclFloatArray* scoreThreshold,
                                                             int32_t centerPointBox, aclTensor* selectedIndices,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

ACLNN_API aclnnStatus aclnnNonMaxSuppression(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_NON_MAX_SUPPRESSION_H_// End content from: aclnn_non_max_suppression.h

// Begin content from: aclnn_topk.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_TOPK_H_
#define OP_API_INC_TOPK_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnTopk的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：完成计算输入的k个极值及下标。
 *
 * @param [in] self: npu
 * npu device侧的aclTensor，数据类型支持INT8、UINT8、INT16、INT32、INT64、FLOAT16、FLOAT32、DOUBLE。
 * 支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] k:
 * int64_t类型整数。表示计算维度上输出的极值个数。取值范围为[0, self.size(dim)]。
 * @param [in] dim:
 * int64_t类型整数。表示计算维度。取值范围为[-self.dim(), self.dim())。
 * @param [in] largest:
 * bool类型数据。True表示计算维度上的结果应由大到小输出，False表示计算维度上的结果由小到大输出。
 * @param [in] sorted:
 * bool类型数据。True表示输出结果需要排序（若largest为True则结果从大到小排序，若largest为False则结果从小到大排序），
 * False表示输出结果不排序，按输入时的数据顺序输出。
 * @param [in] valuesOut:
 * dnpu device侧的aclTensor，数据类型支持INT8、UINT8、INT16、INT32、INT64、FLOAT16、FLOAT32、DOUBLE，
 * 且数据类型与self保持一致，支持连续和非连续的Tensor，数据格式支持ND。
 * @param [in] indicesOut:
 * npu device侧的aclTensor，数据类型支持INT64。支持连续和非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTopkGetWorkspaceSize(const aclTensor* self, int64_t k, int64_t dim, bool largest,
                                                bool sorted, aclTensor* valuesOut, aclTensor* indicesOut,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);
/**
 * @brief aclnnTopk的第二段接口，用于执行计算。
 *
 * 算子功能：完成计算输入的k个极值及下标。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnTopkGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnTopk(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                const aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_TOPK_H_
// End content from: aclnn_topk.h

// Begin content from: aclnn_div.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_DIV_H_
#define OP_API_INC_DIV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnDiv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * @param [in] self: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与other构成互相推导关系，shape需要与other满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与other一致。
 * @param [in] other: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要与self构成互相推导关系，shape需要与self满足broadcast关系。
 * 支持非连续的Tensor，数据格式支持ND，且数据格式需要与self一致。
 * @param [in]
 * mode：余数处理方式的整型常量，枚举值如下：0-默认不执行舍入。1-将除法的小数部分舍入为零。2-向下舍入除法的结果。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持整型，浮点类型，且数据类型需要是self与other推导之后可转换的数据类型，shape需要是self与other
 * broadcast之后的shape，数据格式支持ND，且数据格式需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDivGetWorkspaceSize(const aclTensor* self, const aclTensor* other, aclTensor* out,
                                               uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDivs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnDivsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, aclTensor* out,
                                                uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDivMod的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnDivModGetWorkspaceSize(const aclTensor* self, const aclTensor* other, int mode,
                                                  aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDivMods的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnDivModsGetWorkspaceSize(const aclTensor* self, const aclScalar* other, int mode,
                                                   aclTensor* out, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceDiv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceDivGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other,
                                                      uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceDivs的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceDivsGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other,
                                                       uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceDivMod的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceDivModGetWorkspaceSize(aclTensor* selfRef, const aclTensor* other, int mode,
                                                         uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnInplaceDivMods的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 */
ACLNN_API aclnnStatus aclnnInplaceDivModsGetWorkspaceSize(aclTensor* selfRef, const aclScalar* other, int mode,
                                                          uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnDiv的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnDivGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnDiv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

ACLNN_API aclnnStatus aclnnDivs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

ACLNN_API aclnnStatus aclnnDivMod(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

ACLNN_API aclnnStatus aclnnDivMods(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                   aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceDiv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                      aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceDivs(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                       aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceDivMod(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                         aclrtStream stream);

ACLNN_API aclnnStatus aclnnInplaceDivMods(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                          aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_DIV_H_
// End content from: aclnn_div.h

// Begin content from: aclnn_mv.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_LEVEL2_ACLNN_MV_H_
#define OP_API_INC_LEVEL2_ACLNN_MV_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnMv的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_infer
 *
 * 算子功能： 计算矩阵input与向量vec的乘积
 * @param [in] self: npu device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、COMPLEX64、COMPLEX128类型。支持
 * [非连续的Tensor](#)，shape为n*m的二维张量，数据格式支持ND([参考](#))。
 * @param [in] vec: npu device侧的aclTensor。数据类型支持FLOAT16、FLOAT、DOUBLE、COMPLEX64、COMPLEX128类型，且数据类型与
 * self保持一致。支持[非连续的Tensor](#)，shape为长度为m的一维张量，数据格式支持ND([参考](#))。
 * @param [in] out: npu
 * device侧的aclTensor，数据类型支持FLOAT16、FLOAT、DOUBLE、COMPLEX64、COMPLEX128类型，且数据类型与self
 * 保持一致。支持[非连续的Tensor](#)，shape为长度为n的一维张量，数据格式支持ND([参考](#))。
 * @param [in] cubeMathType(INT8,
 * 计算输入)：INT8类型的枚举值，用于判断Cube单元应该使用哪种计算逻辑进行运算，0：KEEP_DTYPE, 保
 * 持输入数据类型进行计算，若输入是FP32，Ascend910系列芯片暂不支持，该值为0时会报错。1：ALLOW_FP32_DOWN_PRECISION，允许转换输入
 * 数据类型降低精度计算，若输入为FP32，在Ascend910系列芯片上转换为FP16进行计算，在Ascend910B系列芯片上转换为HF16进行计算。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnMvGetWorkspaceSize(const aclTensor* self, const aclTensor* vec, aclTensor* out,
                                              int8_t cubeMathType, uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnMv的第二段接口，用于执行计算
 *
 * 算子功能： 计算矩阵input与向量vec的乘积
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnMvGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnMv(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_LEVEL2_ACLNN_MV_H_// End content from: aclnn_mv.h

// Begin content from: aclnn_qr.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_QR_H_
#define OP_API_INC_QR_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnQr的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能： 对输入Tensor完成QR操作
 * @param [in] self: 计算输入, 数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128, 数据格式支持ND,
 * 支持非连续的Tensor
 * @param [in] some: 计算输入, 指定输出的tensor shape是否为complete, 数据类型支持BOOL
 * @param [out] Q: 计算输出,
 * Qr分解后得到的正交矩阵，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，且与self相同 数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [out] R: 计算输出,
 * Qr分解后得到的上三角矩阵，数据类型支持FLOAT、FLOAT16、DOUBLE、COMPLEX64、COMPLEX128，且与self相同 数据格式支持ND,
 * 支持非连续的Tensor。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnQrGetWorkspaceSize(const aclTensor* self, bool some, aclTensor* Q, aclTensor* R,
                                              uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief: aclnnQr的第二段接口，用于执行计算
 *
 * 算子功能： 对输入Tensor完成Qr 操作
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnQrGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnQr(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor, aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_QR_H_// End content from: aclnn_qr.h

// Begin content from: aclnn_scatter_nd_update.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_SCATTER_ND_UPDATE_H_
#define OP_API_INC_SCATTER_ND_UPDATE_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnScatterNdUpdate的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnnop_ops_infer
 * @domain aclnnop_ops_train
 * 算子功能： 将tensor updates中的值按指定的索引indices逐个更新tensor var中的值。
 * @param [in] varRef: npu device侧的aclTensor, 数据类型支持FLOAT16, FLOAT32, BOOL
 * INT64，BFLOAT16，支持非连续的Tensor，数据格式支持ND。
 * @param [in] indices: npu device侧的aclTensor，数据类型支持INT32, INT64类型。支持非连续的Tensor，数据格式支持ND。
 * @param [in] updates: npu device侧的aclTensor，数据类型支持FLOAT16, FLOAT32, BOOL
 * INT64，BFLOAT16，支持非连续的Tensor，数据格式支持ND,
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码
 */
ACLNN_API aclnnStatus aclnnScatterNdUpdateGetWorkspaceSize(aclTensor* varRef, const aclTensor* indices,
                                                           const aclTensor* updates, uint64_t* workspaceSize,
                                                           aclOpExecutor** executor);

/**
 * @brief: aclnnScatterNdUpdate的第二段接口，用于执行计算
 * @domain aclnnop_ops_infer
 * @domain aclnnop_ops_train
 * 算子功能： 将tensor updates中的值按指定的索引indices逐个更新tensor var中的值。
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnScatterNdUpdateGetWorkspaceSize获取。
 * @param [in] stream: acl stream流。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnScatterNdUpdate(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_SCATTER_ND_UPDATE_H_// End content from: aclnn_scatter_nd_update.h

// Begin content from: aclnn_nll_loss_backward.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_NLL_LOSS_BACKWARD_H_
#define OP_API_INC_NLL_LOSS_BACKWARD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnNLLLossBackward的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 *
 * 算子功能：负对数似然损失函数的反向传播。
 *
 * @param [in] gradOutput: npu
 * device侧的aclTensor，shape为(N,)或者为(1,)，数据类型支持FLOAT。支持非连续的Tensor，数据格式支持ND。
 * @param [in] self: npu device侧的aclTensor，shape为(N,C)或者(C,)，其中N表示batch
 * size，C表示类别数。数据类型支持FLOAT。支持非连续的Tensor, 数据格式支持ND。
 * @param [in] target: npu device侧的aclTensor，表示真实标签，shape为(N,) 或者(1,)，其中每个元素的取值范围是[0, C - 1]。
 * 数据类型支持INT64，UINT8 ，支持非连续的Tensor， 数据格式支持ND。
 * @param [in] weight: npu
 * device侧的aclTensor，表示各个类别的权重，shape为(C,)。数据类型支持FLOAT。支持非连续的Tensor，数据格式支持ND。
 * @param [in] reduction: host侧的int64_t，指定损失函数的计算方式，支持 0('none') | 1('mean') | 2('sum')。
 * 'none' 表示不应用减少，'mean' 表示输出的总和将除以输出中的元素数，'sum' 表示输出将被求和。
 * @param [in] ignoreIndex: host侧的int64_t，指定一个被忽略且不影响输入梯度的目标值。
 * @param [in] totalWeight: npu
 * device侧的aclTensor，shape为(1,)，数据类型支持FLOAT。且数据类型为与weight相同。数据格式支持ND。
 * @param [in] out: npu device侧的aclTensor。shape需要与self一致。支持非连续的Tensor，数据格式支持ND。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLossBackwardGetWorkspaceSize(const aclTensor* gradOutput, const aclTensor* self,
                                                           const aclTensor* target, const aclTensor* weight,
                                                           int64_t reduction, int64_t ignoreIndex,
                                                           const aclTensor* totalWeight, aclTensor* out,
                                                           uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnNLLLossBackward的第二段接口，用于执行计算。
 *
 * 算子功能：负对数似然损失函数的反向传播。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspace_size: 在npu device侧申请的workspace大小，由第一段接口aclnnNLLLossBackwardGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnNLLLossBackward(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                           aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_ADD_H_
// End content from: aclnn_nll_loss_backward.h

// Begin content from: aclnn_upsample_bicubic2d_aa.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef OP_API_INC_UNAMPLE_BICUBIC2D_AA_H_
#define OP_API_INC_UNAMPLE_BICUBIC2D_AA_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnUpsampleBicubic2dAA的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_ops_train
 * 
 * 算子功能：upsample_bicubic2d的抗锯齿计算。
 *
 * @param [in] x: npu device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持NCHW。
 * @param [in] outputSize: 输入IntArray，size大小为2。表示输输出out在H和W维度上的空间大小。
 * @param [in] alignCorners: bool常量，表示是否使用角对齐计算。
 * @param [in] scalesH: double常量，表示输出out的height维度乘数。
 * @param [in] scalesW: double常量，表示输出out的width维度乘数。
 * @param [out] out: npu
 * device侧的aclTensor，数据类型支持FLOAT、BFLOAT16、FLOAT16，且数据类型与x的数据类型一致。
 * 支持[非连续的Tensor](#非连续Tensor说明)，数据格式支持NCHW。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2dAAGetWorkspaceSize(const aclTensor* x, const aclIntArray* outputSize,
                                                             const bool alignCorners, const double scalesH,
                                                             const double scalesW, aclTensor* out,
                                                             uint64_t* workspaceSize, aclOpExecutor** executor);

/**
 * @brief aclnnUpsampleBicubic2dAA的第二段接口，用于执行计算。
 * 
 * 算子功能：upsample_bicubic2d的抗锯齿计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，
 * 由第一段接口aclnnUpsampleBicubic2dAAGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnUpsampleBicubic2dAA(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                             aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_UNAMPLE_BICUBIC2D_AA_H_
// End content from: aclnn_upsample_bicubic2d_aa.h

// Begin content from: aclnn_threshold.h
/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#ifndef OP_API_INC_THRESHOLD_H_
#define OP_API_INC_THRESHOLD_H_

// #include "aclnn/aclnn_base.h"
// #include "aclnn_util.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * @brief aclnnThreshold的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能: 对输入self进行阈值操作。当self中的元素大于threshold时，返回元素本身；否则，返回value。
 * @param [in] self: npu device侧的aclTensor，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 支持[非连续的Tensor](),数据格式支持ND[参考](#)
 * @param [in] threshold: npu device侧的aclScalar，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 表示对输入self的元素进行大小判断的阈值。
 * @param [in] value: npu device侧的aclScalar，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 表示输入self的元素小于阈值时的返回值。
 * @param [out] out: npu device侧的aclTensor，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 支持非连续的Tensor。且数据类型、数据格式和shape需要与self一致。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnThresholdGetWorkspaceSize(const aclTensor* self, const aclScalar* threshold,
                                                     const aclScalar* value, aclTensor* out, uint64_t* workspaceSize,
                                                     aclOpExecutor** executor);
/**
 * @brief aclnnThreshold的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnThresholdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnThreshold(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                     aclrtStream stream);

/**
 * @brief aclnnInplaceThreshold的第一段接口，根据具体的计算流程，计算workspace大小。
 * @domain aclnn_math
 *
 * 算子功能: 对输入self进行阈值操作。当self中的元素大于threshold时，返回元素本身；否则，返回value。
 * @param [in] selfRef: npu device侧的aclTensor，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 支持[非连续的Tensor](),数据格式支持ND[参考](#)
 * @param [in] threshold: npu device侧的aclScalar，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 表示对输入self的元素进行大小判断的阈值。
 * @param [in] value: npu device侧的aclScalar，支持数据类型FLOAT、FLOAT16、INT32、INT8、UINT8、INT16、INT64。
 * 表示输入self的元素小于阈值时的返回值。
 * @param [out] workspaceSize: 返回用户需要在npu device侧申请的workspace大小。
 * @param [out] executor: 返回op执行器，包含算子计算流程。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceThresholdGetWorkspaceSize(aclTensor* selfRef, const aclScalar* threshold,
                                                            const aclScalar* value, uint64_t* workspaceSize,
                                                            aclOpExecutor** executor);
/**
 * @brief aclnnInplaceThreshold的第二段接口，用于执行计算。
 *
 * @param [in] workspace: 在npu device侧申请的workspace内存起址。
 * @param [in] workspaceSize: 在npu device侧申请的workspace大小，由第一段接口aclnnInplaceThresholdGetWorkspaceSize获取。
 * @param [in] executor: op执行器，包含了算子计算流程。
 * @param [in] stream: acl stream流。
 * @return aclnnStatus: 返回状态码。
 */
ACLNN_API aclnnStatus aclnnInplaceThreshold(void* workspace, uint64_t workspaceSize, aclOpExecutor* executor,
                                            aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif  // OP_API_INC_THRESHOLD_H_
// End content from: aclnn_threshold.h

// Begin content from: aclnn_foreach_reciprocal.h

/*
 * calution: this file was generated automaticlly donot change it.
*/

#ifndef ACLNN_FOREACH_RECIPROCAL_H_
#define ACLNN_FOREACH_RECIPROCAL_H_

// #include "aclnn/acl_meta.h"

#ifdef __cplusplus
extern "C" {
#endif

/* funtion: aclnnForeachReciprocalGetWorkspaceSize
 * parameters :
 * x : dynamic
 * out : dynamic
 * workspaceSize : size of workspace(output).
 * executor : executor context(output).
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachReciprocalGetWorkspaceSize(
    const aclTensorList *x,
    const aclTensorList *out,
    uint64_t *workspaceSize,
    aclOpExecutor **executor);

/* funtion: aclnnForeachReciprocal
 * parameters :
 * workspace : workspace memory addr(input).
 * workspaceSize : size of workspace(input).
 * executor : executor context(input).
 * stream : acl stream.
 */
__attribute__((visibility("default")))
aclnnStatus aclnnForeachReciprocal(
    void *workspace,
    uint64_t workspaceSize,
    aclOpExecutor *executor,
    aclrtStream stream);

#ifdef __cplusplus
}
#endif

#endif
// End content from: aclnn_foreach_reciprocal.h

